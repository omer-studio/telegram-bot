#!/usr/bin/env python3
"""
×’×™×‘×•×™ ×˜×‘×œ××•×ª ××™×•×ª×¨×•×ª ×œ×¤× ×™ ××—×™×§×”
××’×‘×” ××ª ×›×œ ×”×˜×‘×œ××•×ª ×”××™×•×ª×¨×•×ª ×œ×§×‘×¦×™ JSON ×‘×ª×™×§×™×™×ª backups/
"""

import psycopg2
import json
import os
import csv
from datetime import datetime
from config import DB_URL

def create_backup_directory():
    """×™×•×¦×¨ ×ª×™×§×™×™×ª ×’×™×‘×•×™ ×¢× ×—×•×ª××ª ×–××Ÿ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = f"backups/unused_tables_backup_{timestamp}"
    os.makedirs(backup_dir, exist_ok=True)
    return backup_dir

def backup_table_to_json(table_name, backup_dir):
    """××’×‘×” ×˜×‘×œ×” ×œ×§×•×‘×¥ JSON"""
    try:
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()
        
        # ×‘×“×™×§×ª ×§×™×•× ×”×˜×‘×œ×”
        cur.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_name = %s
            )
        """, (table_name,))
        
        if not cur.fetchone()[0]:
            print(f"âš ï¸ ×˜×‘×œ×” {table_name} ×œ× ×§×™×™××ª - ××“×œ×’")
            cur.close()
            conn.close()
            return False
        
        # ×§×‘×œ×ª ×›×œ ×”× ×ª×•× ×™×
        cur.execute(f"SELECT * FROM {table_name}")
        rows = cur.fetchall()
        
        # ×§×‘×œ×ª ×©××•×ª ×”×¢××•×“×•×ª
        cur.execute("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = %s
            ORDER BY ordinal_position
        """, (table_name,))
        
        columns = [row[0] for row in cur.fetchall()]
        
        # ×”××¨×” ×œ×¨×©×™××ª dictionaries
        data = []
        for row in rows:
            row_dict = {}
            for i, value in enumerate(row):
                # ×”××¨×ª datetime ×œstring
                if hasattr(value, 'isoformat'):
                    value = value.isoformat()
                row_dict[columns[i]] = value
            data.append(row_dict)
        
        # ×©××™×¨×” ×œ×§×•×‘×¥ JSON
        json_file = os.path.join(backup_dir, f"{table_name}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'table_name': table_name,
                'backup_timestamp': datetime.now().isoformat(),
                'row_count': len(data),
                'columns': columns,
                'data': data
            }, f, ensure_ascii=False, indent=2)
        
        # ×©××™×¨×” ×œ×§×•×‘×¥ CSV ×’× ×›×Ÿ
        csv_file = os.path.join(backup_dir, f"{table_name}.csv")
        if data:
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=columns)
                writer.writeheader()
                writer.writerows(data)
        
        cur.close()
        conn.close()
        
        print(f"âœ… {table_name}: {len(data)} ×©×•×¨×•×ª × ×’×‘×• ×œ-{json_file} ×•-{csv_file}")
        return True
        
    except Exception as e:
        print(f"âŒ ×©×’×™××” ×‘×’×™×‘×•×™ {table_name}: {e}")
        return False

def backup_unused_tables():
    """××’×‘×” ××ª ×›×œ ×”×˜×‘×œ××•×ª ×”××™×•×ª×¨×•×ª"""
    print("ğŸ“¦ === ×’×™×‘×•×™ ×˜×‘×œ××•×ª ××™×•×ª×¨×•×ª ×œ×¤× ×™ ××—×™×§×” ===")
    
    # ×™×¦×™×¨×ª ×ª×™×§×™×™×ª ×’×™×‘×•×™
    backup_dir = create_backup_directory()
    print(f"ğŸ“ ×ª×™×§×™×™×ª ×’×™×‘×•×™: {backup_dir}")
    
    # ×¨×©×™××ª ×˜×‘×œ××•×ª ××™×•×ª×¨×•×ª
    unused_tables = [
        'gpt_usage_log',
        'system_logs', 
        'critical_users',
        'billing_usage',
        'errors_stats',
        'free_model_limits',
        'chat_messages_old'  # ×× ×§×™×™××ª
    ]
    
    backup_results = {}
    
    for table in unused_tables:
        print(f"\nğŸ”„ ××’×‘×” ×˜×‘×œ×”: {table}")
        success = backup_table_to_json(table, backup_dir)
        backup_results[table] = success
    
    # ×™×¦×™×¨×ª ×§×•×‘×¥ ×¡×™×›×•×
    summary = {
        'backup_timestamp': datetime.now().isoformat(),
        'backup_directory': backup_dir,
        'tables_backed_up': backup_results,
        'successful_backups': sum(backup_results.values()),
        'total_tables': len(backup_results),
        'note': '×”×˜×‘×œ××•×ª ×”××œ×” ×”×•×©×‘×ª×• ×‘×§×•×“ ×•×¢×›×©×™×• × ×™×ª×Ÿ ×œ××—×•×§ ××•×ª×Ÿ ×‘×‘×˜×—×”'
    }
    
    summary_file = os.path.join(backup_dir, 'backup_summary.json')
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    # ×“×•×— ×¡×™×›×•×
    successful = sum(backup_results.values())
    total = len(backup_results)
    
    print(f"\nğŸ“Š === ×¡×™×›×•× ×’×™×‘×•×™ ===")
    print(f"ğŸ“ ×ª×™×§×™×™×ª ×’×™×‘×•×™: {backup_dir}")
    print(f"âœ… ×˜×‘×œ××•×ª ×©× ×’×‘×• ×‘×”×¦×œ×—×”: {successful}/{total}")
    print(f"ğŸ“„ ×§×•×‘×¥ ×¡×™×›×•×: {summary_file}")
    
    for table, success in backup_results.items():
        status = "âœ…" if success else "âŒ"
        print(f"   {status} {table}")
    
    if successful == total:
        print(f"\nğŸ‰ ×›×œ ×”×˜×‘×œ××•×ª × ×’×‘×• ×‘×”×¦×œ×—×”!")
        print(f"ğŸš« ×›×¢×ª × ×™×ª×Ÿ ×œ××—×•×§ ××ª ×”×˜×‘×œ××•×ª ×”××™×•×ª×¨×•×ª ×‘×‘×˜×—×”")
        return True
    else:
        failed = total - successful
        print(f"\nâš ï¸ {failed} ×˜×‘×œ××•×ª × ×›×©×œ×• ×‘×’×™×‘×•×™ - ×‘×“×•×§ ×œ×¤× ×™ ××—×™×§×”!")
        return False

def verify_backup_integrity(backup_dir):
    """××××ª ×ª×§×™× ×•×ª ×”×’×™×‘×•×™"""
    print(f"\nğŸ” ××××ª ×ª×§×™× ×•×ª ×’×™×‘×•×™ ×‘-{backup_dir}")
    
    try:
        # ×§×¨×™××ª ×§×•×‘×¥ ×”×¡×™×›×•×
        summary_file = os.path.join(backup_dir, 'backup_summary.json')
        if not os.path.exists(summary_file):
            print("âŒ ×§×•×‘×¥ ×¡×™×›×•× ×œ× × ××¦×")
            return False
        
        with open(summary_file, 'r', encoding='utf-8') as f:
            summary = json.load(f)
        
        # ×‘×“×™×§×ª ×§×™×•× ×§×‘×¦×™×
        missing_files = []
        for table, success in summary['tables_backed_up'].items():
            if success:
                json_file = os.path.join(backup_dir, f"{table}.json")
                csv_file = os.path.join(backup_dir, f"{table}.csv")
                
                if not os.path.exists(json_file):
                    missing_files.append(f"{table}.json")
                if not os.path.exists(csv_file):
                    missing_files.append(f"{table}.csv")
        
        if missing_files:
            print(f"âŒ ×§×‘×¦×™× ×—×¡×¨×™×: {missing_files}")
            return False
        
        print("âœ… ×›×œ ×§×‘×¦×™ ×”×’×™×‘×•×™ ×§×™×™××™× ×•×ª×§×™× ×™×")
        return True
        
    except Exception as e:
        print(f"âŒ ×©×’×™××” ×‘××™××•×ª ×’×™×‘×•×™: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ“¦ ××ª×—×™×œ ×’×™×‘×•×™ ×˜×‘×œ××•×ª ××™×•×ª×¨×•×ª...")
    
    # ×‘×™×¦×•×¢ ×”×’×™×‘×•×™
    backup_success = backup_unused_tables()
    
    if backup_success:
        # ××™××•×ª ×ª×§×™× ×•×ª ×”×’×™×‘×•×™
        backup_dirs = [d for d in os.listdir('backups') if d.startswith('unused_tables_backup_')]
        if backup_dirs:
            latest_backup = os.path.join('backups', sorted(backup_dirs)[-1])
            verify_backup_integrity(latest_backup)
        
        print(f"\nğŸ¯ ×”×’×™×‘×•×™ ×”×•×©×œ× ×‘×”×¦×œ×—×”!")
        print(f"ğŸš« ×›×¢×ª × ×™×ª×Ÿ ×œ×”××©×™×š ×œ××—×™×§×ª ×”×˜×‘×œ××•×ª ×”××™×•×ª×¨×•×ª")
    else:
        print(f"\nâš ï¸ ×”×’×™×‘×•×™ ×œ× ×”×•×©×œ× ×‘××œ×•××• - ×‘×“×•×§ ×©×’×™××•×ª ×œ×¤× ×™ ××—×™×§×”!")
    
    print(f"\nâœ… ×¡×§×¨×™×¤×˜ ×’×™×‘×•×™ ×”×¡×ª×™×™×") 