"""
gpt_e_handler.py
----------------
×× ×•×¢ gpt_e: ×—×™×“×•×“, ×ª×™×§×•×Ÿ ×•×”×©×œ××ª ×¤×¨×•×¤×™×œ ×¨×’×©×™ ×¢×œ ×‘×¡×™×¡ ×”×™×¡×˜×•×¨×™×” ×•×¤×¨×•×¤×™×œ ×§×™×™×.
××©×ª××© ×‘-Gemini 1.5 Pro (×—×™× ××™) - ×œ×œ× ×¦×•×¨×š ×‘-fallback.

- ××•×¤×¢×œ ×›×œ X ×”×•×“×¢×•×ª ××©×ª××© (× ×™×ª×Ÿ ×œ×©×™× ×•×™ ×‘×§×œ×•×ª).
- ×¡×•×¨×§ Y ×”×•×“×¢×•×ª ××—×¨×•× ×•×ª (× ×™×ª×Ÿ ×œ×©×™× ×•×™ ×‘×§×œ×•×ª).
- ××ª××§×“ ×‘×¢×“×›×•×Ÿ ×”×§×•× ×¤×œ×™×§×˜ ×”××¨×›×–×™ (primary_conflict) ×•×©×“×•×ª × ×•×¡×¤×™× ×œ×¤×™ ×”×¦×•×¨×š.
- ×©×•×œ×— ×œ-GPT ××ª ×”×”×™×¡×˜×•×¨×™×” ×•×”×¤×¨×•×¤×™×œ, ××§×‘×œ ×©×“×•×ª ×—×“×©×™×/××ª×•×§× ×™× ×‘×œ×‘×“.
- ××¢×“×›×Ÿ Google Sheets, user_state, ×•×œ×•×’×™×.
"""

# âœ… ×§×‘×•×¢×™× × ×•×—×™× ×œ×©×™× ×•×™
GPT_E_RUN_EVERY_MESSAGES = 10  # ×›×œ ×›××” ×”×•×“×¢×•×ª ×œ×”×¤×¢×™×œ GPT-E
GPT_E_SCAN_LAST_MESSAGES = 15  # ×›××” ×”×•×“×¢×•×ª ××—×¨×•× ×•×ª ×œ×¡×¨×•×§

import logging
import asyncio
import json
import lazy_litellm as litellm
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import pytz

# ×™×™×‘×•× ×¤×•× ×§×¦×™×•×ª ×¢×–×¨
from utils import get_chat_history_messages, get_israel_time
from sheets_handler import get_user_summary, update_user_profile, reset_gpt_c_run_count
from prompts import build_profile_extraction_enhanced_prompt, JSON_RESPONSE_INSTRUCTION, JSON_EXAMPLE
from gpt_utils import normalize_usage_dict, safe_get_usage_value, measure_llm_latency, calculate_gpt_cost, extract_json_from_text
from config import GPT_MODELS, GPT_PARAMS

# ×”×’×“×¨×ª ×œ×•×’×¨
logger = logging.getLogger(__name__)

def should_run_gpt_e(chat_id: str, total_messages: int) -> bool:
    """
    ×‘×•×“×§ ×”×× ×¦×¨×™×š ×œ×”×¤×¢×™×œ ××ª gpt_e ×œ×¤×™ ××¡×¤×¨ ×”×”×•×“×¢×•×ª.
    
    :param chat_id: ××–×”×” ×”××©×ª××©
    :param total_messages: ××¡×¤×¨ ×”×”×•×“×¢×•×ª ×”×›×•×œ×œ ×©×œ ×”××©×ª××©
    :return: True ×× ×¦×¨×™×š ×œ×”×¤×¢×™×œ gpt_e, False ××—×¨×ª
    """
    # âœ… ×œ×•×’×™×§×” ×¤×©×•×˜×”: ×›×œ X ×”×•×“×¢×•×ª
    if total_messages > 0 and total_messages % GPT_E_RUN_EVERY_MESSAGES == 0:
        logger.info(f"[gpt_e] Triggering run - {total_messages} messages (every {GPT_E_RUN_EVERY_MESSAGES})")
        return True
    
    return False

def build_fields_list():
    """×‘×•× ×” ×¨×©×™××ª ×©×“×•×ª ××•×ª×¨×™× ×fields_dict.py"""
    from prompts import _get_filtered_profile_fields
    
    # ×¡×™× ×•×Ÿ ×©×“×•×ª ×œ× ×¨×œ×•×•× ×˜×™×™× ×œgpt_e
    relevant_fields = _get_filtered_profile_fields()
    
    # ×™×¦×™×¨×ª ×¨×©×™××” ×‘×¤×•×¨××˜ ×§×¨×™×
    fields_list = []
    for i in range(0, len(relevant_fields), 3):  # 3 ×©×“×•×ª ×‘×›×œ ×©×•×¨×”
        row = ", ".join(relevant_fields[i:i+3])
        fields_list.append(f"- {row}")
    
    return "\n".join(fields_list)

def prepare_gpt_e_prompt(chat_history: List[Dict], current_profile: str) -> str:
    """
    ××›×™×Ÿ ××ª ×”×¤×¨×•××¤×˜ ×œ-gpt_e ×¢× ×”×”×™×¡×˜×•×¨×™×” ×•×”×¤×¨×•×¤×™×œ ×”×§×™×™×.
    
    :param chat_history: ×¨×©×™××ª ×”×•×“×¢×•×ª ××”×©×™×—×”
    :param current_profile: ×”×¤×¨×•×¤×™×œ ×”×¨×’×©×™ ×”×§×™×™×
    :return: ×¤×¨×•××¤×˜ ××•×›×Ÿ ×œ×©×œ×™×—×” ×œ-GPT
    """
    # ×”××¨×ª ×”×™×¡×˜×•×¨×™×” ×œ×¤×•×¨××˜ ××ª××™×
    formatted_history = []
    for msg in chat_history:
        if msg.get('role') in ['user', 'assistant']:
            formatted_history.append({
                'role': msg['role'],
                'content': msg.get('text', msg.get('content', ''))
            })
    
    # ×™×¦×™×¨×ª ×¤×¨×•××¤×˜ ××•×ª×× ×œ-gpt_e
    fields_list = build_fields_list()
    user_prompt = f"""
××ª×” ××§×‘×œ ×”×™×¡×˜×•×¨×™×” ×©×œ ×©×™×—×” ×‘×™×Ÿ ××©×ª××© ×œ×‘×•×˜, ×•×¤×¨×•×¤×™×œ ×¨×’×©×™ ×§×™×™× ×©×œ ×”××©×ª××©.

××˜×¨×ª×š: ×œ×–×”×•×ª ××™×“×¢ ×—×“×©, ×œ×ª×§×Ÿ ×˜×¢×•×™×•×ª, ×•×œ×—×“×“ ××ª ×”×¤×¨×•×¤×™×œ ×”×¨×’×©×™.
×“×’×© ××™×•×—×“ ×¢×œ ×¢×“×›×•×Ÿ ×”×©×“×” "primary_conflict" - ×”×§×•× ×¤×œ×™×§×˜ ×”××¨×›×–×™ ×©×¢××• ×”××©×ª××© ××ª××•×“×“ ×›×¨×’×¢.

×”×™×¡×˜×•×¨×™×™×ª ×”×©×™×—×” ({GPT_E_SCAN_LAST_MESSAGES} ×”×•×“×¢×•×ª ××—×¨×•× ×•×ª):
{json.dumps(formatted_history, ensure_ascii=False, indent=2)}

×¤×¨×•×¤×™×œ ×¨×’×©×™ ×§×™×™×:
{current_profile}

×”× ×—×™×•×ª:
1. ×–×”×” ××™×“×¢ ×—×“×© ×©×œ× × ×›×œ×œ ×‘×¤×¨×•×¤×™×œ ×”×§×™×™×
2. ×ª×§×Ÿ ×˜×¢×•×™×•×ª ××• ××™-×“×™×•×§×™× ×‘×¤×¨×•×¤×™×œ ×”×§×™×™×  
3. ×—×“×“ ××™×“×¢ ×§×™×™× ×¢× ×¤×¨×˜×™× × ×•×¡×¤×™×
4. **×”×©×“×” primary_conflict ×”×•× ×§×¨×™×˜×™ - ×¢×“×›×Ÿ ××•×ª×• ×× ×™×© ××™×“×¢ ×—×“×© ×¢×œ ××” ×©×”××©×ª××© ××ª××•×“×“ ××™×ª×• ×›×¨×’×¢**
5. ×‘×“×•×§ ×× ×”×§×•× ×¤×œ×™×§×˜ ×”××¨×›×–×™ ×”×©×ª× ×” ××• ×”×ª×¤×ª×— ×‘×”×•×“×¢×•×ª ×”××—×¨×•× ×•×ª
6. ×”×—×–×¨ ×¨×§ ×©×“×•×ª ×—×“×©×™×/××ª×•×§× ×™× ×‘×¤×•×¨××˜ JSON
7. ×× ××™×Ÿ ××™×“×¢ ×—×“×© - ×”×—×–×¨ {{}}
8. ×× ×™×© ××™×“×¢ ××™×©×™ ×©×œ× × ×›× ×¡ ×œ×©×•× ×©×“×” - ×”×•×¡×£ ×œ-"other_insights"

×©×“×•×ª ××•×ª×¨×™×:
{fields_list}

×“×’×© ××™×•×—×“: ×”×©×“×” "primary_conflict" ×¦×¨×™×š ×œ×©×§×£ ×‘××“×•×™×§ ××ª ××” ×©×”××©×ª××© ××ª××•×“×“ ××™×ª×• ×›×¨×’×¢ ×¢×œ ×‘×¡×™×¡ ×”×”×™×¡×˜×•×¨×™×” ×”××—×¨×•× ×”.

{JSON_RESPONSE_INSTRUCTION}
{JSON_EXAMPLE}
"""

    return user_prompt

async def run_gpt_e(chat_id: str) -> Dict[str, Any]:
    """
    ××‘×¦×¢ ×—×™×“×•×“ ×•×ª×™×§×•×Ÿ ×¤×¨×•×¤×™×œ ×¨×’×©×™ ×œ××©×ª××© ×œ×¤×™ ×”×”× ×—×™×•×ª.
    
    :param chat_id: ××–×”×” ×”××©×ª××©
    :return: ××™×œ×•×Ÿ ×¢× ×ª×•×¦××•×ª ×”×¨×™×¦×” (success, changes, errors)
    """
    start_time = get_israel_time()
    logger.info(f"[gpt_e] Starting run_gpt_e for chat_id={chat_id} at {start_time.isoformat()}")
    
    result = {
        'success': False,
        'changes': {},
        'errors': [],
        'tokens_used': 0,
        'execution_time': 0
    }
    
    try:
        # ×©×œ×‘ 1: ×©×œ×™×¤×ª ×”×™×¡×˜×•×¨×™×™×ª ×©×™×—×”
        logger.info(f"[gpt_e] Fetching chat history for chat_id={chat_id}")
        chat_history = get_chat_history_messages(chat_id, limit=GPT_E_SCAN_LAST_MESSAGES)
        
        if not chat_history:
            result['errors'].append("No chat history found")
            logger.warning(f"[gpt_e] No chat history found for chat_id={chat_id}")
            return result
        
        logger.info(f"[gpt_e] Retrieved {len(chat_history)} messages from chat history")
        
        # ×©×œ×‘ 2: ×©×œ×™×¤×ª ×¤×¨×•×¤×™×œ ×§×™×™×
        logger.info(f"[gpt_e] Fetching current profile for chat_id={chat_id}")
        current_profile = get_user_summary(chat_id)
        
        # ×“×™×‘××’ ××¤×•×¨×˜ ×©×œ ×”×¤×¨×•×¤×™×œ ×”× ×•×›×—×™
        print(f"[DEBUG] gpt_e_profile | chat={chat_id} | profile='{current_profile[:35]}{'...' if len(current_profile) > 35 else ''}'")
        
        if not current_profile:
            current_profile = "××™×Ÿ ×¤×¨×•×¤×™×œ ×§×™×™×"
            logger.info(f"[gpt_e] No existing profile found for chat_id={chat_id}")
        
        # ×©×œ×‘ 3: ×”×›× ×ª ×¤×¨×•××¤×˜
        user_prompt = prepare_gpt_e_prompt(chat_history, current_profile)
        
        # ×©×œ×‘ 4: ×©×œ×™×—×” ×œ-GPT
        logger.info(f"[gpt_e] Sending request to GPT for chat_id={chat_id}")
        
        try:
            metadata = {"gpt_identifier": "gpt_e", "chat_id": chat_id}
            params = GPT_PARAMS["gpt_e"]
            model = GPT_MODELS["gpt_e"]
            
            completion_params = {
                "model": model,
                "messages": [
                    {"role": "system", "content": build_profile_extraction_enhanced_prompt()},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": params["temperature"],
                "max_tokens": params["max_tokens"],
                "metadata": metadata
            }
            
            with measure_llm_latency(model):
                response = litellm.completion(**completion_params)
            
            # ×—×™×œ×•×¥ ×”×ª×•×›×Ÿ ××”×ª×’×•×‘×”
            content_raw = response.choices[0].message.content
            if content_raw is None:
                result['errors'].append("Empty response content from GPT")
                logger.error(f"[gpt_e] Empty response content from GPT for chat_id={chat_id}")
                return result
            
            content_raw = content_raw.strip()
            content = extract_json_from_text(content_raw)
            logger.info(f"[gpt_e] Received response from GPT: {content[:200] if content else 'None'}...")
            
            # × ×™×¨××•×œ usage ×‘××•×¤×Ÿ ×‘×˜×•×—
            usage = normalize_usage_dict(response.usage, response.model)
            
            # ×—×™×œ×•×¥ cached_tokens ×‘×‘×˜×—×”
            cached_tokens = safe_get_usage_value(response.usage, 'cached_tokens', 0)
            if cached_tokens == 0 and hasattr(response.usage, 'prompt_tokens_details'):
                cached_tokens = safe_get_usage_value(response.usage.prompt_tokens_details, 'cached_tokens', 0)
            
            # ×¢×“×›×•×Ÿ ×”-usage ×¢× cached_tokens
            usage["cached_tokens"] = cached_tokens
            
            try:
                cost_info = calculate_gpt_cost(
                    prompt_tokens=usage.get("prompt_tokens", 0),
                    completion_tokens=usage.get("completion_tokens", 0),
                    cached_tokens=usage.get("cached_tokens", 0),
                    model_name=response.model,
                    completion_response=response
                )
                usage.update(cost_info)
            except Exception as _cost_e:
                logger.warning(f"[gpt_e] Cost calc failed: {_cost_e}")
            
            result['tokens_used'] = response.usage.total_tokens
            result['cost_data'] = usage
            
            try:
                from gpt_jsonl_logger import GPTJSONLLogger
                # ×‘× ×™×™×ª response ××œ× ×¢× ×›×œ ×”××™×“×¢ ×”× ×“×¨×©
                response_data = {
                    "id": getattr(response, "id", ""),
                    "choices": [
                        {
                            "message": {
                                "content": content_raw,
                                "role": "assistant"
                            }
                        }
                    ],
                    "usage": usage,
                    "model": response.model
                }
                GPTJSONLLogger.log_gpt_call(
                    log_path="data/openai_calls.jsonl",
                    gpt_type="E",
                    request=completion_params if 'completion_params' in locals() else {},
                    response=response_data,
                    cost_usd=usage.get("cost_total", 0) if 'usage' in locals() else 0,
                    extra={"chat_id": chat_id}
                )
            except Exception as log_exc:
                print(f"[LOGGING_ERROR] Failed to log GPT-E call: {log_exc}")
            
        except Exception as e:
            result['errors'].append(f"GPT API error: {str(e)}")
            logger.error(f"[gpt_e] GPT API error for chat_id={chat_id}: {e}")
            return result
        
        if not content:
            result['errors'].append("Empty response from GPT")
            logger.error(f"[gpt_e] Empty response from GPT for chat_id={chat_id}")
            return result

        # ×©×œ×‘ 5: × ×™×ª×•×— ×”×ª×’×•×‘×”
        # × ×™×¡×™×•×Ÿ ×œ×—×œ×¥ JSON ××”×ª×’×•×‘×”
        try:
            # ×—×™×¤×•×© JSON ×‘×ª×’×•×‘×”
            if content.startswith('{') and content.endswith('}'):
                try:
                    changes = json.loads(content)
                except json.JSONDecodeError as e:
                    logging.error(f"[GPT_E] JSON decode error: {e} | Content: {content[:200]}")
                    changes = {}
            else:
                # ×—×™×¤×•×© JSON ×‘×ª×•×š ×”×˜×§×¡×˜
                start_idx = content.find('{')
                end_idx = content.rfind('}') + 1
                if start_idx != -1 and end_idx > start_idx:
                    json_str = content[start_idx:end_idx]
                    try:
                        changes = json.loads(json_str)
                    except json.JSONDecodeError as e:
                        logging.error(f"[GPT_E] JSON decode error: {e} | Content: {json_str[:200]}")
                        changes = {}
                else:
                    changes = {}
            
            logger.info(f"[gpt_e] Parsed changes: {changes}")
            
            # ×©×œ×‘ 6: ×¢×“×›×•×Ÿ ×”×¤×¨×•×¤×™×œ ×× ×™×© ×©×™× ×•×™×™×
            if changes and isinstance(changes, dict):
                # ×”×¡×¨×ª ×©×“×•×ª ×¨×™×§×™×
                changes = {k: v for k, v in changes.items() if v and v != ""}
                
                if changes:
                    logger.info(f"[gpt_e] Updating profile with changes: {changes}")
                    # ×”×“×¤×¡×ª ××™×“×¢ ×—×©×•×‘ ×¢×œ ×¢×“×›×•×Ÿ × ×ª×•× ×™× (×ª××™×“ ×™×•×¤×™×¢!)
                    print(f"ğŸ”„ [GPT-E] ××¢×“×›×Ÿ {len(changes)} ×©×“×•×ª: {list(changes.keys())}")
                    if 'cost_data' in result and result['cost_data']:
                        print(f"ğŸ’° [GPT-E] ×¢×œ×•×ª: {result['cost_data'].get('cost_total', 0):.6f}$ | ×˜×•×§× ×™×: {result['cost_data'].get('total_tokens', 0)}")
                    
                    # ×”×“×’×©×” ××™×•×—×“×ª ×œ×¢×“×›×•×Ÿ primary_conflict
                    if 'primary_conflict' in changes:
                        print(f"ğŸ¯ [GPT-E] ×¢×“×›×•×Ÿ ×§×•× ×¤×œ×™×§×˜ ××¨×›×–×™: {changes['primary_conflict'][:100]}...")
                    
                    # ğŸ”§ ×”×¡×¨×”: ×œ× ××¢×“×›× ×™× ×›××Ÿ ×›×“×™ ×œ×× ×•×¢ ×›×¤×™×œ×•×ª ×¢× _handle_profile_updates
                    # await update_user_profile(chat_id, changes)
                    
                    result['changes'] = changes
                    logger.info(f"[gpt_e] Changes prepared for profile update: {changes}")
                    print(f"âœ… [GPT-E] ×”×©×™× ×•×™×™× ××•×›× ×™× ×œ×¢×“×›×•×Ÿ ×¤×¨×•×¤×™×œ")
                else:
                    logger.info(f"[gpt_e] No meaningful changes found for chat_id={chat_id}")
            else:
                logger.info(f"[gpt_e] No changes to apply for chat_id={chat_id}")
                
        except Exception as e:
            result['errors'].append(f"Failed to parse changes: {e}")
            logger.error(f"[gpt_e] Error parsing changes for chat_id={chat_id}: {e}")
            return result
        
        # ×©×œ×‘ 7: ×¢×“×›×•×Ÿ ×¡×˜×˜×™×¡×˜×™×§×•×ª
        result['success'] = True
        result['execution_time'] = (get_israel_time() - start_time).total_seconds()
        
        logger.info(f"[gpt_e] Completed successfully for chat_id={chat_id} in {result['execution_time']:.2f}s")
        
    except Exception as e:
        result['errors'].append(f"Unexpected error: {str(e)}")
        logger.error(f"[gpt_e] Unexpected error for chat_id={chat_id}: {e}", exc_info=True)
    
    return result

async def update_user_state_after_gpt_e(chat_id: str, result: Dict[str, Any]) -> None:
    """
    ××¢×“×›×Ÿ ××ª ××¦×‘ ×”××©×ª××© ×œ××—×¨ ×¨×™×¦×ª gpt_e.
    
    :param chat_id: ××–×”×” ×”××©×ª××©
    :param result: ×ª×•×¦××•×ª ×”×¨×™×¦×”
    """
    try:
        if result['success']:
            # ××™×¤×•×¡ ××•× ×” gpt_c_run_count ×•×¢×“×›×•×Ÿ last_gpt_e_timestamp
            success = reset_gpt_c_run_count(chat_id)
            
            if success:
                logger.info(f"[gpt_e] Updated user state for chat_id={chat_id} after successful run")
            else:
                logger.error(f"[gpt_e] Failed to update user state for chat_id={chat_id}")
        else:
            logger.warning(f"[gpt_e] Not updating user state for chat_id={chat_id} due to failed run")
        
    except Exception as e:
        logger.error(f"[gpt_e] Error updating user state for chat_id={chat_id}: {e}")

def log_gpt_e_run(chat_id: str, result: Dict[str, Any]) -> None:
    """
    ××ª×¢×“ ××ª ×¨×™×¦×ª gpt_e ×‘×œ×•×’×™×.
    
    :param chat_id: ××–×”×” ×”××©×ª××©
    :param result: ×ª×•×¦××•×ª ×”×¨×™×¦×”
    """
    log_entry = {
        'timestamp': get_israel_time().isoformat(),
        'chat_id': chat_id,
        'success': result['success'],
        'changes_count': len(result.get('changes', {})),
        'tokens_used': result.get('tokens_used', 0),
        'execution_time': result.get('execution_time', 0),
        'errors': result.get('errors', [])
    }
    
    if result.get('changes'):
        log_entry['changes'] = result['changes']
    
    logger.info(f"[gpt_e] Run log: {json.dumps(log_entry, ensure_ascii=False)}")

async def execute_gpt_e_if_needed(chat_id: str) -> Optional[Dict[str, Any]]:
    """
    ×‘×•×“×§ ×× ×¦×¨×™×š ×œ×”×¤×¢×™×œ gpt_e ×•××¤×¢×™×œ ×× ×›×Ÿ.
    :param chat_id: ××–×”×” ×”××©×ª××©
    :return: ×ª×•×¦××•×ª ×”×¨×™×¦×” ×× ×”×•×¤×¢×œ, None ×× ×œ× ×”×•×¤×¢×œ
    """
    from chat_utils import get_user_stats_and_history
    logger.info(f"[gpt_e] Checking if should run for chat_id={chat_id} (every {GPT_E_RUN_EVERY_MESSAGES} user messages)")
    try:
        stats, _ = get_user_stats_and_history(chat_id)
        total_messages = stats.get("total_messages", 0)
        
        if should_run_gpt_e(chat_id, total_messages):
            logger.info(f"[gpt_e] Triggered for chat_id={chat_id} (user sent {total_messages} messages)")
            result = await run_gpt_e(chat_id)
            if result['success']:
                await update_user_state_after_gpt_e(chat_id, result)
            log_gpt_e_run(chat_id, result)
            return result
        else:
            logger.info(f"[gpt_e] Not running for chat_id={chat_id} (user sent {total_messages} messages)")
            return None
    except Exception as e:
        logger.error(f"[gpt_e] Error in execute_gpt_e_if_needed: {e}")
        return None 