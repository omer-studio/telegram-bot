"""
gpt_e_handler.py
----------------
×ž× ×•×¢ gpt_e: ×—×™×“×•×“, ×ª×™×§×•×Ÿ ×•×”×©×œ×ž×ª ×¤×¨×•×¤×™×œ ×¨×’×©×™ ×¢×œ ×‘×¡×™×¡ ×”×™×¡×˜×•×¨×™×” ×•×¤×¨×•×¤×™×œ ×§×™×™×.
×ž×©×ª×ž×© ×‘-Gemini 1.5 Pro (×—×™× ×ž×™) - ×œ×œ× ×¦×•×¨×š ×‘-fallback.

- ×ž×•×¤×¢×œ ×›×œ 25 ×¨×™×¦×•×ª gpt_c, ××• ×ž×¢×œ 15 ×¨×™×¦×•×ª gpt_c ×× ×¢×‘×¨×• 24 ×©×¢×•×ª ×ž××– ×”×¨×™×¦×” ×”××—×¨×•× ×”.
- ×ž×ª×ž×§×“ ×‘×¢×“×›×•×Ÿ ×”×§×•× ×¤×œ×™×§×˜ ×”×ž×¨×›×–×™ (primary_conflict) ×•×©×“×•×ª × ×•×¡×¤×™× ×œ×¤×™ ×”×¦×•×¨×š.
- ×©×•×œ×— ×œ-GPT ××ª ×”×”×™×¡×˜×•×¨×™×” ×•×”×¤×¨×•×¤×™×œ, ×ž×§×‘×œ ×©×“×•×ª ×—×“×©×™×/×ž×ª×•×§× ×™× ×‘×œ×‘×“.
- ×ž×¢×“×›×Ÿ Google Sheets, user_state, ×•×œ×•×’×™×.
"""

import logging
import asyncio
import json
import lazy_litellm as litellm
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import pytz

# ×™×™×‘×•× ×¤×•× ×§×¦×™×•×ª ×¢×–×¨
from utils import get_chat_history_messages, get_israel_time
from sheets_handler import get_user_summary, update_user_profile, get_user_state, reset_gpt_c_run_count
from prompts import build_profile_extraction_enhanced_prompt, JSON_RESPONSE_INSTRUCTION, JSON_EXAMPLE
from gpt_utils import normalize_usage_dict, safe_get_usage_value, measure_llm_latency, calculate_gpt_cost, extract_json_from_text
from config import GPT_MODELS, GPT_PARAMS

# ×”×’×“×¨×ª ×œ×•×’×¨
logger = logging.getLogger(__name__)

def should_run_gpt_e(chat_id: str, gpt_c_run_count: int, last_gpt_e_timestamp: Optional[str]) -> bool:
    """
    ×‘×•×“×§ ×”×× ×¦×¨×™×š ×œ×”×¤×¢×™×œ ××ª gpt_e ×œ×¤×™ ×”×ª× ××™× ×©×”×•×’×“×¨×•.
    
    :param chat_id: ×ž×–×”×” ×”×ž×©×ª×ž×©
    :param gpt_c_run_count: ×ž×¡×¤×¨ ×¨×™×¦×•×ª gpt_c ×ž××– ×”×¤×¢× ×”××—×¨×•× ×” ×©-gpt_e ×¨×¥
    :param last_gpt_e_timestamp: ×˜×™×™×ž×¡×˜×ž×¤ ×©×œ ×”×¨×™×¦×” ×”××—×¨×•× ×” ×©×œ gpt_e
    :return: True ×× ×¦×¨×™×š ×œ×”×¤×¢×™×œ gpt_e, False ××—×¨×ª
    """
    # ×ª× ××™ 1: ×”×’×¢× ×• ×œ-25 ×¨×™×¦×•×ª gpt_c
    if gpt_c_run_count >= 25:
        logger.info(f"[gpt_e] Triggering run - gpt_c_run_count >= 25 ({gpt_c_run_count})")
        return True
    
    # ×ª× ××™ 2: ×ž×¢×œ 15 ×¨×™×¦×•×ª gpt_c ×•×¢×‘×¨×• 24 ×©×¢×•×ª ×ž××– ×”×¨×™×¦×” ×”××—×¨×•× ×”
    if gpt_c_run_count > 15 and last_gpt_e_timestamp:
        try:
            last_run = datetime.fromisoformat(last_gpt_e_timestamp.replace('Z', '+00:00'))
            time_since_last_run = datetime.now(last_run.tzinfo) - last_run
            
            if time_since_last_run >= timedelta(hours=24):
                logger.info(f"[gpt_e] Triggering run - {gpt_c_run_count} gpt_c runs and {time_since_last_run.total_seconds()/3600:.1f} hours since last run")
                return True
        except Exception as e:
            logger.error(f"[gpt_e] Error parsing last_gpt_e_timestamp: {e}")
    
    return False

def build_fields_list():
    """×‘×•× ×” ×¨×©×™×ž×ª ×©×“×•×ª ×ž×•×ª×¨×™× ×žfields_dict.py"""
    from prompts import _get_filtered_profile_fields
    
    # ×¡×™× ×•×Ÿ ×©×“×•×ª ×œ× ×¨×œ×•×•× ×˜×™×™× ×œgpt_e
    relevant_fields = _get_filtered_profile_fields()
    
    # ×™×¦×™×¨×ª ×¨×©×™×ž×” ×‘×¤×•×¨×ž×˜ ×§×¨×™×
    fields_list = []
    for i in range(0, len(relevant_fields), 3):  # 3 ×©×“×•×ª ×‘×›×œ ×©×•×¨×”
        row = ", ".join(relevant_fields[i:i+3])
        fields_list.append(f"- {row}")
    
    return "\n".join(fields_list)

def prepare_gpt_e_prompt(chat_history: List[Dict], current_profile: str) -> str:
    """
    ×ž×›×™×Ÿ ××ª ×”×¤×¨×•×ž×¤×˜ ×œ-gpt_e ×¢× ×”×”×™×¡×˜×•×¨×™×” ×•×”×¤×¨×•×¤×™×œ ×”×§×™×™×.
    
    :param chat_history: ×¨×©×™×ž×ª ×”×•×“×¢×•×ª ×ž×”×©×™×—×”
    :param current_profile: ×”×¤×¨×•×¤×™×œ ×”×¨×’×©×™ ×”×§×™×™×
    :return: ×¤×¨×•×ž×¤×˜ ×ž×•×›×Ÿ ×œ×©×œ×™×—×” ×œ-GPT
    """
    # ×”×ž×¨×ª ×”×™×¡×˜×•×¨×™×” ×œ×¤×•×¨×ž×˜ ×ž×ª××™×
    formatted_history = []
    for msg in chat_history:
        if msg.get('role') in ['user', 'assistant']:
            formatted_history.append({
                'role': msg['role'],
                'content': msg.get('text', msg.get('content', ''))
            })
    
    # ×™×¦×™×¨×ª ×¤×¨×•×ž×¤×˜ ×ž×•×ª×× ×œ-gpt_e
    fields_list = build_fields_list()
    user_prompt = f"""
××ª×” ×ž×§×‘×œ ×”×™×¡×˜×•×¨×™×” ×©×œ ×©×™×—×” ×‘×™×Ÿ ×ž×©×ª×ž×© ×œ×‘×•×˜, ×•×¤×¨×•×¤×™×œ ×¨×’×©×™ ×§×™×™× ×©×œ ×”×ž×©×ª×ž×©.

×ž×˜×¨×ª×š: ×œ×–×”×•×ª ×ž×™×“×¢ ×—×“×©, ×œ×ª×§×Ÿ ×˜×¢×•×™×•×ª, ×•×œ×—×“×“ ××ª ×”×¤×¨×•×¤×™×œ ×”×¨×’×©×™.
×“×’×© ×ž×™×•×—×“ ×¢×œ ×¢×“×›×•×Ÿ ×”×©×“×” "primary_conflict" - ×”×§×•× ×¤×œ×™×§×˜ ×”×ž×¨×›×–×™ ×©×¢×ž×• ×”×ž×©×ª×ž×© ×ž×ª×ž×•×“×“ ×›×¨×’×¢.

×”×™×¡×˜×•×¨×™×™×ª ×”×©×™×—×” (50 ×”×•×“×¢×•×ª ××—×¨×•× ×•×ª):
{json.dumps(formatted_history, ensure_ascii=False, indent=2)}

×¤×¨×•×¤×™×œ ×¨×’×©×™ ×§×™×™×:
{current_profile}

×”× ×—×™×•×ª:
1. ×–×”×” ×ž×™×“×¢ ×—×“×© ×©×œ× × ×›×œ×œ ×‘×¤×¨×•×¤×™×œ ×”×§×™×™×
2. ×ª×§×Ÿ ×˜×¢×•×™×•×ª ××• ××™-×“×™×•×§×™× ×‘×¤×¨×•×¤×™×œ ×”×§×™×™×  
3. ×—×“×“ ×ž×™×“×¢ ×§×™×™× ×¢× ×¤×¨×˜×™× × ×•×¡×¤×™×
4. **×”×©×“×” primary_conflict ×”×•× ×§×¨×™×˜×™ - ×¢×“×›×Ÿ ××•×ª×• ×× ×™×© ×ž×™×“×¢ ×—×“×© ×¢×œ ×ž×” ×©×”×ž×©×ª×ž×© ×ž×ª×ž×•×“×“ ××™×ª×• ×›×¨×’×¢**
5. ×‘×“×•×§ ×× ×”×§×•× ×¤×œ×™×§×˜ ×”×ž×¨×›×–×™ ×”×©×ª× ×” ××• ×”×ª×¤×ª×— ×‘×”×•×“×¢×•×ª ×”××—×¨×•× ×•×ª
6. ×”×—×–×¨ ×¨×§ ×©×“×•×ª ×—×“×©×™×/×ž×ª×•×§× ×™× ×‘×¤×•×¨×ž×˜ JSON
7. ×× ××™×Ÿ ×ž×™×“×¢ ×—×“×© - ×”×—×–×¨ {{}}
8. ×× ×™×© ×ž×™×“×¢ ××™×©×™ ×©×œ× × ×›× ×¡ ×œ×©×•× ×©×“×” - ×”×•×¡×£ ×œ-"other_insights"

×©×“×•×ª ×ž×•×ª×¨×™×:
{fields_list}

×“×’×© ×ž×™×•×—×“: ×”×©×“×” "primary_conflict" ×¦×¨×™×š ×œ×©×§×£ ×‘×ž×“×•×™×§ ××ª ×ž×” ×©×”×ž×©×ª×ž×© ×ž×ª×ž×•×“×“ ××™×ª×• ×›×¨×’×¢ ×¢×œ ×‘×¡×™×¡ ×”×”×™×¡×˜×•×¨×™×” ×”××—×¨×•× ×”.

{JSON_RESPONSE_INSTRUCTION}
{JSON_EXAMPLE}
"""

    return user_prompt

async def run_gpt_e(chat_id: str) -> Dict[str, Any]:
    """
    ×ž×‘×¦×¢ ×—×™×“×•×“ ×•×ª×™×§×•×Ÿ ×¤×¨×•×¤×™×œ ×¨×’×©×™ ×œ×ž×©×ª×ž×© ×œ×¤×™ ×”×”× ×—×™×•×ª.
    
    :param chat_id: ×ž×–×”×” ×”×ž×©×ª×ž×©
    :return: ×ž×™×œ×•×Ÿ ×¢× ×ª×•×¦××•×ª ×”×¨×™×¦×” (success, changes, errors)
    """
    start_time = get_israel_time()
    logger.info(f"[gpt_e] Starting run_gpt_e for chat_id={chat_id} at {start_time.isoformat()}")
    
    result = {
        'success': False,
        'changes': {},
        'errors': [],
        'tokens_used': 0,
        'execution_time': 0
    }
    
    try:
        # ×©×œ×‘ 1: ×©×œ×™×¤×ª ×”×™×¡×˜×•×¨×™×™×ª ×©×™×—×”
        logger.info(f"[gpt_e] Fetching chat history for chat_id={chat_id}")
        chat_history = get_chat_history_messages(chat_id, limit=15)
        
        if not chat_history:
            result['errors'].append("No chat history found")
            logger.warning(f"[gpt_e] No chat history found for chat_id={chat_id}")
            return result
        
        logger.info(f"[gpt_e] Retrieved {len(chat_history)} messages from chat history")
        
        # ×©×œ×‘ 2: ×©×œ×™×¤×ª ×¤×¨×•×¤×™×œ ×§×™×™×
        logger.info(f"[gpt_e] Fetching current profile for chat_id={chat_id}")
        current_profile = get_user_summary(chat_id)
        
        # ×“×™×‘××’ ×ž×¤×•×¨×˜ ×©×œ ×”×¤×¨×•×¤×™×œ ×”× ×•×›×—×™
        print(f"[DEBUG] gpt_e_profile | chat={chat_id} | profile='{current_profile[:35]}{'...' if len(current_profile) > 35 else ''}'")
        
        if not current_profile:
            current_profile = "××™×Ÿ ×¤×¨×•×¤×™×œ ×§×™×™×"
            logger.info(f"[gpt_e] No existing profile found for chat_id={chat_id}")
        
        # ×©×œ×‘ 3: ×”×›× ×ª ×¤×¨×•×ž×¤×˜
        user_prompt = prepare_gpt_e_prompt(chat_history, current_profile)
        
        # ×©×œ×‘ 4: ×©×œ×™×—×” ×œ-GPT
        logger.info(f"[gpt_e] Sending request to GPT for chat_id={chat_id}")
        
        try:
            metadata = {"gpt_identifier": "gpt_e", "chat_id": chat_id}
            params = GPT_PARAMS["gpt_e"]
            model = GPT_MODELS["gpt_e"]
            
            completion_params = {
                "model": model,
                "messages": [
                    {"role": "system", "content": build_profile_extraction_enhanced_prompt()},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": params["temperature"],
                "max_tokens": params["max_tokens"],
                "metadata": metadata
            }
            
            with measure_llm_latency(model):
                response = litellm.completion(**completion_params)
            
            # ×—×™×œ×•×¥ ×”×ª×•×›×Ÿ ×ž×”×ª×’×•×‘×”
            content_raw = response.choices[0].message.content
            if content_raw is None:
                result['errors'].append("Empty response content from GPT")
                logger.error(f"[gpt_e] Empty response content from GPT for chat_id={chat_id}")
                return result
            
            content_raw = content_raw.strip()
            content = extract_json_from_text(content_raw)
            logger.info(f"[gpt_e] Received response from GPT: {content[:200] if content else 'None'}...")
            
            # × ×™×¨×ž×•×œ usage ×‘××•×¤×Ÿ ×‘×˜×•×—
            usage = normalize_usage_dict(response.usage, response.model)
            
            # ×—×™×œ×•×¥ cached_tokens ×‘×‘×˜×—×”
            cached_tokens = safe_get_usage_value(response.usage, 'cached_tokens', 0)
            if cached_tokens == 0 and hasattr(response.usage, 'prompt_tokens_details'):
                cached_tokens = safe_get_usage_value(response.usage.prompt_tokens_details, 'cached_tokens', 0)
            
            # ×¢×“×›×•×Ÿ ×”-usage ×¢× cached_tokens
            usage["cached_tokens"] = cached_tokens
            
            try:
                cost_info = calculate_gpt_cost(
                    prompt_tokens=usage.get("prompt_tokens", 0),
                    completion_tokens=usage.get("completion_tokens", 0),
                    cached_tokens=usage.get("cached_tokens", 0),
                    model_name=response.model,
                    completion_response=response
                )
                usage.update(cost_info)
            except Exception as _cost_e:
                logger.warning(f"[gpt_e] Cost calc failed: {_cost_e}")
            
            result['tokens_used'] = response.usage.total_tokens
            result['cost_data'] = usage
            
            try:
                from gpt_jsonl_logger import GPTJSONLLogger
                GPTJSONLLogger.log_gpt_call(
                    log_path="data/openai_calls.jsonl",
                    gpt_type="E",
                    request=completion_params if 'completion_params' in locals() else {},
                    response=response.model_dump() if 'response' in locals() and hasattr(response, 'model_dump') else {},
                    cost_usd=usage.get("cost_total", 0) if 'usage' in locals() else 0,
                    extra={"chat_id": chat_id}
                )
            except Exception as log_exc:
                print(f"[LOGGING_ERROR] Failed to log GPT-E call: {log_exc}")
            
        except Exception as e:
            result['errors'].append(f"GPT API error: {str(e)}")
            logger.error(f"[gpt_e] GPT API error for chat_id={chat_id}: {e}")
            return result
        
        if not content:
            result['errors'].append("Empty response from GPT")
            logger.error(f"[gpt_e] Empty response from GPT for chat_id={chat_id}")
            return result

        # ×©×œ×‘ 5: × ×™×ª×•×— ×”×ª×’×•×‘×”
        # × ×™×¡×™×•×Ÿ ×œ×—×œ×¥ JSON ×ž×”×ª×’×•×‘×”
        try:
            # ×—×™×¤×•×© JSON ×‘×ª×’×•×‘×”
            if content.startswith('{') and content.endswith('}'):
                try:
                    changes = json.loads(content)
                except json.JSONDecodeError as e:
                    logging.error(f"[GPT_E] JSON decode error: {e} | Content: {content[:200]}")
                    changes = {}
            else:
                # ×—×™×¤×•×© JSON ×‘×ª×•×š ×”×˜×§×¡×˜
                start_idx = content.find('{')
                end_idx = content.rfind('}') + 1
                if start_idx != -1 and end_idx > start_idx:
                    json_str = content[start_idx:end_idx]
                    try:
                        changes = json.loads(json_str)
                    except json.JSONDecodeError as e:
                        logging.error(f"[GPT_E] JSON decode error: {e} | Content: {json_str[:200]}")
                        changes = {}
                else:
                    changes = {}
            
            logger.info(f"[gpt_e] Parsed changes: {changes}")
            
            # ×©×œ×‘ 6: ×¢×“×›×•×Ÿ ×”×¤×¨×•×¤×™×œ ×× ×™×© ×©×™× ×•×™×™×
            if changes and isinstance(changes, dict):
                # ×”×¡×¨×ª ×©×“×•×ª ×¨×™×§×™×
                changes = {k: v for k, v in changes.items() if v and v != ""}
                
                if changes:
                    logger.info(f"[gpt_e] Updating profile with changes: {changes}")
                    # ×”×“×¤×¡×ª ×ž×™×“×¢ ×—×©×•×‘ ×¢×œ ×¢×“×›×•×Ÿ × ×ª×•× ×™× (×ª×ž×™×“ ×™×•×¤×™×¢!)
                    print(f"ðŸ”„ [GPT-E] ×ž×¢×“×›×Ÿ {len(changes)} ×©×“×•×ª: {list(changes.keys())}")
                    if 'cost_data' in result and result['cost_data']:
                        print(f"ðŸ’° [GPT-E] ×¢×œ×•×ª: {result['cost_data'].get('cost_total', 0):.6f}$ | ×˜×•×§× ×™×: {result['cost_data'].get('total_tokens', 0)}")
                    
                    # ×”×“×’×©×” ×ž×™×•×—×“×ª ×œ×¢×“×›×•×Ÿ primary_conflict
                    if 'primary_conflict' in changes:
                        print(f"ðŸŽ¯ [GPT-E] ×¢×“×›×•×Ÿ ×§×•× ×¤×œ×™×§×˜ ×ž×¨×›×–×™: {changes['primary_conflict'][:100]}...")
                    
                    # ðŸ”§ ×”×¡×¨×”: ×œ× ×ž×¢×“×›× ×™× ×›××Ÿ ×›×“×™ ×œ×ž× ×•×¢ ×›×¤×™×œ×•×ª ×¢× _handle_profile_updates
                    # await update_user_profile(chat_id, changes)
                    
                    result['changes'] = changes
                    logger.info(f"[gpt_e] Changes prepared for profile update: {changes}")
                    print(f"âœ… [GPT-E] ×”×©×™× ×•×™×™× ×ž×•×›× ×™× ×œ×¢×“×›×•×Ÿ ×¤×¨×•×¤×™×œ")
                else:
                    logger.info(f"[gpt_e] No meaningful changes found for chat_id={chat_id}")
            else:
                logger.info(f"[gpt_e] No changes to apply for chat_id={chat_id}")
                
        except Exception as e:
            result['errors'].append(f"Failed to parse changes: {e}")
            logger.error(f"[gpt_e] Error parsing changes for chat_id={chat_id}: {e}")
            return result
        
        # ×©×œ×‘ 7: ×¢×“×›×•×Ÿ ×¡×˜×˜×™×¡×˜×™×§×•×ª
        result['success'] = True
        result['execution_time'] = (get_israel_time() - start_time).total_seconds()
        
        logger.info(f"[gpt_e] Completed successfully for chat_id={chat_id} in {result['execution_time']:.2f}s")
        
    except Exception as e:
        result['errors'].append(f"Unexpected error: {str(e)}")
        logger.error(f"[gpt_e] Unexpected error for chat_id={chat_id}: {e}", exc_info=True)
    
    return result

async def update_user_state_after_gpt_e(chat_id: str, result: Dict[str, Any]) -> None:
    """
    ×ž×¢×“×›×Ÿ ××ª ×ž×¦×‘ ×”×ž×©×ª×ž×© ×œ××—×¨ ×¨×™×¦×ª gpt_e.
    
    :param chat_id: ×ž×–×”×” ×”×ž×©×ª×ž×©
    :param result: ×ª×•×¦××•×ª ×”×¨×™×¦×”
    """
    try:
        if result['success']:
            # ××™×¤×•×¡ ×ž×•× ×” gpt_c_run_count ×•×¢×“×›×•×Ÿ last_gpt_e_timestamp
            success = reset_gpt_c_run_count(chat_id)
            
            if success:
                logger.info(f"[gpt_e] Updated user state for chat_id={chat_id} after successful run")
            else:
                logger.error(f"[gpt_e] Failed to update user state for chat_id={chat_id}")
        else:
            logger.warning(f"[gpt_e] Not updating user state for chat_id={chat_id} due to failed run")
        
    except Exception as e:
        logger.error(f"[gpt_e] Error updating user state for chat_id={chat_id}: {e}")

def log_gpt_e_run(chat_id: str, result: Dict[str, Any]) -> None:
    """
    ×ž×ª×¢×“ ××ª ×¨×™×¦×ª gpt_e ×‘×œ×•×’×™×.
    
    :param chat_id: ×ž×–×”×” ×”×ž×©×ª×ž×©
    :param result: ×ª×•×¦××•×ª ×”×¨×™×¦×”
    """
    log_entry = {
        'timestamp': get_israel_time().isoformat(),
        'chat_id': chat_id,
        'success': result['success'],
        'changes_count': len(result.get('changes', {})),
        'tokens_used': result.get('tokens_used', 0),
        'execution_time': result.get('execution_time', 0),
        'errors': result.get('errors', [])
    }
    
    if result.get('changes'):
        log_entry['changes'] = result['changes']
    
    logger.info(f"[gpt_e] Run log: {json.dumps(log_entry, ensure_ascii=False)}")

async def execute_gpt_e_if_needed(chat_id: str) -> Optional[Dict[str, Any]]:
    """
    ×‘×•×“×§ ×× ×¦×¨×™×š ×œ×”×¤×¢×™×œ gpt_e (×›×œ 10 ×”×•×“×¢×•×ª ×ž×©×ª×ž×©) ×•×ž×¤×¢×™×œ ×× ×›×Ÿ.
    :param chat_id: ×ž×–×”×” ×”×ž×©×ª×ž×©
    :return: ×ª×•×¦××•×ª ×”×¨×™×¦×” ×× ×”×•×¤×¢×œ, None ×× ×œ× ×”×•×¤×¢×œ
    """
    from chat_utils import get_user_stats_and_history
    logger.info(f"[gpt_e] Checking if should run for chat_id={chat_id} (every 10 user messages)")
    try:
        stats, _ = get_user_stats_and_history(chat_id)
        total_messages = stats.get("total_messages", 0)
        if total_messages > 0 and total_messages % 10 == 0:
            logger.info(f"[gpt_e] Triggered for chat_id={chat_id} (user sent {total_messages} messages)")
            result = await run_gpt_e(chat_id)
            if result['success']:
                await update_user_state_after_gpt_e(chat_id, result)
            log_gpt_e_run(chat_id, result)
            return result
        else:
            logger.info(f"[gpt_e] Not running for chat_id={chat_id} (user sent {total_messages} messages)")
            return None
    except Exception as e:
        logger.error(f"[gpt_e] Error in execute_gpt_e_if_needed: {e}")
        return None 