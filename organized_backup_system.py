#!/usr/bin/env python3
"""
ğŸ—‚ï¸ ××¢×¨×›×ª ×’×™×‘×•×™ ××¡×•×“×¨×ª ×‘×¨××ª OCD
×™×•×¦×¨×ª ××‘× ×” ×ª×™×§×™×•×ª ××•×©×œ× ×¢× ×§×‘×¦×™× ×™×•××™×™× ×œ×›×œ ×˜×‘×œ×”
×›×œ ×˜×‘×œ×” ×‘×ª×™×§×™×” × ×¤×¨×“×ª ×¢× ×§×‘×¦×™ JSON ×™×•××™×™×
"""

import os
import json
import psycopg2
from datetime import datetime, timedelta
from decimal import Decimal
from config import config
from simple_logger import logger
from admin_notifications import send_admin_notification

DB_URL = config.get("DATABASE_EXTERNAL_URL") or config.get("DATABASE_URL")

# ğŸ¯ ×”×’×“×¨×•×ª ×’×™×‘×•×™ ××¡×•×“×¨
BACKUP_ROOT = "backups/organized_backups"
TABLES_CONFIG = {
    "user_profiles": {
        "folder": "user_profile_backup",
        "filename_prefix": "user_profile_backup"
    },
    "chat_messages": {
        "folder": "chat_history_backup", 
        "filename_prefix": "chat_history_backup"
    },
    "gpt_calls_log": {
        "folder": "gpt_calls_backup",
        "filename_prefix": "gpt_calls_backup"
    }
}

def setup_organized_backup_structure():
    """×™×•×¦×¨ ××ª ××‘× ×” ×”×ª×™×§×™×•×ª ×”××¡×•×“×¨"""
    try:
        # ×™×¦×™×¨×ª ×ª×™×§×™×™×ª ×©×•×¨×©
        os.makedirs(BACKUP_ROOT, exist_ok=True)
        
        # ×™×¦×™×¨×ª ×ª×™×§×™×” ×œ×›×œ ×˜×‘×œ×”
        for table_name, config in TABLES_CONFIG.items():
            folder_path = os.path.join(BACKUP_ROOT, config["folder"])
            os.makedirs(folder_path, exist_ok=True)
            logger.info(f"ğŸ“ ×ª×™×§×™×” ××•×›× ×”: {folder_path}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ×©×’×™××” ×‘×™×¦×™×¨×ª ××‘× ×” ×ª×™×§×™×•×ª: {e}")
        return False

def safe_json_serializer(obj):
    """×××™×¨ ××•×‘×™×™×§×˜×™× ×œJSON ×‘×¦×•×¨×” ×‘×˜×•×—×”"""
    if isinstance(obj, Decimal):
        return float(obj)
    elif isinstance(obj, datetime):
        return obj.isoformat()
    elif hasattr(obj, 'isoformat'):
        return obj.isoformat()
    else:
        return str(obj)

def backup_table_to_organized_file(table_name, backup_date):
    """××’×‘×” ×˜×‘×œ×” ×œ×§×•×‘×¥ JSON ××¡×•×“×¨"""
    try:
        if table_name not in TABLES_CONFIG:
            logger.error(f"âŒ ×˜×‘×œ×” {table_name} ×œ× ××•×’×“×¨×ª ×‘×ª×¦×•×¨×”")
            return None
        
        config_data = TABLES_CONFIG[table_name]
        folder_path = os.path.join(BACKUP_ROOT, config_data["folder"])
        filename = f"{config_data['filename_prefix']}_{backup_date}.json"
        file_path = os.path.join(folder_path, filename)
        
        # ×—×™×‘×•×¨ ×œ××¡×“ × ×ª×•× ×™×
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()
        
        # ×©×œ×™×¤×ª ×”× ×ª×•× ×™×
        cur.execute(f"SELECT * FROM {table_name}")
        rows = cur.fetchall()
        
        # ×©×œ×™×¤×ª ×©××•×ª ×¢××•×“×•×ª
        cur.execute(f"""
            SELECT column_name FROM information_schema.columns 
            WHERE table_name = '{table_name}' AND table_schema = 'public'
            ORDER BY ordinal_position
        """)
        columns = [row[0] for row in cur.fetchall()]
        
        # ×”××¨×” ×œ×¨×©×™××ª ××™×œ×•× ×™×
        data_list = []
        for row in rows:
            row_dict = {}
            for i, column in enumerate(columns):
                row_dict[column] = row[i]
            data_list.append(row_dict)
        
        # ×™×¦×™×¨×ª ××˜×-××™×“×¢
        backup_time = datetime.now()
        metadata = {
            "backup_date": backup_date,
            "backup_timestamp": backup_time.isoformat(),
            "table_name": table_name,
            "records_count": len(data_list),
            "columns": columns,
            "backup_system": "organized_backup_v2",
            "confirmation_code": f"BK-{table_name.upper()[:3]}-{backup_date}-{len(data_list):04d}"
        }
        
        # ×”×›× ×ª ×”××‘× ×” ×”×¡×•×¤×™
        backup_structure = {
            "metadata": metadata,
            "data": data_list
        }
        
        # ×©××™×¨×” ×œ×§×•×‘×¥
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(backup_structure, f, ensure_ascii=False, indent=2, default=safe_json_serializer)
        
        # ×§×‘×œ×ª ×’×•×“×œ ×§×•×‘×¥
        file_size = os.path.getsize(file_path)
        
        cur.close()
        conn.close()
        
        backup_info = {
            "table_name": table_name,
            "file_path": file_path,
            "records_count": len(data_list),
            "file_size_bytes": file_size,
            "file_size_mb": file_size / 1024 / 1024,
            "confirmation_code": metadata["confirmation_code"],
            "backup_timestamp": backup_time
        }
        
        logger.info(f"âœ… {table_name}: {len(data_list)} ×¨×©×•××•×ª â†’ {file_size/1024:.1f}KB")
        return backup_info
        
    except Exception as e:
        logger.error(f"âŒ ×©×’×™××” ×‘×’×™×‘×•×™ {table_name}: {e}")
        return None

def run_organized_backup():
    """××¨×™×¥ ×’×™×‘×•×™ ××¡×•×“×¨ ××œ×"""
    try:
        backup_date = datetime.now().strftime("%d_%m_%Y")
        logger.info(f"ğŸ—‚ï¸ ××ª×—×™×œ ×’×™×‘×•×™ ××¡×•×“×¨ ×œ×ª××¨×™×š {backup_date}")
        
        # ×”×›× ×ª ××‘× ×” ×ª×™×§×™×•×ª
        if not setup_organized_backup_structure():
            return False
        
        # ×’×™×‘×•×™ ×›×œ ×˜×‘×œ×”
        backup_results = {}
        total_records = 0
        total_size_bytes = 0
        
        for table_name in TABLES_CONFIG.keys():
            backup_info = backup_table_to_organized_file(table_name, backup_date)
            
            if backup_info:
                backup_results[table_name] = backup_info
                total_records += backup_info["records_count"]
                total_size_bytes += backup_info["file_size_bytes"]
        
        # ×‘×“×™×§×ª ×”×¦×œ×—×”
        if len(backup_results) == len(TABLES_CONFIG):
            logger.info(f"ğŸ‰ ×’×™×‘×•×™ ××¡×•×“×¨ ×”×•×©×œ×: {total_records} ×¨×©×•××•×ª ×‘-{len(backup_results)} ×§×‘×¦×™×")
            
            # ×”×©×•×•××” ×œ×™×•× ×§×•×“×
            yesterday_comparison = compare_with_yesterday(backup_date, backup_results)
            
            # ×©×œ×™×—×ª ×”×ª×¨××” ××¤×•×¨×˜×ª
            send_detailed_organized_backup_notification(backup_results, total_records, total_size_bytes, yesterday_comparison)
            
            return True
        else:
            logger.error(f"âŒ ×’×™×‘×•×™ ××¡×•×“×¨ × ×›×©×œ: {len(backup_results)}/{len(TABLES_CONFIG)} ×˜×‘×œ××•×ª")
            return False
        
    except Exception as e:
        logger.error(f"âŒ ×©×’×™××” ×‘×’×™×‘×•×™ ××¡×•×“×¨: {e}")
        return False

def compare_with_yesterday(today_date, today_results):
    """××©×•×•×” ××ª ×”×’×™×‘×•×™ ×©×œ ×”×™×•× ×¢× ×××©"""
    try:
        yesterday = (datetime.now() - timedelta(days=1)).strftime("%d_%m_%Y")
        comparison = {}
        
        for table_name, today_info in today_results.items():
            config_data = TABLES_CONFIG[table_name]
            folder_path = os.path.join(BACKUP_ROOT, config_data["folder"])
            yesterday_filename = f"{config_data['filename_prefix']}_{yesterday}.json"
            yesterday_file_path = os.path.join(folder_path, yesterday_filename)
            
            if os.path.exists(yesterday_file_path):
                try:
                    with open(yesterday_file_path, 'r', encoding='utf-8') as f:
                        yesterday_data = json.load(f)
                    
                    yesterday_records = yesterday_data["metadata"]["records_count"]
                    yesterday_size = os.path.getsize(yesterday_file_path)
                    
                    comparison[table_name] = {
                        "yesterday_records": yesterday_records,
                        "today_records": today_info["records_count"],
                        "records_diff": today_info["records_count"] - yesterday_records,
                        "yesterday_size_mb": yesterday_size / 1024 / 1024,
                        "today_size_mb": today_info["file_size_mb"],
                        "size_diff_mb": today_info["file_size_mb"] - (yesterday_size / 1024 / 1024)
                    }
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×§×¨×•× ×’×™×‘×•×™ ×©×œ ×××© ×¢×‘×•×¨ {table_name}: {e}")
            else:
                comparison[table_name] = {
                    "yesterday_records": 0,
                    "today_records": today_info["records_count"],
                    "records_diff": today_info["records_count"],
                    "yesterday_size_mb": 0,
                    "today_size_mb": today_info["file_size_mb"],
                    "size_diff_mb": today_info["file_size_mb"]
                }
        
        return comparison
        
    except Exception as e:
        logger.warning(f"âš ï¸ ×©×’×™××” ×‘×”×©×•×•××” ×¢× ×××©: {e}")
        return {}

def send_detailed_organized_backup_notification(backup_results, total_records, total_size_bytes, yesterday_comparison):
    """×©×•×œ×— ×”×ª×¨××” ××¤×•×¨×˜×ª ×¢×œ ×”×’×™×‘×•×™ ×”××¡×•×“×¨"""
    try:
        backup_time = datetime.now()
        
        # ×›×•×ª×¨×ª ×”×”×•×“×¢×”
        notification = f"ğŸ—‚ï¸ **×’×™×‘×•×™ ××¡×•×“×¨ ×”×•×©×œ× ×‘×”×¦×œ×—×”**\n\n"
        notification += f"ğŸ“… **×ª××¨×™×š:** {backup_time.strftime('%d/%m/%Y')}\n"
        notification += f"ğŸ• **×©×¢×”:** {backup_time.strftime('%H:%M:%S')}\n"
        notification += f"ğŸ“Š **×¡×”\"×› ×¨×©×•××•×ª:** {total_records:,}\n"
        notification += f"ğŸ’¾ **×¡×”\"×› ×’×•×“×œ:** {total_size_bytes/1024/1024:.2f} MB\n"
        notification += f"ğŸ“ **××¡×¤×¨ ×§×‘×¦×™×:** {len(backup_results)}\n\n"
        
        # ×¤×™×¨×•×˜ ×œ×›×œ ×˜×‘×œ×”
        notification += f"ğŸ“‹ **×¤×™×¨×•×˜ ××¤×•×¨×˜:**\n"
        for table_name, info in backup_results.items():
            notification += f"\nğŸ”¹ **{table_name}:**\n"
            notification += f"   ğŸ“Š ×¨×©×•××•×ª: {info['records_count']:,}\n"
            notification += f"   ğŸ’¾ ×’×•×“×œ: {info['file_size_mb']:.2f} MB\n"
            notification += f"   ğŸ”’ ×§×•×“ ××™×©×•×¨: `{info['confirmation_code']}`\n"
            notification += f"   ğŸ“ ×§×•×‘×¥: `{os.path.basename(info['file_path'])}`\n"
            
            # ×”×©×•×•××” ×¢× ×××©
            if table_name in yesterday_comparison:
                comp = yesterday_comparison[table_name]
                records_change = comp["records_diff"]
                size_change = comp["size_diff_mb"]
                
                if records_change > 0:
                    notification += f"   ğŸ“ˆ ×©×™× ×•×™: +{records_change} ×¨×©×•××•×ª (+{size_change:.2f} MB)\n"
                elif records_change < 0:
                    notification += f"   ğŸ“‰ ×©×™× ×•×™: {records_change} ×¨×©×•××•×ª ({size_change:.2f} MB)\n"
                else:
                    notification += f"   â– ××™×Ÿ ×©×™× ×•×™ ××××©\n"
        
        # ××™×§×•× ×”×§×‘×¦×™×
        notification += f"\nğŸ“‚ **××™×§×•× ×”×§×‘×¦×™×:**\n"
        notification += f"```\n{BACKUP_ROOT}/\n"
        for table_name, config_data in TABLES_CONFIG.items():
            notification += f"â”œâ”€â”€ {config_data['folder']}/\n"
        notification += f"```\n"
        
        # ×©××™×¨×” ×œ-30 ×™××™×
        notification += f"\nğŸ—“ï¸ **××“×™× ×™×•×ª ×©××™×¨×”:** 30 ×™××™× ××—×¨×•× ×™×\n"
        notification += f"ğŸ§¹ **× ×™×§×•×™ ××•×˜×•××˜×™:** ×§×‘×¦×™× ×™×©× ×™× ×-30 ×™××™×"
        
        send_admin_notification(notification)
        
    except Exception as e:
        logger.error(f"âŒ ×©×’×™××” ×‘×©×œ×™×—×ª ×”×ª×¨××” ××¤×•×¨×˜×ª: {e}")

def list_organized_backups():
    """××¦×™×’ ×¨×©×™××ª ×’×™×‘×•×™×™× ××¡×•×“×¨×™×"""
    try:
        if not os.path.exists(BACKUP_ROOT):
            print("ğŸ“­ ××™×Ÿ ×ª×™×§×™×™×ª ×’×™×‘×•×™×™× ××¡×•×“×¨×™×")
            return
        
        print("ğŸ—‚ï¸ ×’×™×‘×•×™×™× ××¡×•×“×¨×™× ×–××™× ×™×:")
        print("=" * 60)
        
        # ×¢×‘×•×¨ ×›×œ ×ª×™×§×™×™×ª ×˜×‘×œ×”
        for table_name, config_data in TABLES_CONFIG.items():
            folder_path = os.path.join(BACKUP_ROOT, config_data["folder"])
            
            if not os.path.exists(folder_path):
                continue
            
            print(f"\nğŸ“‚ {config_data['folder']}/")
            
            # ×§×¨×™××ª ×›×œ ×”×§×‘×¦×™× ×‘×ª×™×§×™×”
            files = [f for f in os.listdir(folder_path) if f.endswith('.json')]
            files.sort(reverse=True)  # ×”×›×™ ×—×“×©×™× ×§×•×“×
            
            if not files:
                print("   ğŸ“­ ××™×Ÿ ×§×‘×¦×™ ×’×™×‘×•×™")
                continue
            
            for filename in files[:10]:  # ×”×¦×’ ×¢×“ 10 ××—×¨×•× ×™×
                file_path = os.path.join(folder_path, filename)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    metadata = data.get("metadata", {})
                    records_count = metadata.get("records_count", 0)
                    backup_date = metadata.get("backup_date", "unknown")
                    confirmation_code = metadata.get("confirmation_code", "N/A")
                    
                    file_size = os.path.getsize(file_path)
                    
                    print(f"   ğŸ“„ {filename}")
                    print(f"      ğŸ“… {backup_date} | ğŸ“Š {records_count:,} ×¨×©×•××•×ª | ğŸ’¾ {file_size/1024:.1f}KB")
                    print(f"      ğŸ”’ {confirmation_code}")
                    
                except Exception as e:
                    print(f"   âŒ ×©×’×™××” ×‘×§×¨×™××ª {filename}: {e}")
            
            if len(files) > 10:
                print(f"   ... ×•×¢×•×“ {len(files) - 10} ×§×‘×¦×™×")
        
    except Exception as e:
        print(f"âŒ ×©×’×™××” ×‘×¦×¤×™×™×” ×‘×’×™×‘×•×™×™×: {e}")
        logger.error(f"âŒ ×©×’×™××” ×‘×¦×¤×™×™×” ×‘×’×™×‘×•×™×™×: {e}")

def cleanup_old_organized_backups(days_to_keep=30):
    """×× ×§×” ×’×™×‘×•×™×™× ××¡×•×“×¨×™× ×™×©× ×™×"""
    try:
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        deleted_files = 0
        
        for table_name, config_data in TABLES_CONFIG.items():
            folder_path = os.path.join(BACKUP_ROOT, config_data["folder"])
            
            if not os.path.exists(folder_path):
                continue
            
            files = [f for f in os.listdir(folder_path) if f.endswith('.json')]
            
            for filename in files:
                file_path = os.path.join(folder_path, filename)
                
                try:
                    # ×‘×“×™×§×ª ×ª××¨×™×š ×”×§×•×‘×¥
                    file_date_str = filename.split('_')[-3:]  # dd_mm_yyyy.json
                    if len(file_date_str) == 3:
                        file_date_str = '_'.join(file_date_str).replace('.json', '')
                        file_date = datetime.strptime(file_date_str, "%d_%m_%Y")
                        
                        if file_date < cutoff_date:
                            os.remove(file_path)
                            deleted_files += 1
                            logger.info(f"ğŸ—‘ï¸ × ××—×§ ×§×•×‘×¥ ×™×©×Ÿ: {filename}")
                
                except Exception as e:
                    logger.warning(f"âš ï¸ ×©×’×™××” ×‘×‘×“×™×§×ª ×ª××¨×™×š {filename}: {e}")
        
        if deleted_files > 0:
            logger.info(f"ğŸ§¹ × ××—×§×• {deleted_files} ×§×‘×¦×™ ×’×™×‘×•×™ ×™×©× ×™×")
            
            send_admin_notification(
                f"ğŸ§¹ **× ×™×§×•×™ ×’×™×‘×•×™×™× ××¡×•×“×¨×™×**\n\n" +
                f"ğŸ—‘ï¸ **× ××—×§×•:** {deleted_files} ×§×‘×¦×™×\n" +
                f"ğŸ“… **×™×©× ×™× ×:** {cutoff_date.strftime('%d/%m/%Y')}\n" +
                f"ğŸ’¾ **×©××™×¨×ª:** {days_to_keep} ×™××™× ××—×¨×•× ×™×"
            )
        else:
            logger.info("ğŸ§¹ ××™×Ÿ ×§×‘×¦×™ ×’×™×‘×•×™ ×™×©× ×™× ×œ××—×™×§×”")
        
    except Exception as e:
        logger.error(f"âŒ ×©×’×™××” ×‘× ×™×§×•×™ ×’×™×‘×•×™×™×: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "backup":
            run_organized_backup()
        elif command == "list":
            list_organized_backups()
        elif command == "cleanup":
            days = int(sys.argv[2]) if len(sys.argv) > 2 else 30
            cleanup_old_organized_backups(days)
        else:
            print("×©×™××•×©: python organized_backup_system.py [backup|list|cleanup]")
    else:
        # ×’×™×‘×•×™ ×¨×’×™×œ
        run_organized_backup() 