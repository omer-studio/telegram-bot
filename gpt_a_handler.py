"""
gpt_a_handler.py
----------------
×× ×•×¢ gpt_a: ×œ×•×’×™×§×ª ×”×ª×©×•×‘×” ×”×¨××©×™×ª (main response logic)
×¢× ×× ×’× ×•×Ÿ ×”×•×“×¢×” ×–×× ×™×ª ×•×¤×™×œ×˜×¨ ×—×›× ×œ×‘×—×™×¨×ª ××•×“×œ
"""

import logging
from datetime import datetime
import json
import litellm
import asyncio
import threading
import time
import re
from prompts import SYSTEM_PROMPT
from config import GPT_MODELS, GPT_PARAMS, GPT_FALLBACK_MODELS
from gpt_utils import normalize_usage_dict, billing_guard, measure_llm_latency, calculate_gpt_cost
from notifications import alert_billing_issue, send_error_notification
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from message_handler import format_text_for_telegram  # for type checkers only
# ××¢×¨×›×ª ×‘×™×¦×•×¢×™× ××‘×•×˜×œ×ª ×–×× ×™×ª

# ×™×™×‘×•× ×”×¤×™×œ×˜×¨ ×”×—×›×
# ===============================
# ğŸ¯ ×¤×™×œ×˜×¨ ×—×›× ×œ×‘×—×™×¨×ª ××•×“×œ AI
# ===============================

# ×¡×£ ××™×œ×™× ×œ××•×“×œ ××ª×§×“×
LONG_MESSAGE_THRESHOLD = 50  # ××¢×œ 50 ××™×œ×™× = ××•×“×œ ××ª×§×“×

def create_missing_fields_system_message(chat_id: str) -> str:
    """×™×•×¦×¨ system message ×—×›× ×¢× ×©×“×•×ª ×—×¡×¨×™× ×©×›×“××™ ×œ×©××•×œ ×¢×œ×™×”×"""
    try:
        from sheets_core import get_user_state
        from fields_dict import FIELDS_DICT
        
        profile_data = get_user_state(chat_id).get("profile_data", {})
        
        key_fields = ["age", "attracted_to", "relationship_type", "self_religious_affiliation", 
                     "closet_status", "pronoun_preference", "occupation_or_role", 
                     "self_religiosity_level", "primary_conflict", "goal_in_course"]
        
        missing = [FIELDS_DICT[f]["missing_question"] for f in key_fields
                  if f in FIELDS_DICT and not str(profile_data.get(f, "")).strip() 
                  and FIELDS_DICT[f].get("missing_question", "").strip()]
        
        if len(missing) >= 2:
            missing_text = ', '.join(missing[:4])
            return f"""×¤×¨×˜×™× ×©×”××©×ª××© ×¢×“×™×™×Ÿ ×œ× ×¡×™×¤×¨ ×œ×š ×•×›×“××™ ×œ×©××•×œ ××•×ª×• ×‘×¢×“×™× ×•×ª ×•×‘×¨×’×™×©×•×ª ×‘××˜×¨×” ×œ×”×›×™×¨ ××•×ª×• ×™×•×ª×¨ ×˜×•×‘: {missing_text}

×ª×¡×‘×™×¨ ×œ×• ××ª ×”×¨×¦×™×•× ×œ, ×ª×¡×‘×™×¨ ×œ×• ×œ××” ××ª×” ×©×•××œ, ×ª×’×™×“ ×œ×• ×©×—×©×•×‘ ×œ×š ×œ×”×›×™×¨ ××•×ª×• ×›×“×™ ×œ×”×ª××™× ××ª ×¢×¦××š ××œ×™×•. ×ª×ª×¢× ×™×™×Ÿ ×‘×• - ×ª×‘×—×¨ ××—×“ ××”×©××œ×•×ª ×©× ×¨××™×ª ×œ×š ×”×›×™ ××ª××™××” - ×•×¨×§ ×× ×–×” ××¨×’×™×© ×œ×š ××ª××™× ××– ×ª×©××œ ××•×ª×• ×‘×¢×“×™× ×•×ª ×•×‘×¨×’×™×©×•×ª ×•×ª×©×œ×‘ ××ª ×–×” ×‘××œ×’× ×˜×™×•×ª. (××ª ×”×©××œ×•×ª ×ª×¢×©×” ×‘×›×ª×‘ ××•×“×’×©)"""
        return ""
        
    except Exception as e:
        logging.error(f"×©×’×™××” ×‘×™×¦×™×¨×ª ×”×•×“×¢×ª ×©×“×•×ª ×—×¡×¨×™×: {e}")
        return ""

# ××™×œ×•×ª ××¤×ª×— ×©××¦×“×™×§×•×ª ××•×“×œ ××ª×§×“×
PREMIUM_MODEL_KEYWORDS = [
    # ×–×•×’×™×•×ª ×•××¢×¨×›×•×ª ×™×—×¡×™×
    "× ×™×©×•××™×Ÿ", "×—×ª×•× ×”", "×–×•×’×™×•×ª", "××¢×¨×›×ª ×™×—×¡×™×", "×‘×Ÿ ×–×•×’", "×‘×ª ×–×•×’", "×—×‘×¨×”", "×—×‘×¨",
    "××”×‘×”", "×¨×’×©×•×ª", "×§×©×¨", "×–×™×§×”", "××©×™×›×”", "××™× ×˜×™××™×•×ª", "××™× ×™×•×ª", "×¤×¨×™×“×”", "×’×™×¨×•×©×™×Ÿ",
    "×’×¨×•×©", "× ×¤×¨×“", "× ×©×•×™", "× ×©×•××”", "××’×•×¨×©×ª", "××’×•×¨×©",
    
    # ×¤×¡×™×›×•×œ×•×’×™×” ×•×‘×¨×™××•×ª × ×¤×©
    "×¤×¡×™×›×•×œ×•×’×™×”", "×˜×™×¤×•×œ", "×™×™×¢×•×¥", "×¤×¡×™×›×•×œ×•×’", "××˜×¤×œ", "××“×›×", "×“×™×›××•×Ÿ", "×—×¨×“×”", 
    "×¤×—×“", "×“××’×”", "×‘×œ×‘×•×œ", "×œ×—×¥", "×¡×˜×¨×¡", "×˜×¨××•××”", "×¤×¦×™×¢×” × ×¤×©×™×ª", "×‘×“×™×“×•×ª",
    "×›××‘", "××•×‘×“×Ÿ", "×”×ª×¢×œ×œ×•×ª", "× ×¤×’×¢", "×¡×‘×œ", "×‘×¨×™×•× ×•×ª", "××©××”", "×‘×•×©×”",
    "×ª×§×•×¢", "×©×™×¤×•×˜ ×¢×¦××™", "×”×¡×ª×¨×”", "××•×¨×›×‘", "×§×©×” ×œ×™", "×œ× ×©×œ×", "××‘×•×“",
    
    # ×“×ª×™×•×ª ×•×××•× ×”
    "×“×ª×™×•×ª", "×—×™×œ×•× ×™", "×“×ª×™", "××¡×•×¨×ª×™", "×××•× ×”", "××¦×•×•×ª", "×”×œ×›×”", "×¨×‘", "×¨×‘× ×•×ª",
    "×›×©×¨×•×ª", "×©×‘×ª", "×—×’", "×ª×¤×™×œ×”", "×‘×™×ª ×›× ×¡×ª", "×ª×•×¨×”", "×ª×œ××•×“", "×™×”×“×•×ª",
    "×“×ª×œ×©", "×“×ª×œ×´×©", "×“×ª×™ ×œ×©×¢×‘×¨", "×—×–×¨×” ×‘×©××œ×”", "×™×©×™×‘×”", "×˜×™×¤×•×œ×™ ×”××¨×”",
    
    # ××©×¤×—×” ×•×—×™×™ ×—×‘×¨×”
    "××©×¤×—×”", "×”×•×¨×™×", "×™×œ×“×™×", "×”×¨×™×•×Ÿ", "×œ×™×“×”", "×—×™× ×•×š", "×’×™×œ", "×–×§× ×”", "×¡×‘×", "×¡×‘×ª×",
    "××‘×", "×××", "×‘×Ÿ", "×‘×ª", "××—", "××—×•×ª", "×“×•×“", "×“×•×“×”", "×‘×Ÿ ×“×•×“", "×‘×ª ×“×•×“",
    "××©×¤×—×” ××•×¨×—×‘×ª", "××©×¤×—×” ×‘×™×•×œ×•×’×™×ª", "× ×›×“×™×", "×”×”×•×¨×™×", "×—×™×™× ×›×¤×•×œ×™×",
    
    # ×¢×‘×•×“×” ×•×§×¨×™×™×¨×”
    "×¢×‘×•×“×”", "×§×¨×™×™×¨×”", "×”×©×›×œ×”", "×œ×™××•×“×™×", "××•× ×™×‘×¨×¡×™×˜×”", "××§×¦×•×¢", "×›×œ×›×œ×”", "×©×›×¨",
    "×× ×”×œ", "×¢×•×‘×“", "××¢×¡×™×§", "×¨××™×•×Ÿ ×¢×‘×•×“×”", "×§×•×¨×•×ª ×—×™×™×", "×”×©×›×œ×” ×’×‘×•×”×”",
    
    # ×‘×¨×™××•×ª ×¨×¤×•××™×ª
    "×‘×¨×™××•×ª", "×¨×•×¤×", "×—×•×œ×”", "××—×œ×”", "×ª×¨×•×¤×”", "× ×™×ª×•×—", "×‘×™×ª ×—×•×œ×™×", "×§×•×¤×ª ×—×•×œ×™×",
    "×›××‘", "×›×•××‘", "×¨×¤×•××”", "××‘×—×•×Ÿ", "×˜×™×¤×•×œ ×¨×¤×•××™", "××—×œ×” ×›×¨×•× ×™×ª", "××™×™×“×¡", "HIV", "××—×œ×•×ª",
    
    # ×”×—×œ×˜×•×ª ×•×“×™×œ××•×ª
    "×‘×¢×™×”", "×§×•×©×™", "×”×—×œ×˜×”", "×“×™×œ××”", "×‘×¨×™×¨×”", "××¤×©×¨×•×ª", "×¢×ª×™×“", "×ª×›× ×•×Ÿ", "×™×¢×“", "×—×œ×•×",
    "×œ×‘×—×•×¨", "×œ×”×—×œ×™×˜", "× ×‘×•×š", "××‘×•×œ×‘×œ", "×œ× ×™×•×“×¢", "×¢×–×¨×”", "×—×©×•×‘", "×§×¨×™×˜×™",
    "×—×¡×•×", "×œ× ××¦×œ×™×—", "××¤×—×“", "×œ× ××¢×–", "××ª×‘×™×™×©", "×“×—×™×§×”", "×”×™×× ×¢×•×ª",
    
    # × ×˜×™×™×” ××™× ×™×ª ×•×–×”×•×ª ××™× ×™×ª - LGBTQ+
    "×”×•××•", "×’×™×™", "×‘×™×¡×§×¡×•××œ", "×œ×”×˜×´×‘", "×œ×”×˜×‘", "×”×§×”×™×œ×” ×”×’××”", "× ×˜×™×™×” ××™× ×™×ª", "×–×”×•×ª ××™× ×™×ª",
    "×‘××¨×•×Ÿ", "×™×¦×™××” ××”××¨×•×Ÿ", "××¨×•× ×™×¡×˜", "×“×™×¡×§×¨×˜×™", "××¨×•×Ÿ", "×”××¨×•×Ÿ", "×‘×™", "×“×•", "× ××©×š",
    "×”×•××•×¤×•×‘×™×”", "×”×•××•×¤×•×‘×™×” ×¤× ×™××™×ª", "×”×•××•×¤×•×‘×™×” ×¢×¦××™×ª", "×œ× ××§×‘×œ ××ª ×¢×¦××™",
    "××ª×—× ×’×œ", "××•×›×œ ×‘×ª×—×ª", "×§×•×§×¡×™× ×œ", "× ×©×™", "×’×‘×¨×™", "×˜×¨× ×¡", "×§×•×•×™×¨×™",
    "×¢×œ ×”×¨×¦×£", "×¡×§×¡", "×× ××œ×™", "××•×¨××œ×™", "××¤×—×“",
    
    # ×’×™×œ ×•××¢×‘×¨×™×
    "×‘×’×™×œ ×××•×—×¨", "××¤×¡×¤×¡", "×”×¨×›×‘×ª ×¢×•×‘×¨×ª", "×–××Ÿ ×˜×¡", "×œ× ×¦×¢×™×¨", "×›×‘×¨ ×‘×Ÿ", "×›×‘×¨ ×‘×ª",
    "× ×××¡", "×œ××•×ª", "×¨×•×¦×” ×œ×”×™×•×ª ×—×•×¤×©×™", "××¨×’×™×© ×¦×¢×™×¨", "×××¦×¢ ×”×—×™×™×"
]

# ×“×¤×•×¡×™ ×‘×™×˜×•×™×™× ××•×¨×›×‘×™× (regex) - ×¨×–×” ×•×—×“
COMPLEX_PATTERNS = [
    # ×©××œ×•×ª ××•×¨×›×‘×•×ª
    r"××”\s+×¢×•×©×™×\s+×›×©|××™×š\s+×œ×”×ª××•×“×“\s+×¢×|×¦×¨×™×š\s+×¢×¦×”\s+×‘|×œ×\s+×™×•×“×¢\s+××™×š",
    # ××¦×‘×™ × ×¤×© ×§×©×™×  
    r"×œ×\s+××¨×’×™×©\s+×©×œ×|×× ×™\s+×ª×§×•×¢|××¨×’×™×©\s+××‘×•×“|×× ×™\s+×©×•× ×\s+××ª\s+×¢×¦××™",
    # ×§×‘×œ×” ×¢×¦××™×ª
    r"×œ×\s+×©×œ×\s+×¢×|×§×•×©×™\s+×œ[×§|×›]×‘×œ|×¢×“×™×™×Ÿ\s+×œ×\s+×©×œ×|×”×ª××•×“×“×•×ª\s+×¢×",
    # ×–×•×’×™×•×ª ×•×‘×“×™×“×•×ª
    r"× ×©×•×™\s+×œ|×œ×\s+××¦×œ×™×—\s+×œ××¦×•×|×™×¦××ª×™\s+××‘×œ\s+×œ×\s+×‘×××ª"
]

# ××©×ª× ×” ×’×œ×•×‘×œ×™ ×œ×¢×§×™×‘×” ××—×¨ ×”×”×—×œ×˜×•×ª
filter_decisions_log = {
    "length": 0,
    "keywords": 0, 
    "pattern": 0,
    "default": 0
}

def log_filter_decision(match_type):
    """×¨×•×©× ×”×—×œ×˜×ª ×¤×™×œ×˜×¨ ×œ×¦×•×¨×š × ×™×ª×•×—"""
    global filter_decisions_log
    if match_type in filter_decisions_log:
        filter_decisions_log[match_type] += 1

def get_filter_analytics():
    """××—×–×™×¨ × ×™×ª×•×— ×©×œ ×”×—×œ×˜×•×ª ×”×¤×™×œ×˜×¨"""
    global filter_decisions_log
    total = sum(filter_decisions_log.values())
    if total == 0:
        return {"message": "×¢×“×™×™×Ÿ ×œ× × ×¨×©××• ×”×—×œ×˜×•×ª ×¤×™×œ×˜×¨"}
    
    percentages = {k: round((v/total)*100, 1) for k, v in filter_decisions_log.items()}
    
    return {
        "total_decisions": total,
        "breakdown": filter_decisions_log.copy(),
        "percentages": percentages,
        "premium_usage": round(((total - filter_decisions_log["default"])/total)*100, 1) if total > 0 else 0
    }

def should_use_premium_model(user_message, chat_history_length=0):
    """
    ××—×œ×™×˜ ×”×× ×œ×”×©×ª××© ×‘××•×“×œ ×”××ª×§×“× ××• ×‘××”×™×¨ ×™×•×ª×¨
    
    ×§×¨×™×˜×¨×™×•× ×™× ×œ××•×“×œ ××ª×§×“×:
    1. ×”×•×“×¢×” ××¨×•×›×” (××¢×œ X ××™×œ×™×)
    2. ××™×œ×•×ª ××¤×ª×— ×¨×œ×•×•× ×˜×™×•×ª
    3. ×“×¤×•×¡×™ ×‘×™×˜×•×™×™× ××•×¨×›×‘×™×
    
    Returns:
        tuple: (should_use_premium: bool, reason: str, match_type: str)
    """
    # ×‘×“×™×§×ª ××•×¨×š ×”×•×“×¢×”
    word_count = len(user_message.split())
    if word_count > LONG_MESSAGE_THRESHOLD:
        logging.info(f"ğŸ¯ [PREMIUM_FILTER] ×”×•×“×¢×” ××¨×•×›×”: {word_count} ××™×œ×™× -> ××•×“×œ ××ª×§×“×")
        result = True, f"×”×•×“×¢×” ××¨×•×›×” ({word_count} ××™×œ×™×)", "length"
        log_filter_decision(result[2])
        return result
    
    # ×‘×“×™×§×ª ××™×œ×•×ª ××¤×ª×—
    user_message_lower = user_message.lower()
    found_keywords = [keyword for keyword in PREMIUM_MODEL_KEYWORDS if keyword in user_message_lower]
    if found_keywords:
        logging.info(f"ğŸ¯ [PREMIUM_FILTER] ××™×œ×•×ª ××¤×ª×— × ××¦××•: {found_keywords[:3]} -> ××•×“×œ ××ª×§×“×")
        result = True, f"××™×œ×•×ª ××¤×ª×—: {', '.join(found_keywords[:3])}", "keywords"
        log_filter_decision(result[2])
        return result
    
    # ×‘×“×™×§×ª ×“×¤×•×¡×™ ×‘×™×˜×•×™×™× ××•×¨×›×‘×™×
    for pattern in COMPLEX_PATTERNS:
        if re.search(pattern, user_message_lower):
            logging.info(f"ğŸ¯ [PREMIUM_FILTER] ×“×¤×•×¡ ××•×¨×›×‘ × ××¦×: {pattern} -> ××•×“×œ ××ª×§×“×")
            result = True, f"×“×¤×•×¡ ××•×¨×›×‘ ×–×•×”×”", "pattern"
            log_filter_decision(result[2])
            return result
    
    # ××—×¨×ª, ××•×“×œ ××”×™×¨
    logging.info(f"ğŸš€ [PREMIUM_FILTER] ××§×¨×” ×¨×’×™×œ -> ××•×“×œ ××”×™×¨")
    result = False, "××§×¨×” ×¨×’×™×œ - ××•×“×œ ××”×™×¨", "default"
    log_filter_decision(result[2])
    return result

async def send_temporary_message_after_delay(update, chat_id, delay_seconds=5):
    """
    ×©×•×œ×— ×”×•×“×¢×” ×–×× ×™×ª ××—×¨×™ ×“×™×œ×™×™ ××¡×•×™× ×•××—×–×™×¨ ××ª ×”××•×‘×™×™×§×˜ Message ×©×œ×”

    ×”×©×™× ×•×™ (×”×—×–×¨×ª ×”××•×‘×™×™×§×˜ ×¢×¦××• ×•×œ× ×¨×§ ×”-id) ×××¤×©×¨ ×œ× ×• ×œ××—×•×§ ××ª ×”×”×•×“×¢×” ×‘×§×œ×•×ª ×‘×××¦×¢×•×ª
    await temp_message.delete()â€ ×‘×œ×™ ×´×¦×™×“×´ ××—×¨ ×”-bot, ××” ×©×¤×’×¢ ×‘××—×™×§×” ×‘×¢×‘×¨.
    """
    await asyncio.sleep(delay_seconds)
    try:
        temp_message = await update.message.reply_text("â³ ×× ×™ ×¢×•×‘×“ ×¢×œ ×ª×©×•×‘×” ×‘×©×‘×™×œ×š... ×–×” ××™×“ ××¦×œ×š...")
        logging.info(
            f"ğŸ“¤ [TEMP_MSG] × ×©×œ×—×” ×”×•×“×¢×” ×–×× ×™×ª | chat_id={chat_id} | message_id={temp_message.message_id}"
        )
        return temp_message  # ××—×–×™×¨×™× ××ª ×”××•×‘×™×™×§×˜ ×¢×¦××•
    except Exception as e:
        logging.error(f"âŒ [TEMP_MSG] ×©×’×™××” ×‘×©×œ×™×—×ª ×”×•×“×¢×” ×–×× ×™×ª: {e}")
        return None

async def delete_temporary_message_and_send_new(update, temp_message, new_text):
    """
    ××•×—×§ ××ª ×”×”×•×“×¢×” ×”×–×× ×™×ª (×× ×§×™×™××ª) ×•×©×•×œ×— ×œ××©×ª××© ××ª ×”×ª×©×•×‘×” ×”×××™×ª×™×ª.

    âœ… ×©×™×¤×•×¨: ××©×ª××©×™× ×‘-temp_message.delete()â€ â€“ ×‘×˜×•×— ×•×¤×©×•×˜ ×™×•×ª×¨, ××™×Ÿ ×¦×•×¨×š ×œ×”×ª×××¥ ×œ×”×©×™×’ ××ª ×”-bot.
    """
    from message_handler import format_text_for_telegram  # local import to avoid circular
    formatted_text = format_text_for_telegram(new_text)

    try:
        # ××—×™×§×ª ×”×”×•×“×¢×” ×”×–×× ×™×ª
        if temp_message is not None:
            try:
                await temp_message.delete()
                logging.info(
                    f"ğŸ—‘ï¸ [DELETE_MSG] ×”×•×“×¢×” ×–×× ×™×ª × ××—×§×” | chat_id={temp_message.chat_id} | message_id={temp_message.message_id}"
                )
            except Exception as del_err:
                logging.warning(
                    f"âš ï¸ [DELETE_MSG] ×œ× ×”×¦×œ×—×ª×™ ×œ××—×•×§ ××ª ×”×”×•×“×¢×” ×”×–×× ×™×ª: {del_err} â€“ ×××©×™×š ×œ×©×œ×™×—×”"
                )

        # ×©×œ×™×—×ª ×”×”×•×“×¢×” ×”×—×“×©×”
        await update.message.reply_text(formatted_text, parse_mode="HTML")
        logging.info(f"ğŸ“¤ [NEW_MSG] × ×©×œ×—×” ×”×•×“×¢×” ×—×“×©×” | chat_id={temp_message.chat_id if temp_message else 'unknown'}")
        return True

    except Exception as send_err:
        logging.error(f"âŒ [DELETE_MSG] ×›×©×œ ×‘××—×™×§×”/×©×œ×™×—×”: {send_err}")
        return False

def get_main_response_sync(full_messages, chat_id=None, message_id=None, use_premium=True, filter_reason="", match_type="unknown"):
    """
    ğŸ’ ×× ×•×¢ gpt_a ×”×¨××©×™ - ×’×¨×¡×” ×¡×™× ×›×¨×•× ×™×ª
    """
    # ××“×™×“×ª ×–××Ÿ ×”×ª×—×œ×”
    total_start_time = time.time()
    
    # ×©×œ×‘ 1: ×”×›× ×ª ×”×”×•×“×¢×•×ª
    prep_start_time = time.time()
    
    # ×œ× ×©×•×œ×—×™× ×‘×§×©×” ×œ×©××œ×•×ª ×¨×’×™×©×•×ª ×× ××•×¤×¢×œ extra_emotion (use_premium == True)
    if chat_id and not use_premium:
        missing_fields_message = create_missing_fields_system_message(chat_id)
        if missing_fields_message:
            full_messages.insert(1, {"role": "system", "content": missing_fields_message})
    
    prep_time = time.time() - prep_start_time
    print(f"âš¡ [TIMING] Preparation time: {prep_time:.3f}s")
    
    metadata = {"gpt_identifier": "gpt_a", "chat_id": chat_id, "message_id": message_id}
    params = GPT_PARAMS["gpt_a"]
    
    # ğŸ”¬ ××“×™×“×ª ×‘×™×¦×•×¢×™× ××‘×•×˜×œ×ª ×–×× ×™×ª
    measurement_id = None
    
    # ×‘×—×™×¨×ª ××•×“×œ ×œ×¤×™ ×”×¤×™×œ×˜×¨
    if use_premium:
        model = GPT_MODELS["gpt_a"]  # ×”××•×“×œ ×”××ª×§×“× ×-config
        model_tier = "premium"
        logging.info(f"ğŸ¯ [MODEL_SELECTION] ××©×ª××© ×‘××•×“×œ ××ª×§×“×: {model} | ×¡×™×‘×”: {filter_reason}")
    else:
        model = GPT_FALLBACK_MODELS["gpt_a"]  # ×”××•×“×œ ×”××”×™×¨ ×-config
        model_tier = "fast"
        logging.info(f"ğŸš€ [MODEL_SELECTION] ××©×ª××© ×‘××•×“×œ ××”×™×¨: {model} | ×¡×™×‘×”: {filter_reason}")
    
    completion_params = {
        "model": model,
        "messages": full_messages,
        "temperature": params["temperature"],
        "metadata": metadata,
        "store": True
    }
    
    # ×”×•×¡×¤×ª max_tokens ×¨×§ ×× ×”×•× ×œ× None
    if params["max_tokens"] is not None:
        completion_params["max_tokens"] = params["max_tokens"]

    # ğŸ” [DEBUG] × ×™×ª×•×— ××¤×•×¨×˜ ×©×œ ×”××‘× ×” ×©× ×©×œ×— ×œ-GPT
    print(f"\nğŸ” [GPT_REQUEST_DEBUG] === DETAILED GPT REQUEST ANALYSIS ===")
    print(f"ğŸ¤– [MODEL] {model} | Premium: {use_premium} | Reason: {filter_reason}")
    print(f"ğŸ“Š [PARAMS] Temperature: {params['temperature']} | Max Tokens: {params.get('max_tokens', 'None')}")
    print(f"ğŸ“ [MESSAGES_COUNT] Total messages: {len(full_messages)}")
    
    # × ×™×ª×•×— ×”×•×“×¢×•×ª ×œ×¤×™ ×¡×•×’
    system_count = 0
    user_count = 0
    assistant_count = 0
    
    for i, msg in enumerate(full_messages):
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        content_length = len(content)
        
        if role == "system":
            system_count += 1
            print(f"âš™ï¸ [SYSTEM_{system_count}] Position: {i} | Length: {content_length} chars")
        elif role == "user":
            user_count += 1
            print(f"ğŸ‘¤ [USER_{user_count}] Position: {i} | Length: {content_length} chars")
        elif role == "assistant":
            assistant_count += 1
            print(f"ğŸ¤– [ASSISTANT_{assistant_count}] Position: {i} | Length: {content_length} chars")
    
    print(f"ğŸ“ˆ [SUMMARY] System: {system_count} | User: {user_count} | Assistant: {assistant_count}")
    print(f"ğŸš€ [SENDING] Request to {model}...")
    print(f"ğŸ” [GPT_REQUEST_DEBUG] === END ANALYSIS ===\n")
    
    # ×©×œ×‘ 2: ×§×¨×™××” ×œ-GPT
    gpt_start_time = time.time()
    try:
        # ğŸ”¬ ×ª×–××•×Ÿ ×”×˜×•×§×Ÿ ×”×¨××©×•×Ÿ - ×¦×¨×™×š ×œ×”×©×ª××© ×‘-streaming ×œ×–×”
        with measure_llm_latency(model):
            response = litellm.completion(**completion_params)
        gpt_end_time = time.time()
        gpt_pure_latency = gpt_end_time - gpt_start_time
        
        # ×©×œ×‘ 3: ×¢×™×‘×•×“ ×”×ª×©×•×‘×”
        processing_start_time = time.time()
        
        # ğŸ”¬ ×¨×™×©×•× ×”×˜×•×§×Ÿ ×”×¨××©×•×Ÿ ××‘×•×˜×œ ×–×× ×™×ª
        
        bot_reply = response.choices[0].message.content.strip()
        
        # × ×™×§×•×™ ×ª×’×™ HTML ×œ× × ×ª××›×™× ×©×”××•×“×œ ×¢×œ×•×œ ×œ×”×—×–×™×¨
        # <br> ×ª×’×™× ×œ× × ×ª××›×™× ×‘-Telegram - ×¦×¨×™×š ×œ×”××™×¨ ×œ-\n
        bot_reply = bot_reply.replace('<br>', '\n').replace('<br/>', '\n').replace('<br />', '\n')
        # ×’× × ×™×§×•×™ ×ª×’×™ br ×¢× attributes ×©×•× ×™×
        bot_reply = re.sub(r'<br\s*/?>', '\n', bot_reply)
        
        usage = normalize_usage_dict(response.usage, response.model)
        
        # ×”×•×¡×¤×ª × ×ª×•× ×™ ×¢×œ×•×ª ××“×•×™×§×™× ×œ-usage ×¢×œ ×¡××š completion_response
        try:
            cost_info = calculate_gpt_cost(
                prompt_tokens=usage.get("prompt_tokens", 0),
                completion_tokens=usage.get("completion_tokens", 0),
                cached_tokens=usage.get("cached_tokens", 0),
                model_name=response.model,
                completion_response=response
            )
            usage.update(cost_info)
        except Exception as _cost_e:
            logging.warning(f"[gpt_a] ×œ× ×”×¦×œ×—×ª×™ ×œ×—×©×‘ ×¢×œ×•×ª usage: {_cost_e}")
        
        processing_time = time.time() - processing_start_time
        print(f"âš¡ [TIMING] Processing time: {processing_time:.3f}s")
        
        print(f"âœ… [GPT_RESPONSE_DEBUG] Received {len(bot_reply)} chars from {response.model}")
        print(f"âš¡ [DETAILED_TIMING] GPT pure latency: {gpt_pure_latency:.3f}s | Model: {model}")
        
        # ×©×œ×‘ 4: ×—×™×©×•×‘ ×¢×œ×•×™×•×ª
        billing_start_time = time.time()
        
        # ğŸ”¬ ××“×™×“×ª ×‘×™×¦×•×¢×™× ××‘×•×˜×œ×ª ×–×× ×™×ª
        
        # ğŸ“Š ××¢×§×‘ ××—×¨ ×—×™×•×‘
        if hasattr(response, 'usage'):
            try:
                cost_usd = litellm.completion_cost(completion_response=response)
                if cost_usd > 0:
                    billing_status = billing_guard.add_cost(cost_usd, response.model, "paid" if use_premium else "free")
                    
                    # ×”×ª×¨××•×ª ×œ××“××™×Ÿ
                    if billing_status["warnings"]:
                        for warning in billing_status["warnings"]:
                            logging.warning(f"[ğŸ’° ×ª×§×¦×™×‘] {warning}")
                    
                    # ×”×ª×¨××” ×‘×˜×œ×’×¨× ×× ×¦×¨×™×š
                    status = billing_guard.get_current_status()
                    alert_billing_issue(
                        cost_usd=cost_usd,
                        model_name=response.model,
                        tier="paid" if use_premium else "free",
                        daily_usage=status["daily_usage"],
                        monthly_usage=status["monthly_usage"],
                        daily_limit=status["daily_limit"],
                        monthly_limit=status["monthly_limit"]
                    )
                
            except Exception as cost_error:
                logging.error(f"[ğŸ’°] ×©×’×™××” ×‘×—×™×©×•×‘ ×¢×œ×•×ª: {cost_error}")
        
        billing_time = time.time() - billing_start_time
        print(f"âš¡ [TIMING] Billing time: {billing_time:.3f}s")
        
        # ×¡×™×›×•× ×–×× ×™×
        total_time = time.time() - total_start_time
        print(f"ğŸ“Š [TIMING_SUMMARY] Total: {total_time:.3f}s | GPT: {gpt_pure_latency:.3f}s | Prep: {prep_time:.3f}s | Processing: {processing_time:.3f}s | Billing: {billing_time:.3f}s")
        
        return {
            "bot_reply": bot_reply, 
            "usage": usage, 
            "model": response.model,
            "used_premium": use_premium,
            "filter_reason": filter_reason,
            "match_type": match_type,
            "gpt_pure_latency": gpt_pure_latency,
            "total_time": total_time,
            "prep_time": prep_time,
            "processing_time": processing_time,
            "billing_time": billing_time
        }
        
    except Exception as e:
        logging.error(f"[gpt_a] ×©×’×™××” ×‘××•×“×œ {model}: {e}")
        
        # ğŸ”¬ ×¨×™×©×•× ×©×’×™××” ××‘×•×˜×œ ×–×× ×™×ª
        
        # ×©×œ×™×—×ª ×”×•×“×¢×ª ×©×’×™××” ×˜×›× ×™×ª ×œ××“××™×Ÿ
        send_error_notification(
            error_message=f"×©×’×™××” ×‘×× ×•×¢ ×”×¨××©×™ (gpt_a) - ××•×“×œ: {model}, ×©×’×™××”: {str(e)}",
            chat_id=chat_id,
            user_msg=full_messages[-1]["content"] if full_messages else "×œ× ×–××™×Ÿ",
            error_type="gpt_a_engine_error"
        )
        
        return {
            "bot_reply": "××¦×˜×¢×¨, ×™×© ×œ×™ ×‘×¢×™×” ×˜×›× ×™×ª ×–×× ×™×ª. ×”×¢×‘×¨×ª×™ ××ª ×”×¤×¨×˜×™× ×œ×¢×•××¨ ×©×™×‘×“×•×§ ××ª ×–×”. × ×¡×” ×©×•×‘ ×‘×¢×•×“ ×›××” ×“×§×•×ª ğŸ”§", 
            "usage": {}, 
            "model": model,
            "used_premium": use_premium,
            "filter_reason": filter_reason,
            "match_type": match_type,
            "error": str(e)
        }

async def get_main_response_with_timeout(full_messages, chat_id=None, message_id=None, update=None):
    """
    ğŸ’ ×©×•×œ×— ×”×•×“×¢×” ×œ-gpt_a ×¢× × ×™×”×•×œ ×—×›× ×©×œ ×–×× ×™ ×ª×’×•×‘×” ×•×”×•×“×¢×•×ª ×–×× ×™×•×ª
    """
    # ×©×œ×‘ 1: ×§×‘×™×¢×ª ××•×“×œ ×œ×¤×™ ×¤×™×œ×˜×¨ ×—×›×
    user_message = full_messages[-1]["content"] if full_messages else ""
    chat_history_length = len([msg for msg in full_messages if msg["role"] in ["user", "assistant"]])
    
    use_premium, filter_reason, match_type = should_use_premium_model(user_message, chat_history_length)
    
    # ×©×œ×‘ 2: ×”×›× ×ª ×˜×™×™××¨ ×œ×”×•×“×¢×” ×–×× ×™×ª
    temp_message_task = None
    temp_message = None
    
    if update and chat_id:
        # ×”×ª×—×œ×ª ×˜×™×™××¨ ×œ×”×•×“×¢×” ×–×× ×™×ª (××—×¨×™ 5 ×©× ×™×•×ª)
        temp_message_task = asyncio.create_task(
            send_temporary_message_after_delay(update, chat_id, delay_seconds=5)
        )
    
    # ×©×œ×‘ 3: ×”×¤×¢×œ×ª GPT ×‘-thread × ×¤×¨×“
    gpt_start_time = time.time()
    
    try:
        # ×”×¨×¦×ª GPT ×‘-thread ×›×“×™ ×©×œ× ×œ×—×¡×•× ××ª ×”××™×¨×•×¢×™×
        loop = asyncio.get_event_loop()
        gpt_result = await loop.run_in_executor(
            None, 
            get_main_response_sync, 
            full_messages, 
            chat_id, 
            message_id, 
            use_premium, 
            filter_reason,
            match_type
        )
        
        gpt_duration = time.time() - gpt_start_time
        logging.info(f"â±ï¸ [GPT_TIMING] GPT ×”×¡×ª×™×™× ×ª×•×š {gpt_duration:.2f} ×©× ×™×•×ª")
        
        # ××“×™×“×” ××¤×•×¨×˜×ª ×©×œ ×”×–×× ×™×
        print(f"ğŸ“Š [TIMING_BREAKDOWN] Total GPT time: {gpt_duration:.3f}s")
        if 'gpt_pure_latency' in gpt_result:
            pure_latency = gpt_result.get('gpt_pure_latency', 0)
            total_time = gpt_result.get('total_time', 0)
            prep_time = gpt_result.get('prep_time', 0)
            processing_time = gpt_result.get('processing_time', 0)
            billing_time = gpt_result.get('billing_time', 0)
            
            print(f"ğŸ“Š [TIMING_BREAKDOWN] Pure GPT latency: {pure_latency:.3f}s")
            print(f"ğŸ“Š [TIMING_BREAKDOWN] Preparation: {prep_time:.3f}s | Processing: {processing_time:.3f}s | Billing: {billing_time:.3f}s")
            print(f"ğŸ“Š [TIMING_BREAKDOWN] Total internal time: {total_time:.3f}s | Thread overhead: {gpt_duration - total_time:.3f}s")
        
        # ×©×œ×‘ 4: ×‘×™×˜×•×œ ××• ×¢×“×›×•×Ÿ ×”×•×“×¢×” ×–×× ×™×ª
        if temp_message_task:
            if not temp_message_task.done():
                # GPT ×”×¡×ª×™×™× ×œ×¤× ×™ ×©×”×”×•×“×¢×” ×”×–×× ×™×ª × ×©×œ×—×” â€“ ××‘×˜×œ×™× ××•×ª×”
                temp_message_task.cancel()
                logging.info(
                    f"âœ… [TIMING] GPT ××”×™×¨ ({gpt_duration:.1f}s) - ×”×•×“×¢×” ×–×× ×™×ª ×‘×•×˜×œ×” ×œ×¤× ×™ ×©× ×©×œ×—×”"
                )
            else:
                # ×”×”×•×“×¢×” ×”×–×× ×™×ª × ×©×œ×—×” â€“ ××•×—×§×™× ××•×ª×” ×•×©×•×œ×—×™× ××ª ×”×ª×©×•×‘×”
                temp_message = await temp_message_task
                if temp_message and update:
                    success = await delete_temporary_message_and_send_new(
                        update,
                        temp_message,
                        gpt_result["bot_reply"]
                    )
                    if success:
                        logging.info(
                            f"ğŸ”„ [TIMING] GPT ××™×˜×™ ({gpt_duration:.1f}s) - ×”×•×“×¢×” ×–×× ×™×ª × ××—×§×” ×•× ×©×œ×—×” ×—×“×©×”"
                        )
                        gpt_result["message_already_sent"] = True
                    else:
                        # ×× ×”××—×™×§×”/×©×œ×™×—×” × ×›×©×œ×•, × × ×¡×” ×œ×©×œ×•×— ×—×™×¨×•×
                        logging.warning(
                            f"âš ï¸ [EMERGENCY] ××—×™×§×”/×©×œ×™×—×” × ×›×©×œ×•, ×©×•×œ×— ×”×•×“×¢×ª ×—×™×¨×•×"
                        )
                        try:
                            emergency_text = (
                                f"××¦×˜×¢×¨ ×¢×œ ×”×¢×™×›×•×‘. ×”×ª×©×•×‘×” ×©×œ×™:\n\n{gpt_result['bot_reply'][:1000]}..."
                                if len(gpt_result['bot_reply']) > 1000
                                else gpt_result["bot_reply"]
                            )
                            from message_handler import format_text_for_telegram  # local import to avoid circular
                            formatted_emergency_text = format_text_for_telegram(emergency_text)
                            await update.message.reply_text(
                                formatted_emergency_text, parse_mode="HTML"
                            )
                            gpt_result["message_already_sent"] = True
                        except Exception as emergency_error:
                            logging.error(
                                f"âŒ [EMERGENCY] ×’× ×”×•×“×¢×ª ×—×™×¨×•× × ×›×©×œ×”: {emergency_error}"
                            )
        
        return gpt_result
        
    except Exception as e:
        logging.error(f"[gpt_a] ×©×’×™××” ×›×œ×œ×™×ª: {e}")
        
        # ×‘×™×˜×•×œ ×”×•×“×¢×” ×–×× ×™×ª ×‘××§×¨×” ×©×œ ×©×’×™××”
        if temp_message_task and not temp_message_task.done():
            temp_message_task.cancel()
        
        # ×©×œ×™×—×ª ×”×•×“×¢×ª ×©×’×™××” ×˜×›× ×™×ª ×œ××“××™×Ÿ
        send_error_notification(
            error_message=f"×©×’×™××” ×›×œ×œ×™×ª ×‘-get_main_response_with_timeout: {str(e)}",
            chat_id=chat_id,
            user_msg=full_messages[-1]["content"] if full_messages else "×œ× ×–××™×Ÿ", 
            error_type="gpt_a_timeout_error"
        )
        
        return {
            "bot_reply": "××¦×˜×¢×¨, ×™×© ×œ×™ ×‘×¢×™×” ×˜×›× ×™×ª ×–×× ×™×ª. ×”×¢×‘×¨×ª×™ ××ª ×”×¤×¨×˜×™× ×œ×¢×•××¨ ×©×™×‘×“×•×§ ××ª ×–×”. × ×¡×” ×©×•×‘ ×‘×¢×•×“ ×›××” ×“×§×•×ª ğŸ”§", 
            "usage": {}, 
            "model": "error",
            "used_premium": use_premium,
            "filter_reason": filter_reason,
            "match_type": match_type,
            "error": str(e)
        }

# ×¤×•× ×§×¦×™×” ×™×©× ×” ×œ×ª××™××•×ª ×œ××—×•×¨
def get_main_response(full_messages, chat_id=None, message_id=None):
    """
    ğŸ’ ×’×¨×¡×” ×¡×™× ×›×¨×•× ×™×ª ×™×©× ×” - ×œ×ª××™××•×ª ×œ××—×•×¨
    """
    user_message = full_messages[-1]["content"] if full_messages else ""
    chat_history_length = len([msg for msg in full_messages if msg["role"] in ["user", "assistant"]])
    
    use_premium, filter_reason, match_type = should_use_premium_model(user_message, chat_history_length)
    
    return get_main_response_sync(full_messages, chat_id, message_id, use_premium, filter_reason, match_type)

# ×¤×•× ×§×¦×™×™×ª enforce_single_question ×”×•×¡×¨×” â€“ ×”×›×œ ×¢×•×‘×¨ ×“×¨×š ×”××•×“×œ ×‘×”×ª×× ×œ×”× ×—×™×•×ª ×‘×¤×¨×•××˜ 