"""
gpt_a_handler.py
----------------
×× ×•×¢ gpt_a: ×œ×•×’×™×§×ª ×”×ª×©×•×‘×” ×”×¨××©×™×ª (main response logic)
×¢× ×× ×’× ×•×Ÿ ×”×•×“×¢×” ×–×× ×™×ª ×•×¤×™×œ×˜×¨ ×—×›× ×œ×‘×—×™×¨×ª ××•×“×œ
"""

from simple_logger import logger
from datetime import datetime
import json
import lazy_litellm as litellm
import asyncio
import threading
import time
import re
from prompts import SYSTEM_PROMPT
from config import GPT_MODELS, GPT_PARAMS, GPT_FALLBACK_MODELS, should_log_debug_prints
from gpt_utils import normalize_usage_dict, billing_guard, measure_llm_latency, calculate_gpt_cost
from notifications import alert_billing_issue, send_error_notification
from simple_config import TimeoutConfig
from typing import TYPE_CHECKING
from db_manager import safe_str
if TYPE_CHECKING:
    from message_handler import format_text_for_telegram, send_message_with_retry  # for type checkers only
# ××¢×¨×›×ª ×‘×™×¦×•×¢×™× ××‘×•×˜×œ×ª ×–×× ×™×ª

# ×™×™×‘×•× ×”×¤×™×œ×˜×¨ ×”×—×›×
# ===============================
# ğŸ¯ ×¤×™×œ×˜×¨ ×—×›× ×œ×‘×—×™×¨×ª ××•×“×œ AI
# ===============================

# ×¡×£ ××™×œ×™× ×œ××•×“×œ ××ª×§×“×
LONG_MESSAGE_THRESHOLD = 50  # ××¢×œ 50 ××™×œ×™× = ××•×“×œ ××ª×§×“×

# ===============================
# ğŸ“Š ×× ×’× ×•×Ÿ ×¡×¤×™×¨×ª ×”×•×“×¢×•×ª ×œ×©××œ×•×ª ×¤×¨×•×¤×™×œ
# ===============================
# ××•× ×” ×”×•×“×¢×•×ª ×œ×›×œ ××©×ª××© - ×××¤×©×¨ ×œ×©××•×œ ×©××œ×•×ª ×¨×§ ×›×œ 4 ×”×•×“×¢×•×ª
profile_question_counters = {}

# ğŸ†• ×× ×’× ×•×Ÿ ×¤×¡×§ ×–××Ÿ ×œ×©××œ×•×ª ×¤×¨×•×¤×™×œ
# ×›×©×”×‘×•×˜ ×©×•××œ ×©××œ×” - ××ª×—×™×œ ×¤×¡×§ ×–××Ÿ ×©×œ 3 ×”×•×“×¢×•×ª
profile_question_cooldowns = {}

def should_ask_profile_question(chat_id: str) -> bool:
    """
    ×‘×•×“×§ ×× ×”×’×™×¢ ×”×–××Ÿ ×œ×©××•×œ ×©××œ×ª ×¤×¨×•×¤×™×œ
    
    ×œ×•×’×™×§×”:
    - ×× ×”××©×ª××© ×œ× ×‘×¤×¡×§ ×–××Ÿ - ×©×•××œ×™× ×©××œ×”
    - ×›×©×©×•××œ×™× ×©××œ×” - ××ª×—×™×œ ×¤×¡×§ ×–××Ÿ ×©×œ 3 ×”×•×“×¢×•×ª
    - ×‘××”×œ×š ×”×¤×¡×§ ×–××Ÿ - ×œ× ×©×•××œ×™× ×©××œ×•×ª
    - ××—×¨×™ ×”×¤×¡×§ ×”×–××Ÿ - ×—×•×–×¨×™× ×œ×©××•×œ
    
    Returns:
        bool: True ×× ×¦×¨×™×š ×œ×©××•×œ ×©××œ×”, False ××—×¨×ª
    """
    safe_chat_id = safe_str(chat_id)
    if safe_chat_id not in profile_question_cooldowns:
        profile_question_cooldowns[safe_chat_id] = 0
    
    # ×× ×”××©×ª××© ×‘×¤×¡×§ ×–××Ÿ - ××§×˜×™× ×™× ××ª ×”××•× ×”
    if profile_question_cooldowns[safe_chat_id] > 0:
        profile_question_cooldowns[safe_chat_id] -= 1
        logger.info(f"ğŸ“Š [PROFILE_QUESTION] ×‘×¤×¡×§ ×–××Ÿ | chat_id={safe_chat_id} | cooldown_left={profile_question_cooldowns[safe_chat_id]}", source="gpt_a_handler")
        return False
    
    # ×œ× ×‘×¤×¡×§ ×–××Ÿ - ××¤×©×¨ ×œ×©××•×œ
    logger.info(f"ğŸ“Š [PROFILE_QUESTION] ×”×’×™×¢ ×”×–××Ÿ ×œ×©××•×œ ×©××œ×ª ×¤×¨×•×¤×™×œ | chat_id={safe_chat_id}", source="gpt_a_handler")
    return True

def start_profile_question_cooldown(chat_id: str):
    """××ª×—×™×œ ×¤×¡×§ ×–××Ÿ ×©×œ 3 ×”×•×“×¢×•×ª ××—×¨×™ ×©×©××œ×” × ×©××œ×”"""
    safe_chat_id = safe_str(chat_id)
    profile_question_cooldowns[safe_chat_id] = 3
    logger.info(f"ğŸ“Š [PROFILE_QUESTION] ×¤×¡×§ ×–××Ÿ ×”×ª×—×™×œ | chat_id={safe_chat_id} | cooldown=3", source="gpt_a_handler")

def reset_profile_question_counter(chat_id: str):
    """×××¤×¡ ××ª ×”××•× ×” ×œ××©×ª××© ××¡×•×™× (×œ××§×¨×” ×©×œ ×©××œ×” ×©× ×¢× ×ª×”)"""
    safe_chat_id = safe_str(chat_id)
    if safe_chat_id in profile_question_counters:
        profile_question_counters[safe_chat_id] = 0
        logger.info(f"ğŸ“Š [PROFILE_QUESTION] ××•× ×” ××•×¤×¡ | chat_id={safe_chat_id}", source="gpt_a_handler")

def get_profile_question_stats():
    """××—×–×™×¨ ×¡×˜×˜×™×¡×˜×™×§×•×ª ×©×œ ××•× ×™ ×”×©××œ×•×ª"""
    return {
        "total_users": len(profile_question_counters),
        "counters": profile_question_counters.copy(),
        "cooldowns": profile_question_cooldowns.copy()
    }

def did_bot_ask_profile_questions(missing_text, bot_reply, chat_id=None):
    """
    ×‘×•×“×§ ×”×× ×œ×¤×—×•×ª 2 ××™×œ×™× ××ª×•×š missing_text ××•×¤×™×¢×•×ª ×‘×ª×©×•×‘×ª ×”×‘×•×˜.
    ××•×¡×™×£ ×œ×•×’×™× ××¤×•×¨×˜×™× ×œ×¦×•×¨×š ×“×™×‘××’×™× ×’.
    """
    safe_chat_id = safe_str(chat_id) if chat_id else "unknown"
    if not missing_text or not bot_reply:
        logger.debug(f"[PROFILE_QUESTION][DEBUG] missing_text/bot_reply ×¨×™×§×™× | chat_id={safe_chat_id}", source="gpt_a_handler")
        return False
    
    # ××¤×¨×§ ××ª missing_text ×œ××™×œ×™× ×‘×•×“×“×•×ª (×œ×œ× ×¡×™×× ×™ ×¤×™×¡×•×§)
    import re
    missing_words = re.findall(r'\b\w+\b', missing_text.lower())
    bot_words = re.findall(r'\b\w+\b', bot_reply.lower())
    
    # ××•×¦× ××™×œ×™× ××©×•×ª×¤×•×ª
    matches = [word for word in missing_words if word in bot_words]
    
    logger.debug(f"[PROFILE_QUESTION][DEBUG] ×‘×“×™×§×ª ×”×ª×××” ×‘×™×Ÿ ××™×œ×™× | chat_id={safe_chat_id} | missing_words={missing_words[:10]} | bot_words={bot_words[:10]} | matches={matches} | count={len(matches)}", source="gpt_a_handler")
    
    return len(matches) >= 2

def create_missing_fields_system_message(chat_id: str) -> tuple:
    """×™×•×¦×¨ system message ×—×›× ×¢× ×©×“×•×ª ×—×¡×¨×™× ×©×›×“××™ ×œ×©××•×œ ×¢×œ×™×”×
    ××—×–×™×¨ tuple: (system_message, missing_text)"""
    try:
        safe_chat_id = safe_str(chat_id)
        from db_manager import get_user_profile
        try:
            from fields_dict import FIELDS_DICT
        except ImportError:
            FIELDS_DICT = {"dummy": "dummy"}
        if not should_ask_profile_question(safe_chat_id):
            logger.info(f"ğŸ“Š [PROFILE_QUESTION] ×œ× ×”×’×™×¢ ×”×–××Ÿ ×œ×©××•×œ ×©××œ×ª ×¤×¨×•×¤×™×œ | chat_id={safe_chat_id}", source="gpt_a_handler")
            return "", ""
        profile_data = get_user_profile(safe_chat_id) or {}
        key_fields = ["name", "age", "attracted_to", "relationship_type", "self_religious_affiliation", 
                     "closet_status", "pronoun_preference", "occupation_or_role", 
                     "self_religiosity_level", "primary_conflict", "goal_in_course"]
        missing = [FIELDS_DICT[f]["missing_question"] for f in key_fields
                  if f in FIELDS_DICT and not str(profile_data.get(f, "")).strip() 
                  and FIELDS_DICT[f].get("missing_question", "").strip()]
        if len(missing) >= 2:
            missing_text = ', '.join(missing[:4])
            logger.info(f"ğŸ“Š [PROFILE_QUESTION] ×©×•×œ×— ×©××œ×ª ×¤×¨×•×¤×™×œ | chat_id={safe_chat_id} | missing_fields={len(missing)} | missing_text={missing_text}", source="gpt_a_handler")
            return f"""×¤×¨×˜×™× ×©×”××©×ª××© ×¢×“×™×™×Ÿ ×œ× ×¡×™×¤×¨ ×œ×š ×•×—×©×•×‘ ×××•×“ ×œ×©××•×œ ××•×ª×• ×‘×¢×“×™× ×•×ª ×•×‘×¨×’×™×©×•×ª ×‘××˜×¨×” ×œ×”×›×™×¨ ××•×ª×• ×™×•×ª×¨ ×˜×•×‘: {missing_text}
\n×¨××©×™×ª ×ª×¡×‘×™×¨ ×œ×• ××ª ×”×¨×¦×™×•× ×œ, ×ª×¡×‘×™×¨ ×œ×• ×œ××” ××ª×” ×©×•××œ, ×ª×’×™×“ ×œ×• ×©×—×©×•×‘ ×œ×š ×œ×”×›×™×¨ ××•×ª×• ×›×“×™ ×œ×”×ª××™× ××ª ×¢×¦××š ××œ×™×•. ×ª×ª×¢× ×™×™×Ÿ ×‘×• - ×ª×‘×—×¨ ××—×“ ××”×©××œ×•×ª ×©× ×¨××™×ª ×œ×š ×”×›×™ ××ª××™××” - ×•×¨×§ ×× ×–×” ××¨×’×™×© ×œ×š ××ª××™× ××– ×ª×©××œ ××•×ª×• ×‘×¢×“×™× ×•×ª ×•×‘×¨×’×™×©×•×ª ×•×ª×©×œ×‘ ××ª ×–×” ×‘××œ×’× ×˜×™×•×ª. (××ª ×”×©××œ×•×ª ×ª×¢×©×” ×‘×›×ª×‘ ××•×“×’×©)""", missing_text
        logger.info(f"ğŸ“Š [PROFILE_QUESTION] ××™×Ÿ ××¡×¤×™×§ ×©×“×•×ª ×—×¡×¨×™× ×œ×©××œ×” | chat_id={safe_chat_id} | missing_fields={len(missing)}", source="gpt_a_handler")
        return "", ""
    except Exception as e:
        logger.error(f"×©×’×™××” ×‘×™×¦×™×¨×ª ×”×•×“×¢×ª ×©×“×•×ª ×—×¡×¨×™×: {e}", source="gpt_a_handler")
        return "", ""

# ××™×œ×•×ª ××¤×ª×— ×©××¦×“×™×§×•×ª ××•×“×œ ××ª×§×“×
PREMIUM_MODEL_KEYWORDS = [
    # ×–×•×’×™×•×ª ×•××¢×¨×›×•×ª ×™×—×¡×™×
    "× ×™×©×•××™×Ÿ", "×—×ª×•× ×”", "×–×•×’×™×•×ª", "××¢×¨×›×ª ×™×—×¡×™×", "×‘×Ÿ ×–×•×’", "×‘×ª ×–×•×’", "×—×‘×¨×”", "×—×‘×¨",
    "××”×‘×”", "×¨×’×©×•×ª", "×§×©×¨", "×–×™×§×”", "××©×™×›×”", "××™× ×˜×™××™×•×ª", "××™× ×™×•×ª", "×¤×¨×™×“×”", "×’×™×¨×•×©×™×Ÿ",
    "×’×¨×•×©", "× ×¤×¨×“", "× ×©×•×™", "× ×©×•××”", "××’×•×¨×©×ª", "××’×•×¨×©",
    
    # ×¤×¡×™×›×•×œ×•×’×™×” ×•×‘×¨×™××•×ª × ×¤×©
    "×¤×¡×™×›×•×œ×•×’×™×”", "×˜×™×¤×•×œ", "×™×™×¢×•×¥", "×¤×¡×™×›×•×œ×•×’", "××˜×¤×œ", "××“×›×", "×“×™×›××•×Ÿ", "×—×¨×“×”", 
    "×¤×—×“", "×“××’×”", "×‘×œ×‘×•×œ", "×œ×—×¥", "×¡×˜×¨×¡", "×˜×¨××•××”", "×¤×¦×™×¢×” × ×¤×©×™×ª", "×‘×“×™×“×•×ª",
    "×›××‘", "××•×‘×“×Ÿ", "×”×ª×¢×œ×œ×•×ª", "× ×¤×’×¢", "×¡×‘×œ", "×‘×¨×™×•× ×•×ª", "××©××”", "×‘×•×©×”",
    "×ª×§×•×¢", "×©×™×¤×•×˜ ×¢×¦××™", "×”×¡×ª×¨×”", "××•×¨×›×‘", "×§×©×” ×œ×™", "×œ× ×©×œ×", "××‘×•×“",
    
    # ×“×ª×™×•×ª ×•×××•× ×”
    "×“×ª×™×•×ª", "×—×™×œ×•× ×™", "×“×ª×™", "××¡×•×¨×ª×™", "×××•× ×”", "××¦×•×•×ª", "×”×œ×›×”", "×¨×‘", "×¨×‘× ×•×ª",
    "×›×©×¨×•×ª", "×©×‘×ª", "×—×’", "×ª×¤×™×œ×”", "×‘×™×ª ×›× ×¡×ª", "×ª×•×¨×”", "×ª×œ××•×“", "×™×”×“×•×ª",
    "×“×ª×œ×©", "×“×ª×œ×´×©", "×“×ª×™ ×œ×©×¢×‘×¨", "×—×–×¨×” ×‘×©××œ×”", "×™×©×™×‘×”", "×˜×™×¤×•×œ×™ ×”××¨×”",
    
    # ××©×¤×—×” ×•×—×™×™ ×—×‘×¨×”
    "××©×¤×—×”", "×”×•×¨×™×", "×™×œ×“×™×", "×”×¨×™×•×Ÿ", "×œ×™×“×”", "×—×™× ×•×š", "×’×™×œ", "×–×§× ×”", "×¡×‘×", "×¡×‘×ª×",
    "××‘×", "×××", "×‘×Ÿ", "×‘×ª", "××—", "××—×•×ª", "×“×•×“", "×“×•×“×”", "×‘×Ÿ ×“×•×“", "×‘×ª ×“×•×“",
    "××©×¤×—×” ××•×¨×—×‘×ª", "××©×¤×—×” ×‘×™×•×œ×•×’×™×ª", "× ×›×“×™×", "×”×”×•×¨×™×", "×—×™×™× ×›×¤×•×œ×™×",
    
    # ×¢×‘×•×“×” ×•×§×¨×™×™×¨×”
    "×¢×‘×•×“×”", "×§×¨×™×™×¨×”", "×”×©×›×œ×”", "×œ×™××•×“×™×", "××•× ×™×‘×¨×¡×™×˜×”", "××§×¦×•×¢", "×›×œ×›×œ×”", "×©×›×¨",
    "×× ×”×œ", "×¢×•×‘×“", "××¢×¡×™×§", "×¨××™×•×Ÿ ×¢×‘×•×“×”", "×§×•×¨×•×ª ×—×™×™×", "×”×©×›×œ×” ×’×‘×•×”×”",
    
    # ×‘×¨×™××•×ª ×¨×¤×•××™×ª
    "×‘×¨×™××•×ª", "×¨×•×¤×", "×—×•×œ×”", "××—×œ×”", "×ª×¨×•×¤×”", "× ×™×ª×•×—", "×‘×™×ª ×—×•×œ×™×", "×§×•×¤×ª ×—×•×œ×™×",
    "×›××‘", "×›×•××‘", "×¨×¤×•××”", "××‘×—×•×Ÿ", "×˜×™×¤×•×œ ×¨×¤×•××™", "××—×œ×” ×›×¨×•× ×™×ª", "××™×™×“×¡", "HIV", "××—×œ×•×ª",
    
    # ×”×—×œ×˜×•×ª ×•×“×™×œ××•×ª
    "×‘×¢×™×”", "×§×•×©×™", "×”×—×œ×˜×”", "×“×™×œ××”", "×‘×¨×™×¨×”", "××¤×©×¨×•×ª", "×¢×ª×™×“", "×ª×›× ×•×Ÿ", "×™×¢×“", "×—×œ×•×",
    "×œ×‘×—×•×¨", "×œ×”×—×œ×™×˜", "× ×‘×•×š", "××‘×•×œ×‘×œ", "×œ× ×™×•×“×¢", "×¢×–×¨×”", "×—×©×•×‘", "×§×¨×™×˜×™",
    "×—×¡×•×", "×œ× ××¦×œ×™×—", "××¤×—×“", "×œ× ××¢×–", "××ª×‘×™×™×©", "×“×—×™×§×”", "×”×™×× ×¢×•×ª",
    
    # × ×˜×™×™×” ××™× ×™×ª ×•×–×”×•×ª ××™× ×™×ª - LGBTQ+
    "×”×•××•", "×’×™×™", "×‘×™×¡×§×¡×•××œ", "×œ×”×˜×´×‘", "×œ×”×˜×‘", "×”×§×”×™×œ×” ×”×’××”", "× ×˜×™×™×” ××™× ×™×ª", "×–×”×•×ª ××™× ×™×ª",
    "×‘××¨×•×Ÿ", "×™×¦×™××” ××”××¨×•×Ÿ", "××¨×•× ×™×¡×˜", "×“×™×¡×§×¨×˜×™", "××¨×•×Ÿ", "×”××¨×•×Ÿ", "×‘×™", "×“×•", "× ××©×š",
    "×”×•××•×¤×•×‘×™×”", "×”×•××•×¤×•×‘×™×” ×¤× ×™××™×ª", "×”×•××•×¤×•×‘×™×” ×¢×¦××™×ª", "×œ× ××§×‘×œ ××ª ×¢×¦××™",
    "××ª×—× ×’×œ", "××•×›×œ ×‘×ª×—×ª", "×§×•×§×¡×™× ×œ", "× ×©×™", "×’×‘×¨×™", "×˜×¨× ×¡", "×§×•×•×™×¨×™",
    "×¢×œ ×”×¨×¦×£", "×¡×§×¡", "×× ××œ×™", "××•×¨××œ×™", "××¤×—×“",
    
    # ×’×™×œ ×•××¢×‘×¨×™×
    "×‘×’×™×œ ×××•×—×¨", "××¤×¡×¤×¡", "×”×¨×›×‘×ª ×¢×•×‘×¨×ª", "×–××Ÿ ×˜×¡", "×œ× ×¦×¢×™×¨", "×›×‘×¨ ×‘×Ÿ", "×›×‘×¨ ×‘×ª",
    "× ×××¡", "×œ××•×ª", "×¨×•×¦×” ×œ×”×™×•×ª ×—×•×¤×©×™", "××¨×’×™×© ×¦×¢×™×¨", "×××¦×¢ ×”×—×™×™×"
]

# ×“×¤×•×¡×™ ×‘×™×˜×•×™×™× ××•×¨×›×‘×™× (regex) - ×¨×–×” ×•×—×“
COMPLEX_PATTERNS = [
    # ×©××œ×•×ª ××•×¨×›×‘×•×ª
    r"××”\s+×¢×•×©×™×\s+×›×©|××™×š\s+×œ×”×ª××•×“×“\s+×¢×|×¦×¨×™×š\s+×¢×¦×”\s+×‘|×œ×\s+×™×•×“×¢\s+××™×š",
    # ××¦×‘×™ × ×¤×© ×§×©×™×  
    r"×œ×\s+××¨×’×™×©\s+×©×œ×|×× ×™\s+×ª×§×•×¢|××¨×’×™×©\s+××‘×•×“|×× ×™\s+×©×•× ×\s+××ª\s+×¢×¦××™",
    # ×§×‘×œ×” ×¢×¦××™×ª
    r"×œ×\s+×©×œ×\s+×¢×|×§×•×©×™\s+×œ[×§|×›]×‘×œ|×¢×“×™×™×Ÿ\s+×œ×\s+×©×œ×|×”×ª××•×“×“×•×ª\s+×¢×",
    # ×–×•×’×™×•×ª ×•×‘×“×™×“×•×ª
    r"× ×©×•×™\s+×œ|×œ×\s+××¦×œ×™×—\s+×œ××¦×•×|×™×¦××ª×™\s+××‘×œ\s+×œ×\s+×‘×××ª"
]

# ××©×ª× ×” ×’×œ×•×‘×œ×™ ×œ×¢×§×™×‘×” ××—×¨ ×”×”×—×œ×˜×•×ª
filter_decisions_log = {
    "first_20_messages": 0,
    "length": 0,
    "keywords": 0, 
    "pattern": 0,
    "default": 0
}

def log_filter_decision(match_type):
    """×¨×•×©× ×”×—×œ×˜×ª ×¤×™×œ×˜×¨ ×œ×¦×•×¨×š × ×™×ª×•×—"""
    global filter_decisions_log
    if match_type in filter_decisions_log:
        filter_decisions_log[match_type] += 1

def get_filter_analytics():
    """××—×–×™×¨ × ×™×ª×•×— ×©×œ ×”×—×œ×˜×•×ª ×”×¤×™×œ×˜×¨"""
    global filter_decisions_log
    total = sum(filter_decisions_log.values())
    if total == 0:
        return {"message": "×¢×“×™×™×Ÿ ×œ× × ×¨×©××• ×”×—×œ×˜×•×ª ×¤×™×œ×˜×¨"}
    
    percentages = {k: round((v/total)*100, 1) for k, v in filter_decisions_log.items()}
    
    return {
        "total_decisions": total,
        "breakdown": filter_decisions_log.copy(),
        "percentages": percentages,
        "premium_usage": round(((total - filter_decisions_log["default"])/total)*100, 1) if total > 0 else 0
    }

def should_use_extra_emotion_model(user_message, chat_history_length=0):
    """
    ××—×œ×™×˜ ×”×× ×œ×”×©×ª××© ×‘××•×“×œ ×”××ª×§×“× ××• ×‘××”×™×¨ ×™×•×ª×¨
    
    ×§×¨×™×˜×¨×™×•× ×™× ×œ××•×“×œ ××ª×§×“×:
    1. ğŸ†• 20 ×”×”×•×“×¢×•×ª ×”×¨××©×•× ×•×ª ×©×œ ××©×ª××© ×—×“×© (×¨×•×©× ×¨××©×•× ×™ ×—×©×•×‘)
    2. ×”×•×“×¢×” ××¨×•×›×” (××¢×œ X ××™×œ×™×)
    3. ××™×œ×•×ª ××¤×ª×— ×¨×œ×•×•× ×˜×™×•×ª
    4. ×“×¤×•×¡×™ ×‘×™×˜×•×™×™× ××•×¨×›×‘×™×
    
    Returns:
        tuple: (use_extra_emotion: bool, reason: str, match_type: str)
    """
    # ğŸ†• ×‘×“×™×§×” 1: 20 ×”×”×•×“×¢×•×ª ×”×¨××©×•× ×•×ª - ×¨×•×©× ×¨××©×•× ×™ ×—×©×•×‘
    if chat_history_length <= 20:
        logger.info(f"ğŸ¯ [PREMIUM_FILTER] 20 ×”×”×•×“×¢×•×ª ×”×¨××©×•× ×•×ª: {chat_history_length} ×”×•×“×¢×•×ª -> ××•×“×œ ××ª×§×“× (×¨×•×©× ×¨××©×•× ×™)")
        result = True, f"20 ×”×”×•×“×¢×•×ª ×”×¨××©×•× ×•×ª ({chat_history_length}/20) - ×¨×•×©× ×¨××©×•× ×™ ×—×©×•×‘", "first_20_messages"
        log_filter_decision(result[2])
        return result
    
    # ×‘×“×™×§×ª ××•×¨×š ×”×•×“×¢×”
    word_count = len(user_message.split())
    if word_count > LONG_MESSAGE_THRESHOLD:
        logger.info(f"ğŸ¯ [PREMIUM_FILTER] ×”×•×“×¢×” ××¨×•×›×”: {word_count} ××™×œ×™× -> ××•×“×œ ××ª×§×“×")
        result = True, f"×”×•×“×¢×” ××¨×•×›×” ({word_count} ××™×œ×™×)", "length"
        log_filter_decision(result[2])
        return result
    
    # ×‘×“×™×§×ª ××™×œ×•×ª ××¤×ª×—
    user_message_lower = user_message.lower()
    found_keywords = [keyword for keyword in PREMIUM_MODEL_KEYWORDS if keyword in user_message_lower]
    if found_keywords:
        logger.info(f"ğŸ¯ [PREMIUM_FILTER] ××™×œ×•×ª ××¤×ª×— × ××¦××•: {found_keywords[:3]} -> ××•×“×œ ××ª×§×“×")
        result = True, f"××™×œ×•×ª ××¤×ª×—: {', '.join(found_keywords[:3])}", "keywords"
        log_filter_decision(result[2])
        return result
    
    # ×‘×“×™×§×ª ×“×¤×•×¡×™ ×‘×™×˜×•×™×™× ××•×¨×›×‘×™×
    for pattern in COMPLEX_PATTERNS:
        if re.search(pattern, user_message_lower):
            logger.info(f"ğŸ¯ [PREMIUM_FILTER] ×“×¤×•×¡ ××•×¨×›×‘ × ××¦×: {pattern} -> ××•×“×œ ××ª×§×“×")
            result = True, f"×“×¤×•×¡ ××•×¨×›×‘ ×–×•×”×”", "pattern"
            log_filter_decision(result[2])
            return result
    
    # ××—×¨×ª, ××•×“×œ ××”×™×¨
    logger.info(f"ğŸš€ [PREMIUM_FILTER] ××§×¨×” ×¨×’×™×œ -> ××•×“×œ ××”×™×¨")
    result = False, "××§×¨×” ×¨×’×™×œ - ××•×“×œ ××”×™×¨", "default"
    log_filter_decision(result[2])
    return result

async def send_temporary_message_after_delay(update, chat_id, delay_seconds=8):
    """
    ×©×•×œ×— ×”×•×“×¢×” ×–×× ×™×ª ××—×¨×™ ×“×™×œ×™×™ ××¡×•×™× ×•××—×–×™×¨ ××ª ×”××•×‘×™×™×§×˜ Message ×©×œ×”

    ×”×©×™× ×•×™ (×”×—×–×¨×ª ×”××•×‘×™×™×§×˜ ×¢×¦××• ×•×œ× ×¨×§ ×”-id) ×××¤×©×¨ ×œ× ×• ×œ××—×•×§ ××ª ×”×”×•×“×¢×” ×‘×§×œ×•×ª ×‘×××¦×¢×•×ª
    await temp_message.delete()â€ ×‘×œ×™ ×´×¦×™×“×´ ××—×¨ ×”-bot, ××” ×©×¤×’×¢ ×‘××—×™×§×” ×‘×¢×‘×¨.
    """
    try:
        # ×‘×“×™×§×” ×× ×”××©×™××” ×‘×•×˜×œ×” ×œ×¤× ×™ ×”×©×™× ×”
        await asyncio.sleep(delay_seconds)
        
        # ×‘×“×™×§×” × ×•×¡×¤×ª ××—×¨×™ ×”×©×™× ×” - ×× ×”××©×™××” ×‘×•×˜×œ×”, ×œ× × ×©×œ×— ×”×•×“×¢×”
        if asyncio.current_task().cancelled():
            logger.info(f"ğŸ“¤ [TEMP_MSG] ××©×™××” ×‘×•×˜×œ×” ×œ×¤× ×™ ×©×œ×™×—×ª ×”×•×“×¢×” ×–×× ×™×ª | chat_id={safe_str(chat_id)}", source="gpt_a_handler")
            return None
            
        from message_handler import send_system_message  # local import to avoid circular
        temp_message_text = "â³ ×× ×™ ×¢×•×‘×“ ×¢×œ ×ª×©×•×‘×” ×‘×©×‘×™×œ×š... ×–×” ××™×“ ××¦×œ×š..."
        await send_system_message(update, chat_id, temp_message_text)
        
        # × ×—×–×™×¨ None ×›×™ send_system_message ×œ× ××—×–×™×¨ ××ª ×”××•×‘×™×™×§×˜
        logger.info(f"ğŸ“¤ [TEMP_MSG] × ×©×œ×—×” ×”×•×“×¢×” ×–×× ×™×ª | chat_id={safe_str(chat_id)}", source="gpt_a_handler")
        return None  # ×œ× ××—×–×™×¨×™× ××•×‘×™×™×§×˜ ×›×™ send_system_message ×œ× ××—×–×™×¨
    except asyncio.CancelledError:
        logger.info(f"ğŸ“¤ [TEMP_MSG] ××©×™××” ×‘×•×˜×œ×” ×‘×–××Ÿ ×©×œ×™×—×ª ×”×•×“×¢×” ×–×× ×™×ª | chat_id={safe_str(chat_id)}", source="gpt_a_handler")
        return None
    except Exception as e:
        logger.error(f"âŒ [TEMP_MSG] ×©×’×™××” ×‘×©×œ×™×—×ª ×”×•×“×¢×” ×–×× ×™×ª: {e}")
        return None

async def delete_temporary_message_and_send_new(update, temp_message, new_text):
    """
    ××•×—×§ ××ª ×”×”×•×“×¢×” ×”×–×× ×™×ª (×× ×§×™×™××ª) ×•×©×•×œ×— ×œ××©×ª××© ××ª ×”×ª×©×•×‘×” ×”×××™×ª×™×ª.

    âœ… ×©×™×¤×•×¨: ××©×ª××©×™× ×‘-send_message ×¢× is_gpt_a_response=True â€“ ×‘×˜×•×— ×•×¤×©×•×˜ ×™×•×ª×¨.
    """
    from message_handler import send_message  # local import to avoid circular

    try:
        # ××—×™×§×ª ×”×”×•×“×¢×” ×”×–×× ×™×ª - ×œ× ×¨×œ×•×•× ×˜×™ ×›×™ send_message ×œ× ××—×–×™×¨ ××•×‘×™×™×§×˜
        if temp_message is not None:
            logger.info(f"ğŸ—‘ï¸ [DELETE_MSG] ×”×•×“×¢×” ×–×× ×™×ª ×œ× × ××—×§×” (×œ× ×¨×œ×•×•× ×˜×™ ×¢× send_message)")

        # ×©×œ×™×—×ª ×”×”×•×“×¢×” ×”×—×“×©×”
        chat_id = update.message.chat_id
        await send_message(update, chat_id, new_text, is_bot_message=True, is_gpt_a_response=True)
        logger.info(f"ğŸ“¤ [NEW_MSG] × ×©×œ×—×” ×”×•×“×¢×” ×—×“×©×” | chat_id={safe_str(chat_id)}", source="gpt_a_handler")
        return True

    except Exception as send_err:
        logger.error(f"âŒ [DELETE_MSG] ×›×©×œ ×‘×©×œ×™×—×”: {send_err}")
        return False

def _execute_gpt_call(completion_params, full_messages):
    """×”×¤×¢×œ×ª ×§×¨×™××” ×œ-GPT ×‘××•×¤×Ÿ ×¡×™× ×›×¨×•× ×™"""
    try:
        response = litellm.completion(**completion_params)
        
        # ×—×™×œ×•×¥ ×”×ª×©×•×‘×”
        bot_reply = response.choices[0].message.content
        
        # ×—×™×œ×•×¥ × ×ª×•× ×™ ×©×™××•×©
        usage = normalize_usage_dict(response.usage) if hasattr(response, 'usage') else {}
        
        return {
            "bot_reply": bot_reply,
            "usage": usage,
            "model": completion_params["model"],
            "model_dump": response
        }
    except Exception as e:
        logger.error(f"[gpt_a] ×©×’×™××” ×‘××•×“×œ {completion_params['model']}: {e}")
        raise e

def get_main_response_sync(full_messages, chat_id=None, message_id=None, use_extra_emotion=True, filter_reason="", match_type="unknown"):
    """
    ğŸ’ ×× ×•×¢ gpt_a ×”×¨××©×™ - ×’×¨×¡×” ×¡×™× ×›×¨×•× ×™×ª
    """
    # ××“×™×“×ª ×–××Ÿ ×”×ª×—×œ×”
    total_start_time = time.time()
    
    # ×©×œ×‘ 1: ×”×›× ×ª ×”×”×•×“×¢×•×ª
    prep_start_time = time.time()
    
    # ×”×”×•×¡×¤×” ×©×œ ×”×©×“×•×ª ×”×—×¡×¨×™× ××ª×‘×¦×¢×ª ×›×¢×ª ×‘-message_handler.py
    # ×›×“×™ ×œ×”×™×× ×¢ ××›×¤×™×œ×•×™×•×ª, ×”×¡×¨× ×• ××ª ×”×§×•×“ ××›××Ÿ
    
    prep_time = time.time() - prep_start_time
    print(f"âš¡ [TIMING] Preparation time: {prep_time:.3f}s")
    
    metadata = {"gpt_identifier": "gpt_a", "chat_id": chat_id, "message_id": message_id}
    params = GPT_PARAMS["gpt_a"]
    
    # ğŸ”¬ ××“×™×“×ª ×‘×™×¦×•×¢×™× ××‘×•×˜×œ×ª ×–×× ×™×ª
    measurement_id = None
    
    # ×‘×—×™×¨×ª ××•×“×œ ×œ×¤×™ ×”×¤×™×œ×˜×¨
    if use_extra_emotion:
        model = GPT_MODELS["gpt_a"]  # ×”××•×“×œ ×”××ª×§×“× ×-config
        model_tier = "premium"
        logger.info(f"ğŸ¯ [MODEL_SELECTION] ××©×ª××© ×‘××•×“×œ ××ª×§×“×: {model} | ×¡×™×‘×”: {filter_reason}")
    else:
        model = GPT_FALLBACK_MODELS["gpt_a"]  # ×”××•×“×œ ×”××”×™×¨ ×-config
        model_tier = "fast"
        logger.info(f"ğŸš€ [MODEL_SELECTION] ××©×ª××© ×‘××•×“×œ ××”×™×¨: {model} | ×¡×™×‘×”: {filter_reason}")
    
    completion_params = {
        "model": model,
        "messages": full_messages,
        "temperature": params["temperature"],
        "metadata": metadata
    }
    
    # ×”×•×¡×¤×ª max_tokens ×¨×§ ×× ×”×•× ×œ× None
    if params["max_tokens"] is not None:
        completion_params["max_tokens"] = params["max_tokens"]

    # ğŸ” [DEBUG] × ×™×ª×•×— ××¤×•×¨×˜ ×©×œ ×”××‘× ×” ×©× ×©×œ×— ×œ-GPT
    print(f"\nğŸ” [GPT_REQUEST_DEBUG] === DETAILED GPT REQUEST ANALYSIS ===")
    print(f"ğŸ¤– [MODEL] {model} | ExtraEmotion: {use_extra_emotion} | Reason: {filter_reason}")
    print(f"ğŸ“Š [PARAMS] Temperature: {params['temperature']} | Max Tokens: {params.get('max_tokens', 'None')}")
    print(f"ğŸ“ [MESSAGES_COUNT] Total messages: {len(full_messages)}")
    
    # × ×™×ª×•×— ×”×•×“×¢×•×ª ×œ×¤×™ ×¡×•×’
    system_count = 0
    user_count = 0
    assistant_count = 0
    
    for i, msg in enumerate(full_messages):
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        content_length = len(content)
        
        if role == "system":
            system_count += 1
            print(f"âš™ï¸ [SYSTEM_{system_count}] Position: {i} | Length: {content_length} chars")
        elif role == "user":
            user_count += 1
            print(f"ğŸ‘¤ [USER_{user_count}] Position: {i} | Length: {content_length} chars")
        elif role == "assistant":
            assistant_count += 1
            print(f"ğŸ¤– [ASSISTANT_{assistant_count}] Position: {i} | Length: {content_length} chars")
    
    print(f"ğŸ“ˆ [SUMMARY] System: {system_count} | User: {user_count} | Assistant: {assistant_count}")
    print(f"ğŸš€ [SENDING] Request to {model}...")
    print(f"ğŸ” [GPT_REQUEST_DEBUG] === END ANALYSIS ===\n")
    
    # ×©×œ×‘ 2: ×”×¤×¢×œ×ª GPT ×‘-thread × ×¤×¨×“
    gpt_start_time = time.time()
    log_memory_usage("before_gpt_call")
    
    try:
        # ğŸš¨ ×”×’×“×œ×ª timeout ×œ-45 ×©× ×™×•×ª (×‘××§×•× 30) ×œ×˜×™×¤×•×œ ×‘-latency ×’×‘×•×”
        GPT_TIMEOUT_SECONDS = TimeoutConfig.GPT_PROCESSING_TIMEOUT
        
        # ×”×¨×¦×ª GPT ×™×©×™×¨×•×ª (×œ×œ× ×¨×§×•×¨×¡×™×”)
        gpt_result = _execute_gpt_call(completion_params, full_messages)
        
        gpt_duration = time.time() - gpt_start_time
        log_memory_usage("after_gpt_call")
        logger.info(f"â±ï¸ [GPT_TIMING] GPT ×”×¡×ª×™×™× ×ª×•×š {gpt_duration:.2f} ×©× ×™×•×ª")
        
        # ×©×œ×‘ 3: ×¢×™×‘×•×“ ×”×ª×©×•×‘×”
        processing_start_time = time.time()
        
        # ğŸ”¬ ×¨×™×©×•× ×”×˜×•×§×Ÿ ×”×¨××©×•×Ÿ ××‘×•×˜×œ ×–×× ×™×ª
        
        bot_reply = gpt_result["bot_reply"]
        
        # ğŸ†• × ×™×§×•×™ ×ª×©×•×‘×” ××˜×§×¡×˜ ×˜×›× ×™ ×•×××—×•×¨×™ ×”×§×œ×¢×™×
        bot_reply = clean_bot_response(bot_reply)
        
        # × ×™×§×•×™ ×ª×’×™ HTML ×œ× × ×ª××›×™× ×©×”××•×“×œ ×¢×œ×•×œ ×œ×”×—×–×™×¨
        # <br> ×ª×’×™× ×œ× × ×ª××›×™× ×‘-Telegram - ×¦×¨×™×š ×œ×”××™×¨ ×œ-\n
        bot_reply = bot_reply.replace('<br>', '\n').replace('<br/>', '\n').replace('<br />', '\n')
        # ×’× × ×™×§×•×™ ×ª×’×™ br ×¢× attributes ×©×•× ×™×
        bot_reply = re.sub(r'<br\s*/?>', '\n', bot_reply)
        
        usage = gpt_result["usage"]
        
        # ×”×•×¡×¤×ª × ×ª×•× ×™ ×¢×œ×•×ª ××“×•×™×§×™× ×œ-usage ×¢×œ ×¡××š completion_response
        try:
            cost_info = calculate_gpt_cost(
                prompt_tokens=usage.get("prompt_tokens", 0),
                completion_tokens=usage.get("completion_tokens", 0),
                cached_tokens=usage.get("cached_tokens", 0),
                model_name=gpt_result["model"],
                completion_response=gpt_result
            )
            usage.update(cost_info)
        except Exception as _cost_e:
            logger.warning(f"[gpt_a] ×œ× ×”×¦×œ×—×ª×™ ×œ×—×©×‘ ×¢×œ×•×ª usage: {_cost_e}")
        
        processing_time = time.time() - processing_start_time
        print(f"âš¡ [TIMING] Processing time: {processing_time:.3f}s")
        
        if should_log_debug_prints():
            print(f"[GPT_A_RESPONSE] {len(bot_reply)} chars from {gpt_result['model']}")
        print(f"âš¡ [DETAILED_TIMING] GPT pure latency: {gpt_duration:.3f}s | Model: {gpt_result['model']}")
        
        # ×©×œ×‘ 4: ×—×™×©×•×‘ ×¢×œ×•×™×•×ª
        billing_start_time = time.time()
        
        #  ××¢×§×‘ ××—×¨ ×—×™×•×‘
        if 'cost_info' in locals():
            cost_usd = cost_info.get("cost_total", 0.0)
            if cost_usd and cost_usd > 0:
                billing_status = billing_guard.add_cost(cost_usd, gpt_result["model"], "paid" if use_extra_emotion else "free")
                
                # ×”×ª×¨××•×ª ×œ××“××™×Ÿ
                if billing_status.get("warnings"):
                    for warning in billing_status["warnings"]:
                        logger.warning(f"[ğŸ’° ×ª×§×¦×™×‘] {warning}")
                
                # ×”×ª×¨××” ×‘×˜×œ×’×¨× ×× ×¦×¨×™×š
                status = billing_guard.get_current_status()
                alert_billing_issue(
                    cost_usd=cost_usd,
                    model_name=gpt_result["model"],
                    tier="paid" if use_extra_emotion else "free",
                    daily_usage=status["daily_usage"],
                    monthly_usage=status["monthly_usage"],
                    daily_limit=status["daily_limit"],
                    monthly_limit=status["monthly_limit"]
                )
            
        billing_time = time.time() - billing_start_time
        print(f"âš¡ [TIMING] Billing time: {billing_time:.3f}s")
        
        # ×¡×™×›×•× ×–×× ×™×
        total_time = time.time() - total_start_time
        print(f"ğŸ“Š [TIMING_SUMMARY] Total: {total_time:.3f}s | GPT: {gpt_duration:.3f}s | Prep: {prep_time:.3f}s | Processing: {processing_time:.3f}s | Billing: {billing_time:.3f}s")
        
        # ğŸ’¾ ×©××™×¨×ª ××˜×¨×™×§×•×ª ×–××Ÿ ××¤×•×¨×˜×•×ª ×œ××¡×“ ×”× ×ª×•× ×™×
        try:
            from db_manager import save_system_metrics
            save_system_metrics(
                metric_type="gpt_timing",
                chat_id=str(chat_id) if chat_id else None,
                gpt_latency_seconds=gpt_duration,
                prep_time_seconds=prep_time,
                processing_time_seconds=processing_time,
                billing_time_seconds=billing_time,
                response_time_seconds=total_time,
                additional_data={
                    "message_id": message_id,
                    "gpt_type": "A",
                    "model": gpt_result["model"],
                    "extra_emotion": use_extra_emotion,
                    "filter_reason": filter_reason,
                    "match_type": match_type,
                    "tokens_used": usage.get("total_tokens", 0),
                    "cost_usd": usage.get("cost_total", 0)
                }
            )
        except Exception as save_err:
            logger.warning(f"Could not save GPT timing metrics: {save_err}")
        
        # ğŸ†• ×‘×“×™×§×” ×× ×”×ª×©×•×‘×” ××›×™×œ×” ×©××œ×ª ×¤×¨×•×¤×™×œ ×•×”×ª×—×œ×ª ×¤×¡×§ ×–××Ÿ
        if chat_id and detect_profile_question_in_response(bot_reply):
            start_profile_question_cooldown(chat_id)
            logger.info(f"âœ… [PROFILE_QUESTION] ×”×•×¤×¢×œ ×¤×¡×§ ×–××Ÿ! | chat_id={safe_str(chat_id)}", source="gpt_a_handler")
        
        result = {
            "bot_reply": bot_reply, 
            "usage": usage, 
            "model": gpt_result["model"],
            "used_extra_emotion": use_extra_emotion,
            "filter_reason": filter_reason,
            "match_type": match_type,
            "gpt_pure_latency": gpt_duration,
            "total_time": total_time,
            "prep_time": prep_time,
            "processing_time": processing_time,
            "billing_time": billing_time
        }
        try:
            from gpt_jsonl_logger import GPTJSONLLogger
            # ×‘× ×™×™×ª response ××œ× ×¢× ×›×œ ×”××™×“×¢ ×”× ×“×¨×©
            response_data = {
                "id": getattr(gpt_result.get("model_dump"), "id", ""),
                "choices": [
                    {
                        "message": {
                            "content": gpt_result["bot_reply"],
                            "role": "assistant"
                        }
                    }
                ],
                "usage": gpt_result["usage"],
                "model": gpt_result["model"]
            }
            GPTJSONLLogger.log_gpt_call(
                log_path="data/openai_calls.jsonl",
                gpt_type="A",
                request=completion_params,
                response=response_data,
                cost_usd=usage.get("cost_total", 0),
                extra={
                    "chat_id": chat_id, 
                    "message_id": message_id,
                    "gpt_pure_latency": gpt_duration,
                    "total_time": total_time,
                    "prep_time": prep_time,
                    "processing_time": processing_time,
                    "billing_time": billing_time
                }
            )
        except Exception as log_exc:
            print(f"[LOGGING_ERROR] Failed to log GPT-A call: {log_exc}")
        return result
        
    except Exception as e:
        logger.error(f"[gpt_a] ×©×’×™××” ×‘××•×“×œ {gpt_result['model']}: {e}")
        
        # ×©×œ×™×—×ª ×”×•×“×¢×ª ×©×’×™××” ×˜×›× ×™×ª ×œ××“××™×Ÿ
        send_error_notification(
            error_message=f"×©×’×™××” ×›×œ×œ×™×ª ×‘-get_main_response_sync: {str(e)}",
            chat_id=chat_id,
            user_msg=full_messages[-1]["content"] if full_messages else "×œ× ×–××™×Ÿ", 
            error_type="gpt_a_engine_error"
        )
        
        error_reply = "××¦×˜×¢×¨, ×™×© ×œ×™ ×‘×¢×™×” ×˜×›× ×™×ª ×–×× ×™×ª. ×”×¢×‘×¨×ª×™ ××ª ×”×¤×¨×˜×™× ×œ×¢×•××¨ ×©×™×‘×“×•×§ ××ª ×–×”. × ×¡×” ×©×•×‘ ×‘×¢×•×“ ×›××” ×“×§×•×ª ğŸ”§"
        # ğŸ› DEBUG: ×©×œ×™×—×ª ×”×•×“×¢×ª ×©×’×™××”
        print("=" * 80)
        print("âŒ ERROR MESSAGE DEBUG")
        print("=" * 80)
        print(f"ğŸ“ ERROR TEXT: {repr(error_reply)}")
        print(f"ğŸ“Š LENGTH: {len(error_reply)} chars")
        print(f"ğŸ“Š NEWLINES: {error_reply.count(chr(10))}")
        print(f"ğŸ“Š DOTS: {error_reply.count('.')}")
        print(f"ğŸ“Š QUESTIONS: {error_reply.count('?')}")
        print(f"ğŸ“Š EXCLAMATIONS: {error_reply.count('!')}")
        print("=" * 80)
        
        # ğŸ†• ×‘×“×™×§×” ×× ×”×ª×©×•×‘×” ××›×™×œ×” ×©××œ×ª ×¤×¨×•×¤×™×œ ×•×”×ª×—×œ×ª ×¤×¡×§ ×–××Ÿ (×’× ×‘××§×¨×” ×©×’×™××”)
        if chat_id and detect_profile_question_in_response(error_reply):
            start_profile_question_cooldown(chat_id)
        
        return {
            "bot_reply": error_reply, 
            "usage": {}, 
            "model": gpt_result["model"],
            "used_extra_emotion": use_extra_emotion,
            "filter_reason": filter_reason,
            "match_type": match_type,
            "error": str(e)
        }

async def get_main_response_with_timeout(full_messages, chat_id=None, message_id=None, update=None):
    """
    ğŸ’ ×©×•×œ×— ×”×•×“×¢×” ×œ-gpt_a ×¢× × ×™×”×•×œ ×—×›× ×©×œ ×–×× ×™ ×ª×’×•×‘×”
    """
    # ×©×œ×‘ 1: ×§×‘×™×¢×ª ××•×“×œ ×œ×¤×™ ×¤×™×œ×˜×¨ ×—×›×
    user_message = full_messages[-1]["content"] if full_messages else ""
    
    # ğŸ†• ×§×‘×œ×ª ××¡×¤×¨ ×”×”×•×“×¢×•×ª ×”×××™×ª×™ ××”×”×™×¡×˜×•×¨×™×”
    chat_history_length = 0
    if chat_id:
        try:
            from chat_utils import get_user_stats
            user_stats = get_user_stats(chat_id)
            chat_history_length = user_stats.get("total_messages", 0)
        except Exception as e:
            logger.warning(f"×©×’×™××” ×‘×§×‘×œ×ª ××¡×¤×¨ ×”×•×“×¢×•×ª ××”×”×™×¡×˜×•×¨×™×”: {e}")
            # fallback ×œ×¡×¤×™×¨×” ×-full_messages
            chat_history_length = len([msg for msg in full_messages if msg["role"] in ["user", "assistant"]])
    else:
        # ×× ××™×Ÿ chat_id, × ×©×ª××© ×‘×¡×¤×™×¨×” ×-full_messages
        chat_history_length = len([msg for msg in full_messages if msg["role"] in ["user", "assistant"]])
    
    use_extra_emotion, filter_reason, match_type = should_use_extra_emotion_model(user_message, chat_history_length)
    
    # ×©×œ×‘ 2: ×”×¤×¢×œ×ª GPT ×‘-thread × ×¤×¨×“ ×¢× timeout ×××™×ª×™
    gpt_start_time = time.time()
    
    try:
        # ğŸš¨ ×”×’×“×œ×ª timeout ×œ-45 ×©× ×™×•×ª (×‘××§×•× 30) ×œ×˜×™×¤×•×œ ×‘-latency ×’×‘×•×”
        GPT_TIMEOUT_SECONDS = TimeoutConfig.GPT_PROCESSING_TIMEOUT
        
        # ğŸ”§ ×ª×™×§×•×Ÿ ×§×¨×™×˜×™: ×™×™×©×•× timeout ×××™×ª×™ ×‘××§×•× ×¨×§ ×”×’×“×¨×”
        # ×”×¨×¦×ª GPT ×‘-thread ×¢× timeout ××ª×§×“×
        loop = asyncio.get_event_loop()
        gpt_task = loop.run_in_executor(
            None, 
            get_main_response_sync, 
            full_messages, 
            chat_id, 
            message_id, 
            use_extra_emotion, 
            filter_reason,
            match_type
        )
        
        # ×™×™×©×•× timeout ×××™×ª×™ ×¢× asyncio.wait_for
        gpt_result = await asyncio.wait_for(gpt_task, timeout=GPT_TIMEOUT_SECONDS)
        
        gpt_duration = time.time() - gpt_start_time
        logger.info(f"â±ï¸ [GPT_TIMING] GPT ×”×¡×ª×™×™× ×ª×•×š {gpt_duration:.2f} ×©× ×™×•×ª")
        
        # ğŸ†• ×‘×“×™×§×” ×× ×”×ª×©×•×‘×” ××›×™×œ×” ×©××œ×ª ×¤×¨×•×¤×™×œ ×•×”×ª×—×œ×ª ×¤×¡×§ ×–××Ÿ
        if chat_id and detect_profile_question_in_response(gpt_result["bot_reply"]):
            start_profile_question_cooldown(chat_id)
        
        # ××“×™×“×” ××¤×•×¨×˜×ª ×©×œ ×”×–×× ×™×
        print(f"ğŸ“Š [TIMING_BREAKDOWN] Total GPT time: {gpt_duration:.3f}s")
        if 'gpt_pure_latency' in gpt_result:
            pure_latency = gpt_result.get('gpt_pure_latency', 0)
            total_time = gpt_result.get('total_time', 0)
            prep_time = gpt_result.get('prep_time', 0)
            processing_time = gpt_result.get('processing_time', 0)
            billing_time = gpt_result.get('billing_time', 0)
            
            print(f"ğŸ“Š [TIMING_BREAKDOWN] Pure GPT latency: {pure_latency:.3f}s")
            print(f"ğŸ“Š [TIMING_BREAKDOWN] Preparation: {prep_time:.3f}s | Processing: {processing_time:.3f}s | Billing: {billing_time:.3f}s")
            print(f"ğŸ“Š [TIMING_BREAKDOWN] Total internal time: {total_time:.3f}s | Thread overhead: {gpt_duration - total_time:.3f}s")
        
        # ×©×œ×‘ 3: ×©×œ×™×—×ª ×”×•×“×¢×” ×–×× ×™×ª ×× GPT ××™×˜×™
        if gpt_duration >= 8.0 and update:
            try:
                from message_handler import send_system_message
                temp_message_text = "â³ ×× ×™ ×¢×•×‘×“ ×¢×œ ×ª×©×•×‘×” ×‘×©×‘×™×œ×š... ×–×” ××™×“ ××¦×œ×š..."
                await send_system_message(update, chat_id, temp_message_text)
                logger.info(f"ğŸ“¤ [TEMP_MSG] × ×©×œ×—×” ×”×•×“×¢×” ×–×× ×™×ª | chat_id={safe_str(chat_id)}", source="gpt_a_handler")
            except Exception as temp_err:
                logger.warning(f"âš ï¸ [TEMP_MSG] ×œ× ×”×¦×œ×—×ª×™ ×œ×©×œ×•×— ×”×•×“×¢×” ×–×× ×™×ª: {temp_err}")
        
        return gpt_result
        
    except asyncio.TimeoutError:
        logger.error(f"[gpt_a] Timeout - GPT ×œ× ×”×’×™×‘ ×ª×•×š {GPT_TIMEOUT_SECONDS} ×©× ×™×•×ª")
        
        # ×©×œ×™×—×ª ×”×ª×¨××” ×œ××“××™×Ÿ ×¢×œ timeout
        send_error_notification(
            error_message=f"GPT timeout - ×œ× ×”×’×™×‘ ×ª×•×š {GPT_TIMEOUT_SECONDS} ×©× ×™×•×ª",
            chat_id=chat_id,
            user_msg=full_messages[-1]["content"] if full_messages else "×œ× ×–××™×Ÿ", 
            error_type="gpt_a_timeout_error"
        )
        
        return {
            "bot_reply": "××¦×˜×¢×¨, ×× ×™ ×¢×¡×•×§ ×›×¨×’×¢. × ×¡×” ×©×•×‘ ×‘×¢×•×“ ×›××” ×“×§×•×ª ğŸ”„", 
            "usage": {}, 
            "model": "timeout",
            "used_extra_emotion": use_extra_emotion,
            "filter_reason": filter_reason,
            "match_type": match_type,
            "error": "timeout"
        }
        
    except Exception as e:
        logger.error(f"[gpt_a] ×©×’×™××” ×›×œ×œ×™×ª: {e}")
        
        # ×©×œ×™×—×ª ×”×•×“×¢×ª ×©×’×™××” ×˜×›× ×™×ª ×œ××“××™×Ÿ
        send_error_notification(
            error_message=f"×©×’×™××” ×›×œ×œ×™×ª ×‘-get_main_response_with_timeout: {str(e)}",
            chat_id=chat_id,
            user_msg=full_messages[-1]["content"] if full_messages else "×œ× ×–××™×Ÿ", 
            error_type="gpt_a_timeout_error"
        )
        
        # ğŸ†• ×‘×“×™×§×” ×× ×”×ª×©×•×‘×” ××›×™×œ×” ×©××œ×ª ×¤×¨×•×¤×™×œ ×•×”×ª×—×œ×ª ×¤×¡×§ ×–××Ÿ (×’× ×‘××§×¨×” ×©×’×™××”)
        error_reply = "××¦×˜×¢×¨, ×™×© ×œ×™ ×‘×¢×™×” ×˜×›× ×™×ª ×–×× ×™×ª. ×”×¢×‘×¨×ª×™ ××ª ×”×¤×¨×˜×™× ×œ×¢×•××¨ ×©×™×‘×“×•×§ ××ª ×–×”. × ×¡×” ×©×•×‘ ×‘×¢×•×“ ×›××” ×“×§×•×ª ğŸ”§"
        if chat_id and detect_profile_question_in_response(error_reply):
            start_profile_question_cooldown(chat_id)
        
        return {
            "bot_reply": "××¦×˜×¢×¨, ×™×© ×œ×™ ×‘×¢×™×” ×˜×›× ×™×ª ×–×× ×™×ª. ×”×¢×‘×¨×ª×™ ××ª ×”×¤×¨×˜×™× ×œ×¢×•××¨ ×©×™×‘×“×•×§ ××ª ×–×”. × ×¡×” ×©×•×‘ ×‘×¢×•×“ ×›××” ×“×§×•×ª ğŸ”§", 
            "usage": {}, 
            "model": "error",
            "used_extra_emotion": use_extra_emotion,
            "filter_reason": filter_reason,
            "match_type": match_type,
            "error": str(e)
        }

# ×¤×•× ×§×¦×™×” ×™×©× ×” ×œ×ª××™××•×ª ×œ××—×•×¨
def get_main_response(full_messages, chat_id=None, message_id=None):
    """
    ğŸ’ ×’×¨×¡×” ×¡×™× ×›×¨×•× ×™×ª ×™×©× ×” - ×œ×ª××™××•×ª ×œ××—×•×¨
    """
    user_message = full_messages[-1]["content"] if full_messages else ""
    
    # ğŸ†• ×§×‘×œ×ª ××¡×¤×¨ ×”×”×•×“×¢×•×ª ×”×××™×ª×™ ××”×”×™×¡×˜×•×¨×™×”
    chat_history_length = 0
    if chat_id:
        try:
            from chat_utils import get_user_stats
            user_stats = get_user_stats(chat_id)
            chat_history_length = user_stats.get("total_messages", 0)
        except Exception as e:
            logger.warning(f"×©×’×™××” ×‘×§×‘×œ×ª ××¡×¤×¨ ×”×•×“×¢×•×ª ××”×”×™×¡×˜×•×¨×™×”: {e}")
            # fallback ×œ×¡×¤×™×¨×” ×-full_messages
            chat_history_length = len([msg for msg in full_messages if msg["role"] in ["user", "assistant"]])
    else:
        # ×× ××™×Ÿ chat_id, × ×©×ª××© ×‘×¡×¤×™×¨×” ×-full_messages
        chat_history_length = len([msg for msg in full_messages if msg["role"] in ["user", "assistant"]])
    
    use_extra_emotion, filter_reason, match_type = should_use_extra_emotion_model(user_message, chat_history_length)
    
    return get_main_response_sync(full_messages, chat_id, message_id, use_extra_emotion, filter_reason, match_type)

def detect_profile_question_in_response(bot_reply: str) -> bool:
    """
    ××–×”×” ×× ×”×ª×©×•×‘×” ×©×œ ×”×‘×•×˜ ××›×™×œ×” ×©××œ×ª ×¤×¨×•×¤×™×œ
    
    ×‘×•×“×§:
    - ×”×× ×™×© ×©××œ×•×ª ×¢× ×¡×™××Ÿ ×©××œ×”
    - ×”×× ×™×© ××™×œ×•×ª ××¤×ª×— ×©×œ ×©××œ×•×ª ×¤×¨×•×¤×™×œ
    - ×”×× ×™×© ×˜×§×¡×˜ ××•×“×’×© (×©××œ×•×ª ×¤×¨×•×¤×™×œ × ×›×ª×‘×•×ª ×‘××•×“×’×©)
    
    Returns:
        bool: True ×× ×–×•×”×ª×” ×©××œ×ª ×¤×¨×•×¤×™×œ, False ××—×¨×ª
    """
    if not bot_reply:
        return False
    
    # ××™×œ×•×ª ××¤×ª×— ×©×œ ×©××œ×•×ª ×¤×¨×•×¤×™×œ
    profile_keywords = [
        "×’×™×œ", "×‘×Ÿ ×›××”", "×‘×ª ×›××”", "×›××” ×©× ×™×",
        "×œ××™ ××ª/×” × ××©×š", "× ×˜×™×™×” ××™× ×™×ª", "×”×•××•", "×œ×¡×‘×™×ª", "×‘×™×¡×§×¡×•××œ",
        "××¢×¨×›×ª ×™×—×¡×™×", "×–×•×’×™×•×ª", "× ×©×•×™", "× ×©×•××”", "×¨×•×•×§", "×¨×•×•×§×”",
        "×“×ª×™", "×—×™×œ×•× ×™", "××¡×•×¨×ª×™", "×××•× ×”", "×“×ª×™×•×ª",
        "×‘××¨×•×Ÿ", "×™×¦×™××” ××”××¨×•×Ÿ", "××™ ×™×•×“×¢", "××™ ×œ× ×™×•×“×¢",
        "×¢×‘×•×“×”", "××§×¦×•×¢", "×ª×¤×§×™×“", "×œ×™××•×“×™×", "××•× ×™×‘×¨×¡×™×˜×”",
        "×‘×¢×™×” ×¢×™×§×¨×™×ª", "×§×•× ×¤×œ×™×§×˜", "××˜×¨×”", "×™×¢×“"
    ]
    
    # ×‘×“×™×§×” 1: ×¡×™××Ÿ ×©××œ×”
    has_question_mark = "?" in bot_reply
    
    # ×‘×“×™×§×” 2: ××™×œ×•×ª ××¤×ª×—
    has_profile_keywords = any(keyword in bot_reply for keyword in profile_keywords)
    
    # ×‘×“×™×§×” 3: ×˜×§×¡×˜ ××•×“×’×© (×©××œ×•×ª ×¤×¨×•×¤×™×œ × ×›×ª×‘×•×ª ×‘××•×“×’×©)
    has_bold_text = "<b>" in bot_reply and "</b>" in bot_reply
    
    # ×‘×“×™×§×” 4: ××©×¤×˜×™× ×©××ª×—×™×œ×™× ×‘××™×œ×•×ª ×©××œ×”
    question_starters = ["××”", "××™×š", "××ª×™", "××™×¤×”", "×œ××”", "×”××", "×”×× ××ª/×”", "×”×× ××ª×”"]
    has_question_starters = any(starter in bot_reply for starter in question_starters)
    
    # ×× ×™×© ×œ×¤×—×•×ª 2 ××™× ×“×™×§×˜×•×¨×™× - ×–×• ×›× ×¨××” ×©××œ×ª ×¤×¨×•×¤×™×œ
    indicators = sum([has_question_mark, has_profile_keywords, has_bold_text, has_question_starters])
    
    is_profile_question = indicators >= 2
    
    if is_profile_question:
        logger.info(f"ğŸ“Š [PROFILE_QUESTION] ×–×•×”×ª×” ×©××œ×ª ×¤×¨×•×¤×™×œ ×‘×ª×©×•×‘×” | indicators={indicators} | has_question_mark={has_question_mark} | has_keywords={has_profile_keywords} | has_bold={has_bold_text} | has_starters={has_question_starters}")
    
    return is_profile_question

# ğŸ†• ×¤×•× ×§×¦×™×” ×œ× ×™×§×•×™ ×ª×©×•×‘×•×ª ××˜×§×¡×˜ ×˜×›× ×™
def clean_bot_response(bot_reply: str) -> str:
    """
    ×× ×§×” ××ª ×ª×©×•×‘×ª ×”×‘×•×˜ ××˜×§×¡×˜ ×˜×›× ×™ ×•×××—×•×¨×™ ×”×§×œ×¢×™×
    """
    if not bot_reply:
        return bot_reply
    
    # × ×™×§×•×™ Self-correction ×•-Thinking Process
    patterns_to_remove = [
        # Self-correction
        r'---\s*Self-correction.*?(?=\n\n|\n---|\Z)',
        r'---\s*Self-correction.*',
        
        # Thinking Process
        r'---\s*\[Thinking Process\].*?(?=\n\n|\n---|\Z)',
        r'---\s*\[Thinking Process\].*',
        
        # ×›×œ ×”×˜×§×¡×˜ ××—×¨×™ ---
        r'---\s*.*?(?=\n\n|\Z)',
        r'---\s*.*',
        
        # ×”×¢×¨×•×ª ×˜×›× ×™×•×ª
        r'\(×”××¢×¨×›×ª ×ª××©×™×š ×›×¨×’×™×œ.*?\)',
        r'\(.*?×”××¢×¨×›×ª.*?\)',
        
        # ×˜×§×¡×˜ ×‘×× ×’×œ×™×ª ×˜×›× ×™
        r'This adheres to the safety protocol\..*',
        r'The instruction states.*',
        r'While it\'s not a direct.*',
        r'Given the context.*',
        r'It\'s safer to err.*',
        
        # × ×™×§×•×™ ×©×•×¨×•×ª ×¨×™×§×•×ª ×›×¤×•×œ×•×ª
        r'\n\s*\n\s*\n+',
    ]
    
    cleaned_reply = bot_reply
    
    for pattern in patterns_to_remove:
        cleaned_reply = re.sub(pattern, '', cleaned_reply, flags=re.DOTALL | re.IGNORECASE)
    
    # × ×™×§×•×™ ×©×•×¨×•×ª ×¨×™×§×•×ª ×‘×ª×—×™×œ×ª ×•×‘×¡×•×£
    cleaned_reply = cleaned_reply.strip()
    
    # ×× ×”×ª×©×•×‘×” ×¨×™×§×” ××—×¨×™ ×”× ×™×§×•×™, × ×—×–×™×¨ ×ª×©×•×‘×” ×‘×¨×™×¨×ª ××—×“×œ
    if not cleaned_reply.strip():
        return "×× ×™ ×›××Ÿ ××™×ª×š. ××” ×¢×•×œ×” ×œ×š ×›×¨×’×¢? ğŸ¤”"
    
    return cleaned_reply

# ğŸ§  Memory logging helper
def log_memory_usage(stage: str):
    """Log current memory usage"""
    try:
        import psutil
        import os
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        logger.info(f"[MEMORY] GPT-A {stage}: {memory_mb:.1f} MB")
        
        # ğŸ’¾ ×©××™×¨×ª ××“×™×“×ª ×–×™×›×¨×•×Ÿ ×œ××¡×“ ×”× ×ª×•× ×™×
        try:
            from db_manager import save_system_metrics
            save_system_metrics(
                metric_type="memory",
                memory_mb=memory_mb,
                memory_stage=f"gpt_a_{stage}",
                additional_data={"component": "gpt_a", "stage": stage}
            )
        except Exception as save_err:
            logger.warning(f"Could not save memory metrics: {save_err}")
            
    except Exception as e:
        logger.warning(f"Could not log memory usage: {e}")