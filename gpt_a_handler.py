"""
gpt_a_handler.py
----------------
×× ×•×¢ gpt_a: ×œ×•×’×™×§×ª ×”×ª×©×•×‘×” ×”×¨××©×™×ª (main response logic)
×¢× ×× ×’× ×•×Ÿ ×”×•×“×¢×” ×–×× ×™×ª ×•×¤×™×œ×˜×¨ ×—×›× ×œ×‘×—×™×¨×ª ××•×“×œ
"""

import logging
from datetime import datetime
import json
import litellm
import asyncio
import threading
import time
from prompts import SYSTEM_PROMPT
from config import GPT_MODELS, GPT_PARAMS
from gpt_utils import normalize_usage_dict
from gpt_utils import billing_guard
from notifications import alert_billing_issue

# ×™×™×‘×•× ×”×¤×™×œ×˜×¨ ×”×—×›×
from smart_model_filter import should_use_premium_model

async def send_temporary_message_after_delay(update, chat_id, delay_seconds=5):
    """
    ×©×•×œ×— ×”×•×“×¢×” ×–×× ×™×ª ××—×¨×™ ×“×™×œ×™×™ ××¡×•×™× ×•××—×–×™×¨ ××ª ×”-message_id ×©×œ×”
    """
    await asyncio.sleep(delay_seconds)
    try:
        temp_message = await update.message.reply_text("â³ ×× ×™ ×¢×•×‘×“ ×¢×œ ×ª×©×•×‘×” ×‘×©×‘×™×œ×š... ×–×” ××™×“ ××¦×œ×š...")
        logging.info(f"ğŸ“¤ [TEMP_MSG] × ×©×œ×—×” ×”×•×“×¢×” ×–×× ×™×ª | chat_id={chat_id} | message_id={temp_message.message_id}")
        return temp_message.message_id
    except Exception as e:
        logging.error(f"âŒ [TEMP_MSG] ×©×’×™××” ×‘×©×œ×™×—×ª ×”×•×“×¢×” ×–×× ×™×ª: {e}")
        return None

async def edit_temporary_message(update, chat_id, temp_message_id, new_text):
    """
    ××—×œ×™×£ ×”×•×“×¢×” ×–×× ×™×ª ×‘×ª×©×•×‘×” ×”×××™×ª×™×ª
    """
    try:
        await update.message.bot.edit_message_text(
            chat_id=chat_id,
            message_id=temp_message_id,
            text=new_text,
            parse_mode="HTML"
        )
        logging.info(f"âœ… [EDIT_MSG] ×”×•×“×¢×” ×–×× ×™×ª ×”×•×—×œ×¤×” | chat_id={chat_id} | message_id={temp_message_id}")
        return True
    except Exception as e:
        logging.error(f"âŒ [EDIT_MSG] ×©×’×™××” ×‘×¢×¨×™×›×ª ×”×•×“×¢×” ×–×× ×™×ª: {e}")
        # ×× ×”×¢×¨×™×›×” × ×›×©×œ×”, × ×©×œ×— ×”×•×“×¢×” ×—×“×©×”
        try:
            await update.message.reply_text(new_text, parse_mode="HTML")
            logging.info(f"ğŸ“¤ [FALLBACK_MSG] × ×©×œ×—×” ×”×•×“×¢×” ×—×“×©×” ×‘××§×•× ×¢×¨×™×›×” | chat_id={chat_id}")
            return True
        except Exception as e2:
            logging.error(f"âŒ [FALLBACK_MSG] ×©×’×™××” ×’× ×‘×”×•×“×¢×” ×—×“×©×”: {e2}")
            return False

def get_main_response_sync(full_messages, chat_id=None, message_id=None, use_premium=True, filter_reason=""):
    """
    ×’×¨×¡×” ×¡×™× ×›×¨×•× ×™×ª ×©×œ get_main_response - ×œ×©×™××•×© ×‘-thread
    """
    metadata = {"gpt_identifier": "gpt_a", "chat_id": chat_id, "message_id": message_id}
    params = GPT_PARAMS["gpt_a"]
    
    # ×‘×—×™×¨×ª ××•×“×œ ×œ×¤×™ ×”×¤×™×œ×˜×¨
    if use_premium:
        model = GPT_MODELS["gpt_a"]  # gemini/gemini-2.5-pro
        logging.info(f"ğŸ¯ [MODEL_SELECTION] ××©×ª××© ×‘××•×“×œ ××ª×§×“×: {model} | ×¡×™×‘×”: {filter_reason}")
    else:
        model = "gemini/gemini-1.5-flash"  # ××”×™×¨ ×•××™×›×•×ª×™
        logging.info(f"ğŸš€ [MODEL_SELECTION] ××©×ª××© ×‘××•×“×œ ××”×™×¨: {model} | ×¡×™×‘×”: {filter_reason}")
    
    completion_params = {
        "model": model,
        "messages": full_messages,
        "temperature": params["temperature"],
        "metadata": metadata,
        "store": True
    }
    
    # ×”×•×¡×¤×ª max_tokens ×¨×§ ×× ×”×•× ×œ× None
    if params["max_tokens"] is not None:
        completion_params["max_tokens"] = params["max_tokens"]
    
    try:
        import litellm
        response = litellm.completion(**completion_params)
        
        bot_reply = response.choices[0].message.content.strip()
        usage = normalize_usage_dict(response.usage, response.model)
        
        # ğŸ“Š ××¢×§×‘ ××—×¨ ×—×™×•×‘
        if hasattr(response, 'usage'):
            try:
                cost_usd = litellm.completion_cost(completion_response=response)
                if cost_usd > 0:
                    billing_status = billing_guard.add_cost(cost_usd, response.model, "paid" if use_premium else "free")
                    
                    # ×”×ª×¨××•×ª ×œ××“××™×Ÿ
                    if billing_status["warnings"]:
                        for warning in billing_status["warnings"]:
                            logging.warning(f"[ğŸ’° ×ª×§×¦×™×‘] {warning}")
                    
                    # ×”×ª×¨××” ×‘×˜×œ×’×¨× ×× ×¦×¨×™×š
                    status = billing_guard.get_current_status()
                    alert_billing_issue(
                        cost_usd=cost_usd,
                        model_name=response.model,
                        tier="paid" if use_premium else "free",
                        daily_usage=status["daily_usage"],
                        monthly_usage=status["monthly_usage"],
                        daily_limit=status["daily_limit"],
                        monthly_limit=status["monthly_limit"]
                    )
                
            except Exception as cost_error:
                logging.error(f"[ğŸ’°] ×©×’×™××” ×‘×—×™×©×•×‘ ×¢×œ×•×ª: {cost_error}")
        
        return {
            "bot_reply": bot_reply, 
            "usage": usage, 
            "model": response.model,
            "used_premium": use_premium,
            "filter_reason": filter_reason
        }
        
    except Exception as e:
        logging.error(f"[gpt_a] ×©×’×™××” ×‘××•×“×œ {model}: {e}")
        return {
            "bot_reply": "[×©×’×™××” ×‘×× ×•×¢ ×”×¨××©×™ - × ×¡×” ×©×•×‘]", 
            "usage": {}, 
            "model": model,
            "used_premium": use_premium,
            "filter_reason": filter_reason,
            "error": str(e)
        }

async def get_main_response_with_timeout(full_messages, chat_id=None, message_id=None, update=None):
    """
    ğŸ’ ×©×•×œ×— ×”×•×“×¢×” ×œ-gpt_a ×¢× × ×™×”×•×œ ×—×›× ×©×œ ×–×× ×™ ×ª×’×•×‘×” ×•×”×•×“×¢×•×ª ×–×× ×™×•×ª
    """
    # ×©×œ×‘ 1: ×§×‘×™×¢×ª ××•×“×œ ×œ×¤×™ ×¤×™×œ×˜×¨ ×—×›×
    user_message = full_messages[-1]["content"] if full_messages else ""
    chat_history_length = len([msg for msg in full_messages if msg["role"] in ["user", "assistant"]])
    
    use_premium, filter_reason = should_use_premium_model(user_message, chat_history_length)
    
    # ×©×œ×‘ 2: ×”×›× ×ª ×˜×™×™××¨ ×œ×”×•×“×¢×” ×–×× ×™×ª
    temp_message_task = None
    temp_message_id = None
    
    if update and chat_id:
        # ×”×ª×—×œ×ª ×˜×™×™××¨ ×œ×”×•×“×¢×” ×–×× ×™×ª (××—×¨×™ 5 ×©× ×™×•×ª)
        temp_message_task = asyncio.create_task(
            send_temporary_message_after_delay(update, chat_id, delay_seconds=5)
        )
    
    # ×©×œ×‘ 3: ×”×¤×¢×œ×ª GPT ×‘-thread × ×¤×¨×“
    gpt_start_time = time.time()
    
    try:
        # ×”×¨×¦×ª GPT ×‘-thread ×›×“×™ ×©×œ× ×œ×—×¡×•× ××ª ×”××™×¨×•×¢×™×
        loop = asyncio.get_event_loop()
        gpt_result = await loop.run_in_executor(
            None, 
            get_main_response_sync, 
            full_messages, 
            chat_id, 
            message_id, 
            use_premium, 
            filter_reason
        )
        
        gpt_duration = time.time() - gpt_start_time
        logging.info(f"â±ï¸ [GPT_TIMING] GPT ×”×¡×ª×™×™× ×ª×•×š {gpt_duration:.2f} ×©× ×™×•×ª")
        
        # ×©×œ×‘ 4: ×‘×™×˜×•×œ ××• ×¢×“×›×•×Ÿ ×”×•×“×¢×” ×–×× ×™×ª
        if temp_message_task:
            if not temp_message_task.done():
                # GPT ×”×¡×ª×™×™× ×œ×¤× ×™ 5 ×©× ×™×•×ª - ××‘×˜×œ×™× ×”×•×“×¢×” ×–×× ×™×ª
                temp_message_task.cancel()
                logging.info(f"âœ… [TIMING] GPT ××”×™×¨ ({gpt_duration:.1f}s) - ×”×•×“×¢×” ×–×× ×™×ª ×‘×•×˜×œ×”")
            else:
                # ×”×•×“×¢×” ×–×× ×™×ª ×›×‘×¨ × ×©×œ×—×” - ××—×œ×™×¤×™× ××•×ª×”
                temp_message_id = await temp_message_task
                if temp_message_id and update and chat_id:
                    success = await edit_temporary_message(
                        update, 
                        chat_id, 
                        temp_message_id, 
                        gpt_result["bot_reply"]
                    )
                    if success:
                        logging.info(f"ğŸ”„ [TIMING] GPT ××™×˜×™ ({gpt_duration:.1f}s) - ×”×•×“×¢×” ×–×× ×™×ª ×”×•×—×œ×¤×”")
                        # ××¡×× ×™× ×©×”×”×•×“×¢×” ×›×‘×¨ × ×©×œ×—×” ×“×¨×š ×”×¢×¨×™×›×”
                        gpt_result["message_already_sent"] = True
        
        return gpt_result
        
    except Exception as e:
        logging.error(f"[gpt_a] ×©×’×™××” ×›×œ×œ×™×ª: {e}")
        
        # ×‘×™×˜×•×œ ×”×•×“×¢×” ×–×× ×™×ª ×‘××§×¨×” ×©×œ ×©×’×™××”
        if temp_message_task and not temp_message_task.done():
            temp_message_task.cancel()
        
        return {
            "bot_reply": "[×©×’×™××” ×‘×× ×•×¢ ×”×¨××©×™ - × ×¡×” ×©×•×‘]", 
            "usage": {}, 
            "model": "error",
            "used_premium": use_premium,
            "filter_reason": filter_reason,
            "error": str(e)
        }

# ×¤×•× ×§×¦×™×” ×™×©× ×” ×œ×ª××™××•×ª ×œ××—×•×¨
def get_main_response(full_messages, chat_id=None, message_id=None):
    """
    ğŸ’ ×’×¨×¡×” ×¡×™× ×›×¨×•× ×™×ª ×™×©× ×” - ×œ×ª××™××•×ª ×œ××—×•×¨
    """
    user_message = full_messages[-1]["content"] if full_messages else ""
    chat_history_length = len([msg for msg in full_messages if msg["role"] in ["user", "assistant"]])
    
    use_premium, filter_reason = should_use_premium_model(user_message, chat_history_length)
    
    return get_main_response_sync(full_messages, chat_id, message_id, use_premium, filter_reason) 