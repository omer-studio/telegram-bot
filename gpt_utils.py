import logging
import litellm
import time
import os
import json
import traceback
from contextlib import contextmanager
from datetime import datetime
from config import GEMINI_API_KEY, DATA_DIR, config, should_log_gpt_cost_debug, should_log_debug_prints, GPT_MODELS

USD_TO_ILS = 3.7  # ×©×¢×¨ ×”×“×•×œ×¨-×©×§×œ (×™×© ×œ×¢×“×›×Ÿ ×œ×¤×™ ×”×¦×•×¨×š)

@contextmanager
def measure_llm_latency(model_name):
    """Context manager ×œ××“×™×“×ª ×–××Ÿ ×ª×’×•×‘×” ×©×œ ××•×“×œ LLM"""
    start_time = time.time()
    try:
        yield
    finally:
        latency = time.time() - start_time
        # ×¨×–×”: ×¨×§ DEBUG ×œ×œ×•×’ ×›×œ×œ×™ + ×¨×™×©×•× ×œ×§×•×‘×¥ ×× latency ×’×‘×•×”
        logging.debug(f"[{model_name}] latency: {latency:.2f}s")
        if latency > 10:  # ×¨×§ ×× ××™×˜×™ ×××•×“
            # TODO: Implement performance metric logging if needed
            pass

# ğŸ§  ××¢×¨×›×ª fallback ×—×›××” - ×—×™× ××™ â†’ ×‘×ª×©×œ×•×
class SmartGeminiManager:
    """
    ğŸ¯ ×× ×”×œ ×—×›× ×¢×‘×•×¨ Gemini - ××ª×—×™×œ ×¢× ×—×™× ××™, ×¢×•×‘×¨ ×œ×‘×ª×©×œ×•× ×›×©×¦×¨×™×š
    """
    
    def __init__(self):
        self.api_key = GEMINI_API_KEY
        
        # ğŸ†“ ××•×“×œ×™× ×—×™× ××™×™× (×§×•×“×)
        self.free_models = [
            "gemini/gemini-2.0-flash-exp",  # ×”××”×™×¨ ×‘×™×•×ª×¨
            "gemini/gemini-1.5-pro",        # ××™×›×•×ª×™
            "gemini/gemini-1.5-flash"       # ×’×™×‘×•×™
        ]
        
        # ğŸ’° ××•×“×œ×™× ×‘×ª×©×œ×•× (fallback)
        self.paid_models = [
            "gemini/gemini-2.5-pro"         # ×”×˜×•×‘ ×‘×™×•×ª×¨ (×‘×ª×©×œ×•×)
        ]
        
        self.daily_free_used = False  # ×”×× ×”××’×‘×œ×” ×”×—×™× ××™×ª × ×’××¨×” ×”×™×•×
        self.last_reset_day = time.strftime("%Y-%m-%d")
    
    def _reset_daily_counter(self):
        """×××¤×¡ ××ª ×”×¡×¤×™×¨×” ×”×™×•××™×ª"""
        current_day = time.strftime("%Y-%m-%d")
        if current_day != self.last_reset_day:
            self.daily_free_used = False
            self.last_reset_day = current_day
            if should_log_debug_prints():
                print(f"ğŸ”„ ×™×•× ×—×“×©! ×××¤×¡ ××’×‘×œ×•×ª ×—×™× ××™×•×ª ({current_day})")
    
    def smart_completion(self, messages, **kwargs):
        """
        ğŸ§  ×‘×™×¦×•×¢ ×—×›× - ×× ×¡×” ×—×™× ××™ ×§×•×“×, ×¢×•×‘×¨ ×œ×‘×ª×©×œ×•× ×× ×¦×¨×™×š
        """
        self._reset_daily_counter()
        
        # ğŸ†“ ×ª×—×™×œ×” - × ×¡×” ××•×“×œ×™× ×—×™× ××™×™×
        if not self.daily_free_used:
            for free_model in self.free_models:
                if config.get("FREE_MODEL_DAILY_LIMIT", 100) <= config.get(f"{free_model}_usage", 0):
                    continue
                    
                try:
                    if should_log_debug_prints():
                        print(f"ğŸ†“ ×× ×¡×” ××•×“×œ ×—×™× ××™: {free_model}")
                    
                    completion_params_copy = kwargs.copy()
                    completion_params_copy["model"] = free_model
                    
                    with measure_llm_latency(free_model):
                        response = litellm.completion(
                            messages=messages,
                            api_key=self.api_key,
                            **completion_params_copy
                        )
                    
                    # ×¢×“×›×•×Ÿ ××•× ×” ×”×©×™××•×©
                    config[f"{free_model}_usage"] = config.get(f"{free_model}_usage", 0) + 1
                    with open(os.path.join(DATA_DIR, "free_model_limits.json"), 'w') as f:
                        json.dump(config, f)
                    
                    if should_log_debug_prints():
                        print(f"âœ… ×”×¦×œ×—×” ×¢× ××•×“×œ ×—×™× ××™: {free_model}")
                    return response, "free", free_model
                    
                except Exception as e:
                    error_msg = str(e).lower()
                    
                    if "quota" in error_msg or "rate limit" in error_msg:
                        if should_log_debug_prints():
                            print(f"ğŸš« ××’×‘×œ×” ×—×™× ××™×ª ×”×’×™×¢×” ×‘-{free_model}")
                        config[f"{free_model}_usage"] = config.get("FREE_MODEL_DAILY_LIMIT", 100)
                        with open(os.path.join(DATA_DIR, "free_model_limits.json"), 'w') as f:
                            json.dump(config, f)
                        continue  # × ×¡×” ××•×“×œ ×—×™× ××™ ×”×‘×
                    
                    elif "expired" in error_msg:
                        if should_log_debug_prints():
                            print(f"âŒ ×‘×¢×™×” ×¢× API Key ×‘-{free_model}")
                        continue
                    
                    else:
                        if should_log_debug_prints():
                            print(f"âš ï¸ ×©×’×™××” ××—×¨×ª ×‘-{free_model}: {e}")
                        continue
            
            # ×× ×”×’×¢× ×• ×œ×›××Ÿ - ×›×œ ×”××•×“×œ×™× ×”×—×™× ××™×™× × ×’××¨×•
            if should_log_debug_prints():
                print("ğŸ”„ ×›×œ ×”××•×“×œ×™× ×”×—×™× ××™×™× ××•×’×‘×œ×™× - ×¢×•×‘×¨ ×œ×‘×ª×©×œ×•×")
            self.daily_free_used = True
        
        # ğŸ’° ×¢×›×©×™×• - × ×¡×” ××•×“×œ×™× ×‘×ª×©×œ×•× (×œ×œ× ×—×¡×™××•×ª!)
        if should_log_debug_prints():
            print("ğŸ’° ×¢×•×‘×¨ ×œ××•×“×œ×™× ×‘×ª×©×œ×•× - ×©×™×¨×•×ª ×¨×¦×™×£!")
        for paid_model in self.paid_models:
            try:
                if should_log_debug_prints():
                    print(f"ğŸ’° ×× ×¡×” ××•×“×œ ×‘×ª×©×œ×•×: {paid_model}")
                
                completion_params_copy = kwargs.copy()
                completion_params_copy["model"] = paid_model
                
                with measure_llm_latency(paid_model):
                    response = litellm.completion(
                        messages=messages,
                        api_key=self.api_key,
                        **completion_params_copy
                    )
                
                if should_log_debug_prints():
                    print(f"âœ… ×”×¦×œ×—×” ×¢× ××•×“×œ ×‘×ª×©×œ×•×: {paid_model}")
                return response, "paid", paid_model
                
            except Exception as e:
                if should_log_debug_prints():
                    print(f"âŒ ×©×’×™××” ×‘××•×“×œ ×‘×ª×©×œ×•× {paid_model}: {e}")
                continue
        
        # ×× ×”×’×¢× ×• ×œ×›××Ÿ - ×›×œ ×”××•×“×œ×™× × ×›×©×œ×•
        raise Exception("âŒ ×›×œ ×”××•×“×œ×™× (×—×™× ××™×™× ×•×‘×ª×©×œ×•×) × ×›×©×œ×•!")
    
    def get_status(self):
        """××—×–×™×¨ ×¡×˜×˜×•×¡ × ×•×›×—×™"""
        self._reset_daily_counter()
        return {
            "free_available": not self.daily_free_used,
            "last_reset": self.last_reset_day,
            "preferred_model": self.free_models[0] if not self.daily_free_used else self.paid_models[0]
        }

# ×™×¦×™×¨×ª instance ×’×œ×•×‘×œ×™
smart_manager = SmartGeminiManager()

def safe_get_usage_value(obj, attr_name, default=0):
    """
    ××—×œ×¥ ×¢×¨×š ×usage object ×‘××•×¤×Ÿ ×‘×˜×•×—, ×›×•×œ×œ ×ª××™×›×” ×‘wrappers ×©×œ OpenAI API ×”×—×“×©
    """
    try:
        if hasattr(obj, attr_name):
            value = getattr(obj, attr_name)
            # ×× ×–×” wrapper object, × × ×¡×” ×œ×”××™×¨ ××•×ª×• ×œ××¡×¤×¨
            if hasattr(value, '__dict__'):
                return default
            return int(value) if value is not None else default
        return default
    except (ValueError, TypeError, AttributeError):
        return default

def calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens=0, model_name=None, usd_to_ils=USD_TO_ILS, completion_response=None):
    if model_name is None:
        from config import GPT_MODELS
        model_name = GPT_MODELS["gpt_a"]
    """
    ××—×©×‘ ××ª ×”×¢×œ×•×ª ×©×œ ×©×™××•×© ×‘-gpt ×œ×¤×™ ××¡×¤×¨ ×”×˜×•×§× ×™× ×•×”××•×“×œ.
    ××©×ª××© ××š ×•×¨×§ ×‘-LiteLLM ×¢× completion_response.
    ××—×–×™×¨ ×¨×§ ××ª ×”×¢×œ×•×ª ×”×›×•×œ×œ×ª (cost_total) ×›×¤×™ ×©××—×•×©×‘ ×¢"×™ LiteLLM, ×‘×œ×™ ×¤×™×œ×•×— ×™×“× ×™.
    """
    if should_log_gpt_cost_debug():
        print(f"[DEBUG] ğŸ”¥ calculate_gpt_cost CALLED! ğŸ”¥")
        print(f"[DEBUG] Input: prompt_tokens={prompt_tokens}, completion_tokens={completion_tokens}, cached_tokens={cached_tokens}, model_name={model_name}")
        print(f"[DEBUG] calculate_gpt_cost - Model: {model_name}, Tokens: {prompt_tokens}p + {completion_tokens}c + {cached_tokens}cache")
    try:
        if completion_response:
            if should_log_gpt_cost_debug():
                print(f"[DEBUG] Using completion_response for cost calculation")
            cost_usd = litellm.completion_cost(completion_response=completion_response)
            if should_log_gpt_cost_debug():
                print(f"[DEBUG] LiteLLM completion_cost returned: {cost_usd}")
        else:
            if should_log_gpt_cost_debug():
                print(f"[DEBUG] No completion_response provided, cannot calculate cost with LiteLLM")
            cost_usd = 0.0
        cost_ils = cost_usd * usd_to_ils
        cost_agorot = cost_ils * 100
        result = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
            "cached_tokens": cached_tokens,
            "cost_total": cost_usd,
            "cost_total_ils": cost_ils,
            "cost_agorot": cost_agorot,
            "model": model_name
        }
        if should_log_gpt_cost_debug():
            print(f"[DEBUG] calculate_gpt_cost returning: {result}")
        return result
    except Exception as e:
        if should_log_gpt_cost_debug():
            print(f"[ERROR] calculate_gpt_cost failed: {e}")
        if should_log_debug_prints():
            print(f"[ERROR] Full traceback: {traceback.format_exc()}")
        return {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
            "cached_tokens": cached_tokens,
            "cost_total": 0.0,
            "cost_total_ils": 0.0,
            "cost_agorot": 0.0,
            "model": model_name
        }

def normalize_usage_dict(usage, model_name=""):
    """
    ×× ×¨××œ ××™×œ×•×Ÿ usage ×©×œ gpt ×œ×¤×•×¨××˜ ××—×™×“ (×œ×•×’×™×/×“×•×—×•×ª).
    ××˜×¤×œ ×‘wrappers ×—×“×©×™× ×©×œ OpenAI API.
    """
    if not usage:
        return {}
    
    # ×× ×–×” usage object ×©×œ OpenAI, × ××™×¨ ××•×ª×• ×‘×‘×˜×—×”
    if hasattr(usage, '__dict__'):
        try:
            result = {
                "prompt_tokens": safe_get_usage_value(usage, 'prompt_tokens', 0),
                "completion_tokens": safe_get_usage_value(usage, 'completion_tokens', 0),
                "total_tokens": safe_get_usage_value(usage, 'total_tokens', 0),
            }
            
            # × ×—×¤×© cached_tokens ×‘××§×•××•×ª ×”×©×•× ×™× ×©×”× ×™×›×•×œ×™× ×œ×”×™×•×ª
            cached_tokens = 0
            if hasattr(usage, 'prompt_tokens_details'):
                cached_tokens = safe_get_usage_value(usage.prompt_tokens_details, 'cached_tokens', 0)
            elif hasattr(usage, 'cached_tokens'):
                cached_tokens = safe_get_usage_value(usage, 'cached_tokens', 0)
            
            result["cached_tokens"] = cached_tokens
            result["model"] = model_name
            return result
        except Exception as e:
            if should_log_debug_prints():
                print(f"[DEBUG] ×©×’×™××” ×‘× ×™×¨××•×œ usage: {e}")
            # fallback ×œ× ×™×¨××•×œ ×¤×©×•×˜
            return {"model": model_name, "prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "cached_tokens": 0}
    
    # ×× ×–×” ×›×‘×¨ dict, ×¤×©×•×˜ × ×•×¡×™×£ ××ª ×”××•×“×œ
    if isinstance(usage, dict):
        result = dict(usage)
        result["model"] = model_name
        return result
    
    # fallback
    return {"model": model_name, "prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "cached_tokens": 0}

# ========================================
# ğŸ›¡ï¸ ××¢×¨×›×ª ×”×’× ×” ×¢×œ ×—×™×•×‘ (××§×•×¨: billing_protection.py)
# ========================================

class BillingProtection:
    """
    ğŸ›¡ï¸ ××¢×¨×›×ª ×”×’× ×” ××¤× ×™ ×—×™×•×‘×™× ××•×¤×¨×–×™×
    """
    
    def __init__(self, daily_limit_usd=5.0, monthly_limit_usd=50.0):
        self.daily_limit_usd = daily_limit_usd      # ××’×‘×œ×” ×™×•××™×ª: $5
        self.monthly_limit_usd = monthly_limit_usd  # ××’×‘×œ×” ×—×•×“×©×™×ª: $50
        
        self.usage_file = "data/billing_usage.json"
        self._ensure_data_dir()
        self.usage_data = self._load_usage()
    
    def _ensure_data_dir(self):
        """×•×•×“× ×©×ª×™×§×™×™×ª data ×§×™×™××ª"""
        import os
        os.makedirs("data", exist_ok=True)
    
    def _load_usage(self):
        """×˜×¢×Ÿ × ×ª×•× ×™ ×©×™××•×© ××§×•×‘×¥"""
        import os
        import json
        if os.path.exists(self.usage_file):
            try:
                with open(self.usage_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        
        # ×‘×¨×™×¨×ª ××—×“×œ
        return {
            "daily": {},    # {"2025-01-20": 1.25}
            "monthly": {},  # {"2025-01": 15.30}
            "alerts_sent": {}
        }
    
    def _save_usage(self):
        """×©××•×¨ × ×ª×•× ×™ ×©×™××•×© ×œ×§×•×‘×¥"""
        import json
        try:
            with open(self.usage_file, 'w', encoding='utf-8') as f:
                json.dump(self.usage_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            if should_log_debug_prints():
                print(f"âš ï¸ ×©×’×™××” ×‘×©××™×¨×ª × ×ª×•× ×™ ×©×™××•×©: {e}")
    
    def _get_current_keys(self):
        """××—×–×™×¨ ××¤×ª×—×•×ª ×ª××¨×™×š × ×•×›×—×™×™×"""
        from utils import get_israel_time
        now = get_israel_time()
        daily_key = now.strftime("%Y-%m-%d")
        monthly_key = now.strftime("%Y-%m")
        return daily_key, monthly_key
    
    def add_cost(self, cost_usd, model_name, tier_type="unknown"):
        """
        ğŸ”„ ××•×¡×™×£ ×¢×œ×•×ª ×œ×¡×¤×™×¨×” ×•×‘×•×“×§ ××’×‘×œ×•×ª
        
        Args:
            cost_usd: ×¢×œ×•×ª ×‘×“×•×œ×¨×™×
            model_name: ×©× ×”××•×“×œ
            tier_type: "free" ××• "paid"
        
        Returns:
            dict: ××™×“×¢ ×¢×œ ×¡×˜×˜×•×¡ ×”××’×‘×œ×•×ª
        """
        daily_key, monthly_key = self._get_current_keys()
        
        # ×¢×“×›×•×Ÿ ×©×™××•×© ×™×•××™
        if daily_key not in self.usage_data["daily"]:
            self.usage_data["daily"][daily_key] = 0.0
        self.usage_data["daily"][daily_key] += cost_usd
        
        # ×¢×“×›×•×Ÿ ×©×™××•×© ×—×•×“×©×™
        if monthly_key not in self.usage_data["monthly"]:
            self.usage_data["monthly"][monthly_key] = 0.0
        self.usage_data["monthly"][monthly_key] += cost_usd
        
        # ğŸ”§ ×ª×™×§×•×Ÿ ×–×œ×™×’×ª ×–×™×›×¨×•×Ÿ: ×”×’×‘×œ×ª ××¡×¤×¨ ×¨×©×•××•×ª ×™×©× ×•×ª
        # ×©××™×¨×” ×¢×œ ××§×¡×™××•× 60 ×™××™× ×©×œ × ×ª×•× ×™× ×™×•××™×™×
        if len(self.usage_data["daily"]) > 60:
            old_keys = sorted(self.usage_data["daily"].keys())[:-30]  # ×©××•×¨ ×¨×§ 30 ×™××™× ××—×¨×•× ×™×
            for old_key in old_keys:
                del self.usage_data["daily"][old_key]
        
        # ×©××™×¨×” ×¢×œ ××§×¡×™××•× 12 ×—×•×“×©×™× ×©×œ × ×ª×•× ×™× ×—×•×“×©×™×™×  
        if len(self.usage_data["monthly"]) > 12:
            old_keys = sorted(self.usage_data["monthly"].keys())[:-6]  # ×©××•×¨ ×¨×§ 6 ×—×•×“×©×™× ××—×¨×•× ×™×
            for old_key in old_keys:
                del self.usage_data["monthly"][old_key]
        
        # ×‘×“×™×§×ª ××’×‘×œ×•×ª
        daily_usage = self.usage_data["daily"][daily_key]
        monthly_usage = self.usage_data["monthly"][monthly_key]
        
        status = {
            "daily_usage": daily_usage,
            "daily_limit": self.daily_limit_usd,
            "daily_percent": (daily_usage / self.daily_limit_usd) * 100,
            "monthly_usage": monthly_usage,
            "monthly_limit": self.monthly_limit_usd,
            "monthly_percent": (monthly_usage / self.monthly_limit_usd) * 100,
            "cost_added": cost_usd,
            "model": model_name,
            "tier": tier_type,
            "warnings": []
        }
        
        # ×”×•×¡×¤×ª ××–×”×¨×•×ª
        if daily_usage >= self.daily_limit_usd:
            status["warnings"].append("ğŸš¨ ×¢×‘×¨×ª ××ª ×”××’×‘×œ×” ×”×™×•××™×ª!")
        elif daily_usage >= self.daily_limit_usd * 0.8:
            status["warnings"].append("âš ï¸ ×”×©×™××•×© ×”×™×•××™ ××¢×œ 80%")
        
        if monthly_usage >= self.monthly_limit_usd:
            status["warnings"].append("ğŸš¨ ×¢×‘×¨×ª ××ª ×”××’×‘×œ×” ×”×—×•×“×©×™×ª!")
        elif monthly_usage >= self.monthly_limit_usd * 0.8:
            status["warnings"].append("âš ï¸ ×”×©×™××•×© ×”×—×•×“×©×™ ××¢×œ 80%")
        
        # ×©××™×¨×”
        self._save_usage()
        
        return status
    
    def get_current_status(self):
        """××—×–×™×¨ ×¡×˜×˜×•×¡ × ×•×›×—×™ ×œ×œ× ×”×•×¡×¤×ª ×¢×œ×•×ª"""
        daily_key, monthly_key = self._get_current_keys()
        
        daily_usage = self.usage_data["daily"].get(daily_key, 0.0)
        monthly_usage = self.usage_data["monthly"].get(monthly_key, 0.0)
        
        return {
            "daily_usage": daily_usage,
            "daily_limit": self.daily_limit_usd,
            "daily_remaining": max(0, self.daily_limit_usd - daily_usage),
            "monthly_usage": monthly_usage,
            "monthly_limit": self.monthly_limit_usd,
            "monthly_remaining": max(0, self.monthly_limit_usd - monthly_usage),
            "daily_percent": (daily_usage / self.daily_limit_usd) * 100,
            "monthly_percent": (monthly_usage / self.monthly_limit_usd) * 100
        }
    
    def get_alert_level(self):
        """
        ğŸ“Š ××—×–×™×¨ ×¨××ª ×”×ª×¨××” (×œ× ×—×•×¡×, ×¨×§ ××ª×¨×™×¢!)
        """
        status = self.get_current_status()
        
        # ×¨××•×ª ×”×ª×¨××”
        if (status["daily_usage"] >= self.daily_limit_usd or 
            status["monthly_usage"] >= self.monthly_limit_usd):
            return "critical", "ğŸš¨ ×¢×‘×¨×ª ××ª ×”××’×‘×œ×”!"
        
        elif (status["daily_usage"] >= self.daily_limit_usd * 0.8 or 
              status["monthly_usage"] >= self.monthly_limit_usd * 0.8):
            return "warning", "âš ï¸ ××ª×§×¨×‘ ×œ××’×‘×œ×” (80%+)"
        
        elif (status["daily_usage"] >= self.daily_limit_usd * 0.5 or 
              status["monthly_usage"] >= self.monthly_limit_usd * 0.5):
            return "info", "ğŸ“Š ×©×™××•×© ×‘×™× ×•× ×™ (50%+)"
        
        return "ok", "âœ… ×©×™××•×© ×ª×§×™×Ÿ"
    
    def print_status(self):
        """×”×“×¤×¡ ×¡×˜×˜×•×¡ × ×•×›×—×™"""
        status = self.get_current_status()
        
        if should_log_debug_prints():
            print("ğŸ’° ×¡×˜×˜×•×¡ ×ª×§×¦×™×‘:")
            print(f"ğŸ“… ×™×•××™: ${status['daily_usage']:.2f} / ${status['daily_limit']:.2f} ({status['daily_percent']:.1f}%)")
            print(f"ğŸ“† ×—×•×“×©×™: ${status['monthly_usage']:.2f} / ${status['monthly_limit']:.2f} ({status['monthly_percent']:.1f}%)")
            
            if status['daily_percent'] > 80:
                print("âš ï¸ ××–×”×¨×”: ×©×™××•×© ×™×•××™ ×’×‘×•×”!")
            if status['monthly_percent'] > 80:
                print("âš ï¸ ××–×”×¨×”: ×©×™××•×© ×—×•×“×©×™ ×’×‘×•×”!")

# ×™×¦×™×¨×ª instance ×’×œ×•×‘×œ×™
billing_guard = BillingProtection(
    daily_limit_usd=5.0,    # $5 ×œ×™×•×
    monthly_limit_usd=50.0  # $50 ×œ×—×•×“×©
)

def _load_daily_limits():
    """×˜×•×¢×Ÿ ××’×‘×œ×•×ª ×™×•××™×•×ª ×•×××¤×¡ ×× ×™×•× ×—×“×©."""
    from utils import get_israel_time
    today_str = get_israel_time().strftime("%Y-%m-%d")
    limits_file = os.path.join(DATA_DIR, "free_model_limits.json")
    
    try:
        with open(limits_file, 'r') as f:
            daily_limits = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        daily_limits = {}
    
    if daily_limits.get("date") != today_str:
        daily_limits = {"date": today_str}
        with open(limits_file, 'w') as f:
            json.dump(daily_limits, f)
        if should_log_debug_prints():
            print(f"ğŸ”„ ×™×•× ×—×“×©! ×××¤×¡ ××’×‘×œ×•×ª ×—×™× ××™×•×ª ({today_str})")
    
    return daily_limits, limits_file

def _try_single_model(model, full_messages, completion_params, is_paid=False):
    """×× ×¡×” ××•×“×œ ×™×—×™×“ ×•××—×–×™×¨ ×ª×’×•×‘×” ××• None ×× × ×›×©×œ."""
    try:
        if should_log_debug_prints():
            symbol = "ğŸ’°" if is_paid else "ğŸ†“"
            print(f"{symbol} ×× ×¡×” ××•×“×œ: {model}")
        
        completion_params_copy = completion_params.copy()
        completion_params_copy["model"] = model
        
        with measure_llm_latency(model):
            response = litellm.completion(messages=full_messages, **completion_params_copy)
        
        if should_log_debug_prints():
            print(f"âœ… ×”×¦×œ×—×” ×¢× ××•×“×œ: {model}")
        return response
        
    except Exception as e:
        if should_log_debug_prints():
            print(f"âŒ ×©×’×™××” ×‘××•×“×œ {model}: {e}")
        return None, e

def try_free_models_first(full_messages, **completion_params):
    """×× ×¡×” ×§×•×“× ××•×“×œ×™× ×—×™× ××™×™×, ××—×¨ ×›×š ×¢×•×‘×¨ ×œ×‘×ª×©×œ×•× - ×’×¨×¡×” ×¨×–×”."""
    free_models = config.get("FREE_MODELS", [])
    paid_models = config.get("PAID_MODELS", [])
    daily_limits, limits_file = _load_daily_limits()
    
    # × ×™×¡×™×•×Ÿ ×¢× ××•×“×œ×™× ×—×™× ××™×™×
    for free_model in free_models:
        if daily_limits.get(free_model, 0) >= config.get("FREE_MODEL_DAILY_LIMIT", 100):
            continue
            
        response, error = _try_single_model(free_model, full_messages, completion_params, is_paid=False)
        if response:
            # ×¢×“×›×•×Ÿ ××•× ×” ×”×©×™××•×©
            daily_limits[free_model] = daily_limits.get(free_model, 0) + 1
            with open(limits_file, 'w') as f:
                json.dump(daily_limits, f)
            return response
        
        # ×˜×™×¤×•×œ ×‘×©×’×™××•×ª ××•×“×œ ×—×™× ××™
        error_msg = str(error).lower()
        if "quota" in error_msg or "rate limit" in error_msg:
            daily_limits[free_model] = config.get("FREE_MODEL_DAILY_LIMIT", 100)
            with open(limits_file, 'w') as f:
                json.dump(daily_limits, f)
    
    # ××¢×‘×¨ ×œ××•×“×œ×™× ×‘×ª×©×œ×•×
    if should_log_debug_prints():
        print("ğŸ”„ ×¢×•×‘×¨ ×œ××•×“×œ×™× ×‘×ª×©×œ×•×")
    logging.info("ğŸ’° ×¢×•×‘×¨ ×œ××•×“×œ×™× ×‘×ª×©×œ×•× - ×©×™×¨×•×ª ×¨×¦×™×£!")
    
    for paid_model in paid_models:
        response, _ = _try_single_model(paid_model, full_messages, completion_params, is_paid=True)
        if response:
            return response
    
    raise Exception("×›×œ ×”××•×“×œ×™× (×—×™× ××™×™× ×•×‘×ª×©×œ×•×) × ×›×©×œ×•")

def normalize_usage_data(usage_data):
    """×× ×¨××œ × ×ª×•× ×™ usage ×œ××‘× ×” ××—×™×“"""
    try:
        if hasattr(usage_data, 'prompt_tokens'):
            return {
                "prompt_tokens": getattr(usage_data, 'prompt_tokens', 0),
                "completion_tokens": getattr(usage_data, 'completion_tokens', 0),
                "total_tokens": getattr(usage_data, 'total_tokens', 0),
                "prompt_tokens_details": getattr(usage_data, 'prompt_tokens_details', {}),
                "completion_tokens_details": getattr(usage_data, 'completion_tokens_details', {})
            }
        elif isinstance(usage_data, dict):
            return usage_data
        else:
            return {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    except Exception as e:
        if should_log_debug_prints():
            print(f"[DEBUG] ×©×’×™××” ×‘× ×™×¨××•×œ usage: {e}")
        return {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

def save_usage_data(usage_data, model_name, cost_agorot=None, chat_id=None, user_message=None, bot_response=None):
    """×©×•××¨ × ×ª×•× ×™ ×©×™××•×© ×œ×§×•×‘×¥ ×œ×•×’ ××¨×›×–×™"""
    try:
        usage_log_path = os.path.join(DATA_DIR, "gpt_usage_log.jsonl")
        
        from utils import get_israel_time
        log_entry = {
            "timestamp": get_israel_time().isoformat(),
            "model": model_name,
            "usage": normalize_usage_data(usage_data),
            "cost_agorot": cost_agorot,
            "chat_id": str(chat_id) if chat_id else None,
            "user_message_length": len(user_message) if user_message else 0,
            "bot_response_length": len(bot_response) if bot_response else 0
        }
        
        with open(usage_log_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            
    except Exception as e:
        if should_log_debug_prints():
            print(f"âš ï¸ ×©×’×™××” ×‘×©××™×¨×ª × ×ª×•× ×™ ×©×™××•×©: {e}")

def print_budget_status():
    """××“×¤×™×¡ ×¡×˜×˜×•×¡ ×ª×§×¦×™×‘ × ×•×›×—×™ - ×¨×§ ×× ××•×¤×¢×œ ×“×™×‘××’"""
    if not should_log_gpt_cost_debug():
        return
        
    try:
        status = billing_guard.get_current_status()
        print("ğŸ’° ×¡×˜×˜×•×¡ ×ª×§×¦×™×‘:")
        print(f"ğŸ“… ×™×•××™: ${status['daily_usage']:.2f} / ${status['daily_limit']:.2f} ({status['daily_percent']:.1f}%)")
        print(f"ğŸ“† ×—×•×“×©×™: ${status['monthly_usage']:.2f} / ${status['monthly_limit']:.2f} ({status['monthly_percent']:.1f}%)")
        
        if status['daily_percent'] > 80:
            print("âš ï¸ ××–×”×¨×”: ×©×™××•×© ×™×•××™ ×’×‘×•×”!")
        if status['monthly_percent'] > 80:
            print("âš ï¸ ××–×”×¨×”: ×©×™××•×© ×—×•×“×©×™ ×’×‘×•×”!")
    except Exception as e:
        logging.error(f"×©×’×™××” ×‘×”×“×¤×¡×ª ×¡×˜×˜×•×¡ ×ª×§×¦×™×‘: {e}")

# ×™×¦×™×¨×ª instance ×’×œ×•×‘×œ×™
smart_manager = SmartGeminiManager()
