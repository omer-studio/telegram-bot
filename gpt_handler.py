"""
gpt_handler.py â€” ×›×œ ×”×¤×•× ×§×¦×™×•×ª ×œ×˜×™×¤×•×œ ×‘Ö¾GPT ×‘××§×•× ××—×“
×‘×’×¨×¡×” ×–×• × ×•×¡×£ ×—×™×©×•×‘ ×¢×œ×•×ª ×œ×›×œ ×¡×•×’ ×˜×•×§×Ÿ (×¨×’×™×œ, ×§×©×“, ×¤×œ×˜) + ×ª×™×¢×•×“ ××œ× ×©×œ ×”×˜×•×§× ×™× + ×”×—×–×¨ ×¢×œ×•×ª ×‘××’×•×¨×•×ª ×œ×›×œ ×§×¨×™××”
"""

import json
import logging
from datetime import datetime
from config import client, SYSTEM_PROMPT

# ××—×™×¨×™× ×§×‘×•×¢×™× (× ×›×•×Ÿ ×œ×™×•× ×™ 2024) ×œÖ¾GPT-4o
COST_PROMPT_REGULAR = 0.002 / 1000    # ×˜×•×§×Ÿ ×§×œ×˜ ×¨×’×™×œ
COST_PROMPT_CACHED = 0.0005 / 1000    # ×˜×•×§×Ÿ ×§×œ×˜ ×§×©×“ (cache)
COST_COMPLETION = 0.006 / 1000        # ×˜×•×§×Ÿ ×¤×œ×˜
USD_TO_ILS = 3.8                      # ×©×¢×¨ ×“×•×œ×¨-×©×§×œ

def write_gpt_log(ttype, usage, model):
    """
    ×©×•××¨ ×œ×•×’ ×©×œ ×”×©×™××•×© ×‘×›×œ ×§×¨×™××” ×œÖ¾GPT
    """
    log_path = "/data/gpt_usage_log.jsonl"
    log_entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "type": ttype,
        "model": model,
        "tokens_prompt": usage.get("prompt_tokens", 0),
        "tokens_completion": usage.get("completion_tokens", 0),
        "tokens_total": usage.get("total_tokens", 0),
        "tokens_cached": usage.get("cached_tokens", 0),  # × ×•×¡×£: ×›××•×ª ×§×©×“
        "cost_prompt_regular": usage.get("cost_prompt_regular", 0),
        "cost_prompt_cached": usage.get("cost_prompt_cached", 0),
        "cost_completion": usage.get("cost_completion", 0),
        "cost_total": usage.get("cost_total", 0),
        "cost_total_ils": usage.get("cost_total_ils", 0),
    }
    try:
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
    except Exception as e:
        logging.error(f"×©×’×™××” ×‘×›×ª×™×‘×” ×œ×§×•×‘×¥ gpt_usage_log: {e}")

def safe_float(val):
    """
    × ×™×¡×™×•×Ÿ ×œ×”××™×¨ ×¢×¨×š ×œ-float, ×‘××§×¨×” ×©×œ ×›×©×œ ×™×—×–×™×¨ 0.0 ×¢× ×œ×•×’ ××–×”×¨×”.
    """
    try:
        return float(val)
    except (ValueError, TypeError):
        logging.warning(f"safe_float: value '{val}' could not be converted to float. Using 0.0 instead.")
        return 0.0



# ============================×”×’'×™×¤×™×˜×™ ×”-1 - ×¤×•×¢×œ ×ª××™×“ ×•×¢×•× ×” ×ª×©×•×‘×” ×œ××©×ª××© ======================= 


def get_main_response(full_messages):
    """
    GPT ×¨××©×™ - × ×•×ª×Ÿ ×ª×©×•×‘×” ×œ××©×ª××©
    ××—×–×™×¨ ×’× ××ª ×›×œ ×¤×¨×˜×™ ×”×¢×œ×•×ª (×˜×•×§× ×™×, ×§×©×“, ××—×™×¨ ××“×•×™×§)
    """
    try:
        print("ğŸ” × ×©×œ×— ×œÖ¾GPT:")
        for m in full_messages:
            print(f"{m['role']}: {m['content']}")

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=full_messages,
            temperature=1,
        )

        # ×©×œ×™×¤×ª × ×ª×•× ×™ usage
        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = response.usage.prompt_tokens_details
        cached_tokens = prompt_tokens_details['cached_tokens']
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens

        # ×—×™×©×•×‘ ×¢×œ×•×ª ×œ×¤×™ ×”×¡×•×’
        cost_prompt_regular = prompt_regular * COST_PROMPT_REGULAR
        cost_prompt_cached = cached_tokens * COST_PROMPT_CACHED
        cost_completion = completion_tokens * COST_COMPLETION
        cost_total = cost_prompt_regular + cost_prompt_cached + cost_completion
        cost_total_ils = round(cost_total * USD_TO_ILS, 4)
        cost_gpt1 = int(round(cost_total_ils * 100))  # ×¢×œ×•×ª ×‘××’×•×¨×•×ª #NEW

        print(f"ğŸ”¢ ×¤×¨×˜×™ ×©×™××•×©: prompt={prompt_tokens} ×§×©×“={cached_tokens} ×¨×’×™×œ={prompt_regular} ×¤×œ×˜={completion_tokens}")
        print(f"ğŸ’¸ ×¢×œ×•×™×•×ª: ×¨×’×™×œ ${cost_prompt_regular:.6f}, ×§×©×“ ${cost_prompt_cached:.6f}, ×¤×œ×˜ ${cost_completion:.6f}, ×¡×”×› ${cost_total:.6f} (â‚ª{cost_total_ils})")

        # ×ª×™×¢×•×“ ××œ× ×œ×œ×•×’ × ×•×¡×£
        usage_log = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,
            "cost_prompt_regular": cost_prompt_regular,
            "cost_prompt_cached": cost_prompt_cached,
            "cost_completion": cost_completion,
            "cost_total": cost_total,
            "cost_total_ils": cost_total_ils,
            "cost_gpt1": cost_gpt1  # ×‘××’×•×¨×•×ª
        }

        # ×©×•×¨×ª ×œ×•×’ ×œ×“×™×‘××’ ×•×œ××¢×§×‘
        from utils import log_event_to_file
        log_event_to_file({
            "event": "gpt_main_call",
            "gpt_input": full_messages,
            "gpt_reply": response.choices[0].message.content,
            "model": response.model,
            "usage": usage_log
        })

        # ×›×ª×™×‘×” ×œ×œ×•×’ ×©×™××•×©
        write_gpt_log("main_reply", usage_log, response.model)

        # ××—×–×™×¨ ××ª ×›×œ ×”×¤×¨××˜×¨×™×
        return (
            response.choices[0].message.content,  # bot_reply
            prompt_tokens,                        # prompt_tokens_total
            cached_tokens,                        # cached_tokens
            prompt_regular,                       # prompt_regular
            completion_tokens,                    # completion_tokens_total
            total_tokens,                         # total_tokens
            cost_prompt_regular,
            cost_prompt_cached,
            cost_completion,
            cost_total,
            cost_total_ils,                       # total_cost_ils ×‘×©"×—
            cost_gpt1,                            # cost_gpt1 ×‘××’×•×¨×•×ª
            response.model                        # model_GPT1
        )
    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-GPT ×¨××©×™: {e}")
        raise



# ============================×”×’'×™×¤×™×˜×™ ×”-2 - ××§×¦×¨ ××ª ×ª×©×•×‘×ª ×”×‘×•×˜ ×× ×”×™× ××¨×•×›×” ××“×™ ×›×“×™ ×œ×—×¡×•×š ×‘×”×™×¡×˜×•×¨×™×” ======================= 


def summarize_bot_reply(reply_text):
    """
    GPT ××§×¦×¨ - ×ª××¦×•×ª ×ª×©×•×‘×ª ×”×‘×•×˜
    (×”×•×¡×¤× ×• ×’× ×›××Ÿ ×—×™×©×•×‘ ×¢×œ×•×ª ××œ× ×•×”×—×–×¨×ª ×¢×œ×•×ª ×‘××’×•×¨×•×ª ×•×§×©×“)
    """
    system_prompt = (
        "×¡×›× ××ª ×”×”×•×“×¢×” ×©×œ×™ ×›××™×œ×• ×× ×™ ××“×‘×¨ ×¢× ×—×‘×¨: "
        "××©×¤×˜ ××—×“ ×—× ×•××™×©×™ ×‘×¡×’× ×•×Ÿ ×—×•×¤×©×™ (×œ× ×ª×™××•×¨ ×™×‘×©), ×‘×’×•×£ ×¨××©×•×Ÿ, ×›×•×œ×œ ×××™×¨×” ××™×©×™×ª ×§×¦×¨×” ×¢×œ ××”×•×ª ×”×ª×’×•×‘×” ×©×œ×™, "
        "×•××– ××ª ×”×©××œ×” ×©×©××œ×ª×™ ×× ×™×©, ×‘×¦×•×¨×” ×—××” ×•×–×•×¨××ª, ×¢×“ 20 ××™×œ×™× ×‘×¡×š ×”×›×œ. ×ª×©×œ×‘ ××™××•×’'×™ ×¨×œ×•×•× ×˜×™ ×× ××ª××™×, ×›××• ×©××“×‘×¨×™× ×‘×•×•×˜×¡××¤. "
        "××œ ×ª×¢×©×” × ×™×ª×•×—×™× ×˜×›× ×™×™× ××• ×ª×™××•×¨ ×©×œ ×”×•×“×¢×” â€“ ×××© ×ª×›×ª×•×‘ ××ª ×–×” ×›××• ×”×•×“×¢×ª ×•×•××˜×¡××¤ ×§×¦×¨×”, ×‘×’×•×£ ×¨××©×•×Ÿ, ×‘×¡×’× ×•×Ÿ ×—×•×¤×©×™ ×•×§×œ×™×œ."
    )
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": reply_text}
            ],
            temperature=0.6,
            max_tokens=40
        )
        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = response.usage.prompt_tokens_details
        cached_tokens = prompt_tokens_details['cached_tokens']
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens

        cost_prompt_regular = prompt_regular * COST_PROMPT_REGULAR
        cost_prompt_cached = cached_tokens * COST_PROMPT_CACHED
        cost_completion = completion_tokens * COST_COMPLETION
        cost_total = cost_prompt_regular + cost_prompt_cached + cost_completion
        cost_total_ils = round(cost_total * USD_TO_ILS, 4)
        cost_gpt2 = int(round(cost_total_ils * 100))  # ×‘××’×•×¨×•×ª #NEW

        usage_log = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,
            "cost_prompt_regular": cost_prompt_regular,
            "cost_prompt_cached": cost_prompt_cached,
            "cost_completion": cost_completion,
            "cost_total": cost_total,
            "cost_total_ils": cost_total_ils,
            "cost_gpt2": cost_gpt2 # ×‘××’×•×¨×•×ª
        }

        write_gpt_log("reply_summary", usage_log, response.model)

        return (
            response.choices[0].message.content.strip(),  # bot_summary
            prompt_tokens,
            cached_tokens,         # cached_tokens_gpt2 #NEW
            prompt_regular,
            completion_tokens,
            total_tokens,
            cost_prompt_regular,
            cost_prompt_cached,
            cost_completion,
            cost_total,
            cost_total_ils,
            cost_gpt2,             # cost_gpt2 ×‘××’×•×¨×•×ª #NEW
            response.model         # model_GPT2
        )
    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-GPT ××§×¦×¨: {e}")
        raise



# ============================×”×’'×™×¤×™×˜×™ ×”-3 - ×¤×•×¢×œ ×ª××™×“ ×•××—×œ×¥ ××™×“×¢ ×œ×ª.×– ×”×¨×’×©×™×ª ======================= 

def extract_user_profile_fields(text):
    """
    GPT ××—×œ×¥ ××™×“×¢ - ××—×œ×¥ ×¤×¨×˜×™× ××™×©×™×™× ××”×”×•×“×¢×” (×’×¨×¡×” ××¢×•×“×›× ×ª)
    """
    system_prompt = """××ª×” ××—×œ×¥ ××™×“×¢ ××™×©×™ ××˜×§×¡×˜. ×”×—×–×¨ JSON ×¢× ×”×©×“×•×ª ×”×‘××™× ×¨×§ ×× ×”× ××•×–×›×¨×™×:

age - ×’×™×œ (××¡×¤×¨ ×‘×œ×‘×“)
pronoun_preference - ×œ×©×•×Ÿ ×¤× ×™×”: "××ª"/"××ª×”"/"××¢×•×¨×‘"
occupation_or_role - ×¢×™×¡×•×§/×ª×¤×§×™×“
attracted_to - ××©×™×›×”: "×’×‘×¨×™×"/"× ×©×™×"/"×©× ×™×”×"/"×œ× ×‘×¨×•×¨"
relationship_type - ××¦×‘ ×–×•×’×™: "×¨×•×•×§"/"× ×©×•×™"/"× ×©×•×™+2"/"×’×¨×•×©" ×•×›×•'
self_religious_affiliation - ×–×”×•×ª ×“×ª×™×ª: "×™×”×•×“×™"/"×¢×¨×‘×™"/"×“×¨×•×–×™"/"× ×•×¦×¨×™"/"×©×•××¨×•× ×™"
self_religiosity_level - ×¨××ª ×“×ª×™×•×ª: "×“×ª×™"/"×—×™×œ×•× ×™"/"××¡×•×¨×ª×™"/"×—×¨×“×™"/"×“×ª×™ ×œ××•××™"
family_religiosity - ×¨×§×¢ ××©×¤×—×ª×™: "××©×¤×—×” ×“×ª×™×ª"/"××©×¤×—×” ×—×™×œ×•× ×™×ª"/"××©×¤×—×” ××¢×•×¨×‘×ª"
closet_status - ××¦×‘ ××¨×•×Ÿ: "×‘××¨×•×Ÿ"/"×™×¦× ×—×œ×§×™×ª"/"×™×¦× ×œ×›×•×œ×"
who_knows - ××™ ×™×•×“×¢ ×¢×œ×™×•
who_doesnt_know - ××™ ×œ× ×™×•×“×¢ ×¢×œ×™×•
attends_therapy - ×˜×™×¤×•×œ: "×›×Ÿ"/"×œ×"/"×˜×™×¤×•×œ ×–×•×’×™"/"×§×‘×•×¦×ª ×ª××™×›×”"
primary_conflict -  ×”×§×•× ×¤×œ×™×§×˜ ×”××¨×›×–×™ ×©××¢×¡×™×§ ××•×ª×• ×‘×—×™×™×•
trauma_history - ×˜×¨××•××•×ª (×‘×¢×“×™× ×•×ª)
goal_in_course - ××˜×¨×•×ª ×‘×§×•×¨×¡
language_of_strength - ××©×¤×˜×™× ××—×–×§×™×
coping_strategies - ×“×¨×›×™ ×”×ª××•×“×“×•×ª - ××” ××¨×™× ××•×ª×• ××” ×¢×•×–×¨ ×œ×•
fears_concerns - ×¤×—×“×™× ×•×—×©×©×•×ª - ×× ×©×™×ª×£ ×‘×¤×—×“ ××¡×•×™×™× ××ª×” ××›× ×™×¡ ××ª ×–×” ×œ×©×
future_vision - ×—×–×•×Ÿ ×¢×ª×™×“
×× ×”×•× ××‘×§×© ×œ××—×•×§ ××ª ×›×œ ××” ×©××ª×” ×™×•×“×¢ ×¢×œ×™×• - ××– ×ª×—×–×™×¨ ×©×“×•×ª ×©×™×¨×™× ×©×™×“×¨×¡×• ××ª ×”×§×™×™××™×
×× ×”×•× ××‘×§×© ×©×ª××—×§ × ×ª×•× ×™× ×¡×¤×¦×™×¤×™× ××– ×ª××—×§ × ×ª×•× ×™× ×¡×¤×¦×™×¤×™× ×›××• ××œ ×ª×–×›×•×¨ ×‘×Ÿ ×›××” ×× ×™


×“×•×’×××•×ª:
"×× ×™ ×‘×Ÿ 25, ×™×”×•×“×™ ×“×ª×™" â†’ {"age": 25, "self_religious_affiliation": "×™×”×•×“×™", "self_religiosity_level": "×“×ª×™"}
"× ×©×•×™ ×¢× ×©× ×™ ×™×œ×“×™×" â†’ {"relationship_type": "× ×©×•×™+2"}
"×¡×™×¤×¨×ª×™ ×œ×”×•×¨×™×, ××‘×œ ×œ×¢××™×ª×™× ×œ×" â†’ {"who_knows": "×”×•×¨×™×", "who_doesnt_know": "×¢××™×ª×™×"}

×¨×§ JSON, ×‘×œ×™ ×”×¡×‘×¨×™×!"""

    usage_data = {
        "prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0,
        "cached_tokens": 0, "cost_prompt_regular": 0, "cost_prompt_cached": 0,
        "cost_completion": 0, "cost_total": 0, "cost_total_ils": 0, "cost_gpt3": 0, "model": ""
    }
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text}
            ],
            temperature=0,
            max_tokens=200  # ×”×’×“×œ×ª×™ ×›×™ ×™×© 20 ×©×“×•×ª ××¤×©×¨×™×™×
        )
        content = response.choices[0].message.content.strip()

        # ×—×™×©×•×‘×™ ×¢×œ×•×ª (×œ×œ× ×©×™× ×•×™)
        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = response.usage.prompt_tokens_details
        cached_tokens = prompt_tokens_details['cached_tokens']
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens

        cost_prompt_regular = prompt_regular * COST_PROMPT_REGULAR
        cost_prompt_cached = cached_tokens * COST_PROMPT_CACHED
        cost_completion = completion_tokens * COST_COMPLETION
        cost_total = cost_prompt_regular + cost_prompt_cached + cost_completion
        cost_total_ils = round(cost_total * USD_TO_ILS, 4)
        cost_gpt3 = int(round(cost_total_ils * 100))

        usage_data = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,
            "cost_prompt_regular": cost_prompt_regular,
            "cost_prompt_cached": cost_prompt_cached,
            "cost_completion": cost_completion,
            "cost_total": cost_total,
            "cost_total_ils": cost_total_ils,
            "cost_gpt3": cost_gpt3,
            "model": response.model
        }

        # ×œ×•×’ ×œ××¢×§×‘
        logging.info(f"ğŸ¤– GPT ××—×œ×¥ ××™×“×¢ ×”×—×–×™×¨: '{content}'")
        write_gpt_log("identity_extraction", usage_data, usage_data.get("model", ""))

        # × ×™×ª×•×— JSON ××”×ª×©×•×‘×”
        if not content.startswith("{"):
            logging.warning("âš ï¸ ×œ× JSON ×ª×§×™×Ÿ, ×× ×¡×” ×œ×—×œ×¥...")
            if "{" in content:
                start = content.find("{")
                end = content.rfind("}") + 1
                content = content[start:end]
                logging.info(f"ğŸ”§ ×—×™×œ×¦×ª×™: '{content}'")

        result = json.loads(content)
        
        # ×‘×“×™×§×•×ª ×”×™×’×™×•×Ÿ ×•validation
        validated_result = validate_extracted_data(result)
        
        logging.info(f"âœ… GPT ××¦× ×©×“×•×ª: {result}")
        if validated_result != result:
            logging.info(f"ğŸ”§ ×œ××—×¨ validation: {validated_result}")
        
        return (
            validated_result,           # extracted_data (×‘××§×•× result)
            prompt_tokens,              # prompt_tokens  
            cached_tokens,              # cached_tokens
            prompt_regular,             # prompt_regular
            completion_tokens,          # completion_tokens
            total_tokens,               # total_tokens
            cost_prompt_regular,        # cost_prompt_regular
            cost_prompt_cached,         # cost_prompt_cached  
            cost_completion,            # cost_completion
            cost_total,                 # cost_total
            cost_total_ils,             # cost_total_ils
            cost_gpt3,                  # cost_gpt3 ×‘××’×•×¨×•×ª
            usage_data.get("model", "") # model
        )

    except json.JSONDecodeError as e:
        logging.error(f"âŒ ×©×’×™××” ×‘×¤×¨×¡×•×¨ JSON: {e}")
        logging.error(f"ğŸ“„ ×”×ª×•×›×Ÿ: '{content}'")

        # ×¤×¨×¡×•×¨ ×™×“× ×™ ×›-fallback - ××¢×•×“×›×Ÿ ×œ×©×“×•×ª ×”×—×“×©×™×
        manual_result = {}
        
        # ×’×™×œ
        if "×‘×Ÿ " in text or "×‘×ª " in text:
            import re
            age_match = re.search(r'×‘[×Ÿ×ª] (\d+)', text)
            if age_match:
                manual_result["age"] = int(age_match.group(1))
        
        # ×–×”×•×ª ×“×ª×™×ª ×•×¨××ª ×“×ª×™×•×ª
        if "×™×”×•×“×™" in text:
            manual_result["self_religious_affiliation"] = "×™×”×•×“×™"
        elif "×¢×¨×‘×™" in text:
            manual_result["self_religious_affiliation"] = "×¢×¨×‘×™"
        elif "×“×¨×•×–×™" in text:
            manual_result["self_religious_affiliation"] = "×“×¨×•×–×™"
            
        if "×—×¨×“×™" in text:
            manual_result["self_religiosity_level"] = "×—×¨×“×™"
        elif "×“×ª×™ ×œ××•××™" in text:
            manual_result["self_religiosity_level"] = "×“×ª×™ ×œ××•××™"
        elif "×“×ª×™" in text:
            manual_result["self_religiosity_level"] = "×“×ª×™"
        elif "××¡×•×¨×ª×™" in text:
            manual_result["self_religiosity_level"] = "××¡×•×¨×ª×™"
        elif "×—×™×œ×•× ×™" in text:
            manual_result["self_religiosity_level"] = "×—×™×œ×•× ×™"
            
        # ××¦×‘ ×–×•×’×™
        if "×¨×•×•×§" in text:
            manual_result["relationship_type"] = "×¨×•×•×§"
        elif "× ×©×•×™" in text:
            if "×©× ×™" in text or "2" in text:
                manual_result["relationship_type"] = "× ×©×•×™+2"
            elif "×©×œ×•×©×”" in text or "3" in text:
                manual_result["relationship_type"] = "× ×©×•×™+3"
            elif "×™×œ×“×™×" in text or "×™×œ×“" in text:
                manual_result["relationship_type"] = "× ×©×•×™+2"  # default
            else:
                manual_result["relationship_type"] = "× ×©×•×™"
        elif "×’×¨×•×©" in text:
            manual_result["relationship_type"] = "×’×¨×•×©"
            
        # ××¦×‘ ××¨×•×Ÿ
        if "×‘××¨×•×Ÿ" in text:
            manual_result["closet_status"] = "×‘××¨×•×Ÿ"
        elif "×™×¦××ª×™" in text:
            manual_result["closet_status"] = "×™×¦× ×—×œ×§×™×ª"
            
        # ×˜×™×¤×•×œ
        if "×¤×¡×™×›×•×œ×•×’" in text or "×˜×™×¤×•×œ" in text:
            manual_result["attends_therapy"] = "×›×Ÿ"

        logging.info(f"ğŸ”§ ×¤×¨×¡×•×¨ ×™×“× ×™ ××¢×•×“×›×Ÿ: {manual_result}")
        
        # validation ×’× ×¢×œ ×”×¤×¨×¡×•×¨ ×”×™×“× ×™
        validated_manual = validate_extracted_data(manual_result)
        if validated_manual != manual_result:
            logging.info(f"ğŸ”§ ×¤×¨×¡×•×¨ ×™×“× ×™ ×œ××—×¨ validation: {validated_manual}")
            
        return (
            validated_manual,           # extracted_data
            0,                          # prompt_tokens (fallback)
            0,                          # cached_tokens (fallback)
            0,                          # prompt_regular (fallback)  
            0,                          # completion_tokens (fallback)
            0,                          # total_tokens (fallback)
            0.0,                        # cost_prompt_regular (fallback)
            0.0,                        # cost_prompt_cached (fallback)
            0.0,                        # cost_completion (fallback)
            0.0,                        # cost_total (fallback)
            0.0,                        # cost_total_ils (fallback)
            0,                          # cost_gpt3 (fallback)
            "fallback"                  # model (fallback)
        )

    except Exception as e:
        logging.error(f"ğŸ’¥ ×©×’×™××” ×›×œ×œ×™×ª ×‘-GPT ××—×œ×¥ ××™×“×¢: {e}")
        return (
            {},                         # extracted_data (×¨×™×§)
            0,                          # prompt_tokens
            0,                          # cached_tokens 
            0,                          # prompt_regular
            0,                          # completion_tokens
            0,                          # total_tokens
            0.0,                        # cost_prompt_regular
            0.0,                        # cost_prompt_cached
            0.0,                        # cost_completion  
            0.0,                        # cost_total
            0.0,                        # cost_total_ils
            0,                          # cost_gpt3
            "error"                     # model
        )


def validate_extracted_data(data):
    """
    ×‘×•×“×§ ×¨×§ ×“×‘×¨×™× ×‘×¡×™×¡×™×™× - ×œ× ××’×‘×™×œ ×ª×•×›×Ÿ
    """
    validated = data.copy()
    
    # ×‘×“×™×§×ª ×’×™×œ ×”×’×™×•× ×™ - ×¨×§ ××¢×œ 80
    if "age" in validated:
        try:
            age = int(validated["age"])
            if age > 80:
                logging.warning(f"âš ï¸ ×’×™×œ {age} ××¢×œ 80, ××¡×™×¨ ××”× ×ª×•× ×™×")
                del validated["age"]
            else:
                validated["age"] = age
        except (ValueError, TypeError):
            logging.warning(f"âš ï¸ ×’×™×œ ×œ× ×ª×§×™×Ÿ: {validated['age']}, ××¡×™×¨ ××”× ×ª×•× ×™×")
            del validated["age"]
    
    # ×”×’×‘×œ×ª ××•×¨×š ×©×“×•×ª ×œ×—×¡×›×•×Ÿ ×‘×˜×•×§× ×™×
    for field, value in list(validated.items()):
        if isinstance(value, str):
            if len(value) > 100:
                logging.warning(f"âš ï¸ ×©×“×” {field} ××¨×•×š ××“×™ ({len(value)} ×ª×•×•×™×), ××§×¦×¨")
                validated[field] = value[:97] + "..."
            elif len(value.strip()) == 0:
                logging.warning(f"âš ï¸ ×©×“×” {field} ×¨×™×§, ××¡×™×¨")
                del validated[field]
    
    return validated
#===============================================================================


# ============================×”×’'×™×¤×™×˜×™ ×”-4 - ××™×–×•×’ ×—×›× ×©×œ ××™×“×¢ ×¨×’×™×© ======================= 

def merge_sensitive_profile_data(existing_profile, new_data, user_message):
    """
    GPT4 - ××™×–×•×’ ×–×”×™×¨ ×•×—×›× ×©×œ ××™×“×¢ ×¨×’×™×© ×‘×ª.×– ×”×¨×’×©×™×ª
    ××˜×¤×œ ×‘××™×–×•×’ ××•×¨×›×‘ ×©×œ ×©×“×•×ª ×›××• who_knows/who_doesnt_know, trauma_history ×•×›×•'
    """
    # ×©×“×•×ª ×©×¦×¨×™×›×™× ××™×–×•×’ ××•×¨×›×‘
    complex_fields = [
        "attracted_to", "who_knows", "who_doesnt_know", "attends_therapy", 
        "primary_conflict", "trauma_history", "goal_in_course", 
        "language_of_strength", "coping_strategies", "fears_concerns", "future_vision"
    ]
    
    # ×‘×“×™×§×” ×× ×‘×××ª ×¦×¨×™×š GPT4
    needs_merge = False
    for field in complex_fields:
        if field in new_data:
            existing_value = existing_profile.get(field, "")
            if existing_value and existing_value.strip():
                needs_merge = True
                break
    
    if not needs_merge:
        logging.info("ğŸ”„ ×œ× × ×“×¨×© ××™×–×•×’ ××•×¨×›×‘, ××—×–×™×¨ ×¢×“×›×•×Ÿ ×¨×’×™×œ")
        return {**existing_profile, **new_data}

    system_prompt = """××ª×” ××•××—×” ×œ××™×–×•×’ ×–×”×™×¨ ×©×œ ××™×“×¢ ×¨×’×™×©. ×§×™×‘×œ×ª:
1. ×ª.×– ×¨×’×©×™×ª ×§×™×™××ª
2. ××™×“×¢ ×—×“×© ××”×”×•×“×¢×”
3. ×”×”×•×“×¢×” ×”××§×•×¨×™×ª ×œ×§×•× ×˜×§×¡×˜

×¢×§×¨×•× ×•×ª ×§×¨×™×˜×™×™×:
- ××œ ×ª××—×§ ××™×“×¢ ××œ× ×× ×”××©×ª××© ×××¨ ×‘××¤×•×¨×© ×©××©×”×• ×”×©×ª× ×”
- ××™×–×•×’ ×—×›×: ×¦×‘×•×¨ ××™×“×¢ ×—×“×© ×¢× ×§×™×™×, ××œ ×ª×“×¨×•×¡
- who_knows â†” who_doesnt_know: ×× ××™×©×”×• ×¢×‘×¨ ××¨×©×™××” ××—×ª ×œ×©× ×™×™×” - ×”×¡×¨ ××•×ª×• ××”×¨×©×™××” ×”×¨××©×•× ×”
- trauma_history: ×¦×‘×•×¨ ×¢× "; " ×‘×™×Ÿ ×˜×¨××•××•×ª ×©×•× ×•×ª
- attracted_to: ×©×œ×‘ ×‘××—×•×–×™× ××• ×ª×™××•×¨ ××“×•×™×§
- ×× ×™×© ×¡×ª×™×¨×” - ×”×¢×“×£ ××ª ×”××™×“×¢ ×”×—×“×© ×× ×”×•× ××¤×•×¨×©

×œ××—×¨ ×”××™×–×•×’, ×¢×“×›×Ÿ ××ª "summary" ×œ×©×§×£ ××ª ×”×–×”×•×ª ×”×¨×’×©×™×ª ×”××¢×•×“×›× ×ª:
- ×’×™×œ, ×–×”×•×ª ×“×ª×™×ª, ××¦×‘ ×–×•×’×™ ×¢×›×©×™×•
- ××¦×‘ ××¨×•×Ÿ × ×•×›×—×™ (××™ ×™×•×“×¢/×œ× ×™×•×“×¢)
- ×©×™× ×•×™×™× ××©××¢×•×ª×™×™× ×©×§×¨×•
×¢×“ 100 ×ª×•×•×™×, ×ª××¦×™×ª×™ ×•×¢×“×›× ×™.

×”×—×–×¨ ×¨×§ JSON ××¢×•×“×›×Ÿ ××œ×, ×‘×œ×™ ×”×¡×‘×¨×™×!"""

    usage_data = {
        "prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0,
        "cached_tokens": 0, "cost_prompt_regular": 0, "cost_prompt_cached": 0,
        "cost_completion": 0, "cost_total": 0, "cost_total_ils": 0, "cost_gpt4": 0, "model": ""
    }

    try:
        # ×”×›× ×ª ×”××™×“×¢ ×œ××™×–×•×’
        merge_request = {
            "existing_profile": existing_profile,
            "new_data": new_data,
            "user_message": user_message
        }
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"××™×“×¢ ×§×™×™×:\n{json.dumps(existing_profile, ensure_ascii=False, indent=2)}\n\n××™×“×¢ ×—×“×©:\n{json.dumps(new_data, ensure_ascii=False, indent=2)}\n\n×”×•×“×¢×” ××§×•×¨×™×ª:\n{user_message}"}
        ]

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0,  # ×“×™×•×§ ××§×¡×™××œ×™ ×œ××™×“×¢ ×¨×’×™×©
            max_tokens=400   # ××¡×¤×™×§ ×œ×›×œ ×”×©×“×•×ª + summary
        )

        content = response.choices[0].message.content.strip()

        # ×—×™×©×•×‘×™ ×¢×œ×•×ª
        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = response.usage.prompt_tokens_details
        cached_tokens = prompt_tokens_details['cached_tokens']
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens

        cost_prompt_regular = prompt_regular * COST_PROMPT_REGULAR
        cost_prompt_cached = cached_tokens * COST_PROMPT_CACHED
        cost_completion = completion_tokens * COST_COMPLETION
        cost_total = cost_prompt_regular + cost_prompt_cached + cost_completion
        cost_total_ils = round(cost_total * USD_TO_ILS, 4)
        cost_gpt4 = int(round(cost_total_ils * 100))

        usage_data = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,
            "cost_prompt_regular": cost_prompt_regular,
            "cost_prompt_cached": cost_prompt_cached,
            "cost_completion": cost_completion,
            "cost_total": cost_total,
            "cost_total_ils": cost_total_ils,
            "cost_gpt4": cost_gpt4,
            "model": response.model
        }

        logging.info(f"ğŸ¤– GPT4 ××™×–×•×’ ×”×—×–×™×¨: '{content[:200]}...'")
        write_gpt_log("sensitive_merge", usage_data, response.model)

        # ×¤×¨×¡×•×¨ ×”×ª×©×•×‘×”
        if not content.startswith("{"):
            if "{" in content:
                start = content.find("{")
                end = content.rfind("}") + 1
                content = content[start:end]

        merged_profile = json.loads(content)
        
        # validation ×¢×œ ×”×ª×•×¦××” ×”×¡×•×¤×™×ª
        validated_profile = validate_extracted_data(merged_profile)
        
        logging.info(f"âœ… GPT4 ×¢×“×›×Ÿ ×ª.×– ×¢× {len(validated_profile)} ×©×“×•×ª")
        if validated_profile != merged_profile:
            logging.info(f"ğŸ”§ ×œ××—×¨ validation: ×”×•×¡×¨×•/×ª×•×§× ×• ×©×“×•×ª")

        return (
            validated_profile,          # merged_profile
            prompt_tokens,              # prompt_tokens
            cached_tokens,              # cached_tokens  
            prompt_regular,             # prompt_regular
            completion_tokens,          # completion_tokens
            total_tokens,               # total_tokens
            cost_prompt_regular,        # cost_prompt_regular
            cost_prompt_cached,         # cost_prompt_cached
            cost_completion,            # cost_completion
            cost_total,                 # cost_total
            cost_total_ils,             # cost_total_ils
            cost_gpt4,                  # cost_gpt4 ×‘××’×•×¨×•×ª
            usage_data.get("model", "") # model
        )

    except json.JSONDecodeError as e:
        logging.error(f"âŒ ×©×’×™××” ×‘×¤×¨×¡×•×¨ JSON ×‘××™×–×•×’ GPT4: {e}")
        logging.error(f"ğŸ“„ ×”×ª×•×›×Ÿ: '{content}'")
        
        # fallback - ××™×–×•×’ ×¤×©×•×˜ ×‘××§×¨×” ×©×œ ×›×©×œ
        fallback_merge = {**existing_profile, **new_data}
        logging.warning("ğŸ”§ ××©×ª××© ×‘××™×–×•×’ fallback ×¤×©×•×˜")
        
        return (
            fallback_merge,             # merged_profile (fallback)
            0,                          # prompt_tokens
            0,                          # cached_tokens
            0,                          # prompt_regular
            0,                          # completion_tokens
            0,                          # total_tokens
            0.0,                        # cost_prompt_regular
            0.0,                        # cost_prompt_cached
            0.0,                        # cost_completion
            0.0,                        # cost_total
            0.0,                        # cost_total_ils
            0,                          # cost_gpt4
            "fallback"                  # model
        )

    except Exception as e:
        logging.error(f"ğŸ’¥ ×©×’×™××” ×›×œ×œ×™×ª ×‘-GPT4 ××™×–×•×’: {e}")
        
        # fallback - ××™×–×•×’ ×¤×©×•×˜ ×‘××§×¨×” ×©×œ ×›×©×œ
        fallback_merge = {**existing_profile, **new_data}
        
        return (
            fallback_merge,             # merged_profile (fallback)
            0,                          # prompt_tokens
            0,                          # cached_tokens
            0,                          # prompt_regular
            0,                          # completion_tokens
            0,                          # total_tokens
            0.0,                        # cost_prompt_regular
            0.0,                        # cost_prompt_cached
            0.0,                        # cost_completion
            0.0,                        # cost_total
            0.0,                        # cost_total_ils
            0,                          # cost_gpt4
            "error"                     # model
        )


# ×¤×•× ×§×¦×™×™×ª ×¢×–×¨ - ×§×•×‘×¢×ª ×× ×œ×”×¤×¢×™×œ GPT4
def should_use_gpt4_merge(existing_profile, new_data):
    """
    ××—×œ×™×˜×” ×× ×œ×”×¤×¢×™×œ GPT4 ×œ××™×–×•×’ ××•×¨×›×‘
    ×¨×§ ×× ×™×© ×©×“×” ××•×¨×›×‘ ×—×“×© ×•×©×“×” ×–×” ×›×‘×¨ ×§×™×™× ×‘×ª.×–
    """
    complex_fields = [
        "attracted_to", "who_knows", "who_doesnt_know", "attends_therapy", 
        "primary_conflict", "trauma_history", "goal_in_course", 
        "language_of_strength", "coping_strategies", "fears_concerns", "future_vision"
    ]
    
    for field in complex_fields:
        if field in new_data:  # GPT3 ××¦× ×©×“×” ××•×¨×›×‘ ×—×“×©
            existing_value = existing_profile.get(field, "")
            if existing_value and existing_value.strip():  # ×•×”×©×“×” ×§×™×™× ×‘×ª.×–
                logging.info(f"ğŸ¯ GPT4 × ×“×¨×© - ×©×“×” '{field}' ××¦×¨×™×š ××™×–×•×’")
                return True
    
    logging.info("âœ… ××™×Ÿ ×¦×•×¨×š ×‘-GPT4 - ×¢×“×›×•×Ÿ ×¤×©×•×˜ ××¡×¤×™×§")
    return False
#===============================================================================


# ============================×¤×•× ×§×¦×™×” ×©××¤×¢×™×œ×” ××ª ×”×’'×™×¤×™×˜×™ ×”×¨×‘×™×¢×™ ×œ×¤×™ ×”×™×’×™×•×Ÿ -×œ× ×¤×•×¢×œ ×ª××™×“ - ×¢×“×›×•×Ÿ ×—×›× ×©×œ ×ª.×– ×”×¨×’×©×™×ª ======================= 

def smart_update_profile(existing_profile, user_message):
    """
    ×¤×•× ×§×¦×™×” ×××—×“×ª ×©××˜×¤×œ×ª ×‘×›×œ ×ª×”×œ×™×š ×¢×“×›×•×Ÿ ×ª.×– ×”×¨×’×©×™×ª:
    1. ××¤×¢×™×œ×” GPT3 ×œ×—×™×œ×•×¥ ××™×“×¢
    2. ×‘×•×“×§×” ×× ×¦×¨×™×š GPT4 ×œ××™×–×•×’ ××•×¨×›×‘
    3. ××—×–×™×¨×” ×ª.×– ××¢×•×“×›× ×ª + ×›×œ × ×ª×•× ×™ ×”×¢×œ×•×™×•×ª
    
    Returns: (updated_profile, extract_usage, merge_usage_or_none)
    """
    logging.info("ğŸ”„ ××ª×—×™×œ ×¢×“×›×•×Ÿ ×—×›× ×©×œ ×ª.×– ×”×¨×’×©×™×ª")
    
    # ×©×œ×‘ 1: GPT3 - ×—×™×œ×•×¥ ××™×“×¢ ×—×“×©
    extract_result = extract_user_profile_fields(user_message)
    new_data = extract_result[0]
    extract_usage = extract_result[1:]  # ×›×œ 12 ×”×¢×¨×›×™× ×”× ×•×¡×¤×™×
    
    logging.info(f"ğŸ¤– GPT3 ×—×™×œ×¥: {list(new_data.keys())}")
    
    # ×× ××™×Ÿ ××™×“×¢ ×—×“×© - ××™×Ÿ ××” ×œ×¢×“×›×Ÿ
    if not new_data:
        logging.info("â„¹ï¸ ××™×Ÿ ××™×“×¢ ×—×“×©, ××—×–×™×¨ ×ª.×– ×œ×œ× ×©×™× ×•×™")
        return existing_profile, extract_usage, None
    
    # ×©×œ×‘ 2: ×‘×“×™×§×” ×× ×¦×¨×™×š GPT4
    if should_use_gpt4_merge(existing_profile, new_data):
        logging.info("ğŸ¯ ××¤×¢×™×œ GPT4 ×œ××™×–×•×’ ××•×¨×›×‘")
        
        # ×©×œ×‘ 3: GPT4 - ××™×–×•×’ ×—×›×
        merge_result = merge_sensitive_profile_data(existing_profile, new_data, user_message)
        updated_profile = merge_result[0]
        merge_usage = merge_result[1:]  # ×›×œ 12 ×”×¢×¨×›×™× ×”× ×•×¡×¤×™×
        
        logging.info(f"âœ… GPT4 ×¢×“×›×Ÿ ×ª.×– ×¢× {len(updated_profile)} ×©×“×•×ª")
        return updated_profile, extract_usage, merge_usage
        
    else:
        logging.info("âœ… ×¢×“×›×•×Ÿ ×¤×©×•×˜ ×œ×œ× GPT4")
        
        # ×¢×“×›×•×Ÿ ×¤×©×•×˜ - ××™×–×•×’ ×¨×’×™×œ
        updated_profile = {**existing_profile, **new_data}
        
        return updated_profile, extract_usage, None


def get_combined_usage_data(extract_usage, merge_usage=None):
    """
    ×¤×•× ×§×¦×™×™×ª ×¢×–×¨ - ××—×‘×¨×ª ××ª × ×ª×•× ×™ ×”×©×™××•×© ×GPT3 ×•-GPT4 (×× ×¨×¥)
    ××—×–×™×¨×” × ×ª×•× ×™× ×××•×—×“×™× ×œ×©××™×¨×” ×‘-sheets
    """
    # × ×ª×•× ×™ GPT3
    extract_data = {
        "extract_prompt_tokens": extract_usage[0],
        "extract_cached_tokens": extract_usage[1], 
        "extract_completion_tokens": extract_usage[3],
        "extract_total_tokens": extract_usage[4],
        "extract_cost_total": extract_usage[8],
        "extract_cost_ils": extract_usage[9],
        "extract_cost_gpt3": extract_usage[10],
        "extract_model": extract_usage[11]
    }
    
    # ×× GPT4 ×¨×¥ - ×”×•×¡×£ ××ª ×”× ×ª×•× ×™× ×©×œ×•
    if merge_usage:
        merge_data = {
            "merge_prompt_tokens": merge_usage[0],
            "merge_cached_tokens": merge_usage[1],
            "merge_completion_tokens": merge_usage[3], 
            "merge_total_tokens": merge_usage[4],
            "merge_cost_total": merge_usage[8],
            "merge_cost_ils": merge_usage[9],
            "merge_cost_gpt4": merge_usage[10],
            "merge_model": merge_usage[11],
            "used_gpt4": True
        }
        return {**extract_data, **merge_data}
    else:
        return {**extract_data, "used_gpt4": False}


# -------------------------------------------------------------
# ×”×¡×‘×¨ ×‘×¡×•×£ ×”×§×•×‘×¥ (×œ×©×™××•×©×š):

"""
ğŸ”µ ××” ×—×“×© ×›××Ÿ?

- ××™×Ÿ ×©×•× ×¤×•× ×§×¦×™×” ×©××•×¡×¨×ª â€” ×”×›×œ ××§×•×¨×™.
- × ×•×¡×¤×• ×—×™×©×•×‘×™ ×¢×œ×•×ª ×•×˜×•×§× ×™× ×œ×›×œ ×§×¨×™××” (×¨×’×™×œ, ×§×©×“, ×¤×œ×˜).
- ×›×œ ×§×¨×™××” ×©×•××¨×ª ×œ×•×’ ×¢× ×›×œ ×”×©×“×•×ª.
- ×¤×•× ×§×¦×™×•×ª ××—×–×™×¨×•×ª ×¢×›×©×™×• ××ª ×›×œ ×”×¢×¨×›×™× â€” ××¤×©×¨ ×œ×©××•×¨ ××•×ª× ×œÖ¾Google Sheets ×•×œ×¢×©×•×ª ×“×•×—×•×ª.
- × ×•×¡×£ ×”×—×–×¨ ×¢×œ×•×ª ×‘××’×•×¨×•×ª (cost_gptX) ×•×§×©×“ (cached_tokens_gptX) ×œ×›×œ ×§×¨×™××”.
- ×‘×›×œ ××§×•× × ×•×¡×£ # ×”×¡×‘×¨ ×§×¦×¨ ×‘×¢×‘×¨×™×ª ×›×“×™ ×©×ª×“×¢ ××” ×§×•×¨×”.
- ××™×Ÿ ××—×™×§×•×ª â€” ×¨×§ ×ª×•×¡×¤×•×ª.

×ª×¢×“×›×Ÿ ××•×ª×™ ×›×©×¢×‘×¨×ª, × ××©×™×š ×œ×—×™×‘×•×¨ ×œÖ¾Google Sheets!
"""
