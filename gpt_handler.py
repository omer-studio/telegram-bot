"""
gpt_handler.py â€” ×›×œ ×”×¤×•× ×§×¦×™×•×ª ×œ×˜×™×¤×•×œ ×‘Ö¾GPT ×‘××§×•× ××—×“
×‘×’×¨×¡×” ×–×• × ×•×¡×£ ×—×™×©×•×‘ ×¢×œ×•×ª ×œ×›×œ ×¡×•×’ ×˜×•×§×Ÿ (×¨×’×™×œ, ×§×©×“, ×¤×œ×˜) + ×ª×™×¢×•×“ ××œ× ×©×œ ×”×˜×•×§× ×™× + ×”×—×–×¨ ×¢×œ×•×ª ×‘××’×•×¨×•×ª ×œ×›×œ ×§×¨×™××”
"""

import json
import logging
from datetime import datetime
from config import client, SYSTEM_PROMPT

# ××—×™×¨×™× ×§×‘×•×¢×™× (× ×›×•×Ÿ ×œ×™×•× ×™ 2024) ×œÖ¾GPT-4o
COST_PROMPT_REGULAR = 0.002 / 1000    # ×˜×•×§×Ÿ ×§×œ×˜ ×¨×’×™×œ
COST_PROMPT_CACHED = 0.0005 / 1000    # ×˜×•×§×Ÿ ×§×œ×˜ ×§×©×“ (cache)
COST_COMPLETION = 0.006 / 1000        # ×˜×•×§×Ÿ ×¤×œ×˜
USD_TO_ILS = 3.8                      # ×©×¢×¨ ×“×•×œ×¨-×©×§×œ

def write_gpt_log(ttype, usage, model):
    """
    ×©×•××¨ ×œ×•×’ ×©×œ ×”×©×™××•×© ×‘×›×œ ×§×¨×™××” ×œÖ¾GPT
    """
    log_path = "/data/gpt_usage_log.jsonl"
    log_entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "type": ttype,
        "model": model,
        "tokens_prompt": usage.get("prompt_tokens", 0),
        "tokens_completion": usage.get("completion_tokens", 0),
        "tokens_total": usage.get("total_tokens", 0),
        "tokens_cached": usage.get("cached_tokens", 0),  # × ×•×¡×£: ×›××•×ª ×§×©×“
        "cost_prompt_regular": usage.get("cost_prompt_regular", 0),
        "cost_prompt_cached": usage.get("cost_prompt_cached", 0),
        "cost_completion": usage.get("cost_completion", 0),
        "cost_total": usage.get("cost_total", 0),
        "cost_total_ils": usage.get("cost_total_ils", 0),
    }
    try:
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
    except Exception as e:
        logging.error(f"×©×’×™××” ×‘×›×ª×™×‘×” ×œ×§×•×‘×¥ gpt_usage_log: {e}")

def safe_float(val):
    """
    × ×™×¡×™×•×Ÿ ×œ×”××™×¨ ×¢×¨×š ×œ-float, ×‘××§×¨×” ×©×œ ×›×©×œ ×™×—×–×™×¨ 0.0 ×¢× ×œ×•×’ ××–×”×¨×”.
    """
    try:
        return float(val)
    except (ValueError, TypeError):
        logging.warning(f"safe_float: value '{val}' could not be converted to float. Using 0.0 instead.")
        return 0.0

def get_main_response(full_messages):
    """
    GPT ×¨××©×™ - × ×•×ª×Ÿ ×ª×©×•×‘×” ×œ××©×ª××©
    ××—×–×™×¨ ×’× ××ª ×›×œ ×¤×¨×˜×™ ×”×¢×œ×•×ª (×˜×•×§× ×™×, ×§×©×“, ××—×™×¨ ××“×•×™×§)
    """
    try:
        print("ğŸ” × ×©×œ×— ×œÖ¾GPT:")
        for m in full_messages:
            print(f"{m['role']}: {m['content']}")

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=full_messages,
            temperature=1,
        )

        # ×©×œ×™×¤×ª × ×ª×•× ×™ usage
        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = response.usage.prompt_tokens_details
        cached_tokens = prompt_tokens_details['cached_tokens']
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens

        # ×—×™×©×•×‘ ×¢×œ×•×ª ×œ×¤×™ ×”×¡×•×’
        cost_prompt_regular = prompt_regular * COST_PROMPT_REGULAR
        cost_prompt_cached = cached_tokens * COST_PROMPT_CACHED
        cost_completion = completion_tokens * COST_COMPLETION
        cost_total = cost_prompt_regular + cost_prompt_cached + cost_completion
        cost_total_ils = round(cost_total * USD_TO_ILS, 4)
        cost_gpt1 = int(round(cost_total_ils * 100))  # ×¢×œ×•×ª ×‘××’×•×¨×•×ª #NEW

        print(f"ğŸ”¢ ×¤×¨×˜×™ ×©×™××•×©: prompt={prompt_tokens} ×§×©×“={cached_tokens} ×¨×’×™×œ={prompt_regular} ×¤×œ×˜={completion_tokens}")
        print(f"ğŸ’¸ ×¢×œ×•×™×•×ª: ×¨×’×™×œ ${cost_prompt_regular:.6f}, ×§×©×“ ${cost_prompt_cached:.6f}, ×¤×œ×˜ ${cost_completion:.6f}, ×¡×”×› ${cost_total:.6f} (â‚ª{cost_total_ils})")

        # ×ª×™×¢×•×“ ××œ× ×œ×œ×•×’ × ×•×¡×£
        usage_log = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,
            "cost_prompt_regular": cost_prompt_regular,
            "cost_prompt_cached": cost_prompt_cached,
            "cost_completion": cost_completion,
            "cost_total": cost_total,
            "cost_total_ils": cost_total_ils,
            "cost_gpt1": cost_gpt1  # ×‘××’×•×¨×•×ª
        }

        # ×©×•×¨×ª ×œ×•×’ ×œ×“×™×‘××’ ×•×œ××¢×§×‘
        from utils import log_event_to_file
        log_event_to_file({
            "event": "gpt_main_call",
            "gpt_input": full_messages,
            "gpt_reply": response.choices[0].message.content,
            "model": response.model,
            "usage": usage_log
        })

        # ×›×ª×™×‘×” ×œ×œ×•×’ ×©×™××•×©
        write_gpt_log("main_reply", usage_log, response.model)

        # ××—×–×™×¨ ××ª ×›×œ ×”×¤×¨××˜×¨×™×
        return (
            response.choices[0].message.content,  # bot_reply
            prompt_tokens,                        # prompt_tokens_total
            cached_tokens,                        # cached_tokens
            prompt_regular,                       # prompt_regular
            completion_tokens,                    # completion_tokens_total
            total_tokens,                         # total_tokens
            cost_prompt_regular,
            cost_prompt_cached,
            cost_completion,
            cost_total,
            cost_total_ils,                       # total_cost_ils ×‘×©"×—
            cost_gpt1,                            # cost_gpt1 ×‘××’×•×¨×•×ª
            response.model                        # model_GPT1
        )
    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-GPT ×¨××©×™: {e}")
        raise

def summarize_bot_reply(reply_text):
    """
    GPT ××§×¦×¨ - ×ª××¦×•×ª ×ª×©×•×‘×ª ×”×‘×•×˜
    (×”×•×¡×¤× ×• ×’× ×›××Ÿ ×—×™×©×•×‘ ×¢×œ×•×ª ××œ× ×•×”×—×–×¨×ª ×¢×œ×•×ª ×‘××’×•×¨×•×ª ×•×§×©×“)
    """
    system_prompt = (
        "×¡×›× ××ª ×”×”×•×“×¢×” ×©×œ×™ ×›××™×œ×• ×× ×™ ××“×‘×¨ ×¢× ×—×‘×¨: "
        "××©×¤×˜ ××—×“ ×—× ×•××™×©×™ ×‘×¡×’× ×•×Ÿ ×—×•×¤×©×™ (×œ× ×ª×™××•×¨ ×™×‘×©), ×‘×’×•×£ ×¨××©×•×Ÿ, ×›×•×œ×œ ×××™×¨×” ××™×©×™×ª ×§×¦×¨×” ×¢×œ ××”×•×ª ×”×ª×’×•×‘×” ×©×œ×™, "
        "×•××– ××ª ×”×©××œ×” ×©×©××œ×ª×™ ×× ×™×©, ×‘×¦×•×¨×” ×—××” ×•×–×•×¨××ª, ×¢×“ 20 ××™×œ×™× ×‘×¡×š ×”×›×œ. ×ª×©×œ×‘ ××™××•×’'×™ ×¨×œ×•×•× ×˜×™ ×× ××ª××™×, ×›××• ×©××“×‘×¨×™× ×‘×•×•×˜×¡××¤. "
        "××œ ×ª×¢×©×” × ×™×ª×•×—×™× ×˜×›× ×™×™× ××• ×ª×™××•×¨ ×©×œ ×”×•×“×¢×” â€“ ×××© ×ª×›×ª×•×‘ ××ª ×–×” ×›××• ×”×•×“×¢×ª ×•×•××˜×¡××¤ ×§×¦×¨×”, ×‘×’×•×£ ×¨××©×•×Ÿ, ×‘×¡×’× ×•×Ÿ ×—×•×¤×©×™ ×•×§×œ×™×œ."
    )
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": reply_text}
            ],
            temperature=0.6,
            max_tokens=40
        )
        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = response.usage.prompt_tokens_details
        cached_tokens = prompt_tokens_details['cached_tokens']
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens

        cost_prompt_regular = prompt_regular * COST_PROMPT_REGULAR
        cost_prompt_cached = cached_tokens * COST_PROMPT_CACHED
        cost_completion = completion_tokens * COST_COMPLETION
        cost_total = cost_prompt_regular + cost_prompt_cached + cost_completion
        cost_total_ils = round(cost_total * USD_TO_ILS, 4)
        cost_gpt2 = int(round(cost_total_ils * 100))  # ×‘××’×•×¨×•×ª #NEW

        usage_log = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,
            "cost_prompt_regular": cost_prompt_regular,
            "cost_prompt_cached": cost_prompt_cached,
            "cost_completion": cost_completion,
            "cost_total": cost_total,
            "cost_total_ils": cost_total_ils,
            "cost_gpt2": cost_gpt2 # ×‘××’×•×¨×•×ª
        }

        write_gpt_log("reply_summary", usage_log, response.model)

        return (
            response.choices[0].message.content.strip(),  # bot_summary
            prompt_tokens,
            cached_tokens,         # cached_tokens_gpt2 #NEW
            prompt_regular,
            completion_tokens,
            total_tokens,
            cost_prompt_regular,
            cost_prompt_cached,
            cost_completion,
            cost_total,
            cost_total_ils,
            cost_gpt2,             # cost_gpt2 ×‘××’×•×¨×•×ª #NEW
            response.model         # model_GPT2
        )
    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-GPT ××§×¦×¨: {e}")
        raise

def extract_user_profile_fields(text):
    """
    GPT ××—×œ×¥ ××™×“×¢ - ××—×œ×¥ ×¤×¨×˜×™× ××™×©×™×™× ××”×”×•×“×¢×”
    (×”×•×¡×¤× ×• ×’× ×›××Ÿ ×—×™×©×•×‘ ×¢×œ×•×ª ××œ× ×•×”×—×–×¨×ª ×¢×œ×•×ª ×‘××’×•×¨×•×ª ×•×§×©×“)
    """
    system_prompt = """××ª×” ××—×œ×¥ ××™×“×¢ ××™×©×™ ××˜×§×¡×˜.
×”×—×–×¨ JSON ×¢× ×”×©×“×•×ª ×”×‘××™× ×× ×”× ××•×–×›×¨×™×:

age - ×’×™×œ (×¨×§ ××¡×¤×¨)
religious_context - ×“×ª×™ ××• ×—×™×œ×•× ×™ ××• ××¡×•×¨×ª×™  
relationship_type - ×¨×•×•×§ ××• × ×©×•×™ ××• ×’×¨×•×©
closet_status - ×‘××¨×•×Ÿ ××• ×™×¦× ××• ×—×œ×§×™

×“×•×’×××•×ª:
"×× ×™ ×‘×Ÿ 25" â†’ {"age": 25}
"×× ×™ ×“×ª×™ ×•×¨×•×•×§" â†’ {"religious_context": "×“×ª×™", "relationship_type": "×¨×•×•×§"}
"×‘×—×•×¨ ×“×ª×™ ×‘×Ÿ 23" â†’ {"age": 23, "religious_context": "×“×ª×™"}

×¨×§ JSON, ×‘×œ×™ ×”×¡×‘×¨×™×!"""
    usage_data = {
        "prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0,
        "cached_tokens": 0, "cost_prompt_regular": 0, "cost_prompt_cached": 0,
        "cost_completion": 0, "cost_total": 0, "cost_total_ils": 0, "cost_gpt3": 0, "model": ""
    }
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text}
            ],
            temperature=0,
            max_tokens=50
        )
        content = response.choices[0].message.content.strip()

        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = response.usage.prompt_tokens_details
        cached_tokens = prompt_tokens_details['cached_tokens']
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens

        cost_prompt_regular = prompt_regular * COST_PROMPT_REGULAR
        cost_prompt_cached = cached_tokens * COST_PROMPT_CACHED
        cost_completion = completion_tokens * COST_COMPLETION
        cost_total = cost_prompt_regular + cost_prompt_cached + cost_completion
        cost_total_ils = round(cost_total * USD_TO_ILS, 4)
        cost_gpt3 = int(round(cost_total_ils * 100))  # ×‘××’×•×¨×•×ª #NEW

        usage_data = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,   # cached_tokens_gpt3 #NEW
            "cost_prompt_regular": cost_prompt_regular,
            "cost_prompt_cached": cost_prompt_cached,
            "cost_completion": cost_completion,
            "cost_total": cost_total,
            "cost_total_ils": cost_total_ils,
            "cost_gpt3": cost_gpt3,           # cost_gpt3 ×‘××’×•×¨×•×ª #NEW
            "model": response.model
        }

        # ×œ×•×’ ×œ××¢×§×‘
        logging.info(f"ğŸ¤– GPT ××—×œ×¥ ××™×“×¢ ×”×—×–×™×¨: '{content}'")
        write_gpt_log("identity_extraction", usage_data, usage_data.get("model", ""))

        # × ×™×ª×•×— JSON ××”×ª×©×•×‘×”
        if not content.startswith("{"):
            logging.warning("âš ï¸ ×œ× JSON ×ª×§×™×Ÿ, ×× ×¡×” ×œ×—×œ×¥...")
            if "{" in content:
                start = content.find("{")
                end = content.rfind("}") + 1
                content = content[start:end]
                logging.info(f"ğŸ”§ ×—×™×œ×¦×ª×™: '{content}'")

        result = json.loads(content)
        logging.info(f"âœ… GPT ××¦× ×©×“×•×ª: {result}")
        return result, usage_data

    except json.JSONDecodeError as e:
        logging.error(f"âŒ ×©×’×™××” ×‘×¤×¨×¡×•×¨ JSON: {e}")
        logging.error(f"ğŸ“„ ×”×ª×•×›×Ÿ: '{content}'")

        # ×¤×¨×¡×•×¨ ×™×“× ×™ ×›-fallback
        manual_result = {}
        if "×‘×Ÿ " in text:
            import re
            age_match = re.search(r'×‘×Ÿ (\d+)', text)
            if age_match:
                manual_result["age"] = int(age_match.group(1))
        if "×“×ª×™" in text:
            manual_result["religious_context"] = "×“×ª×™"
        if "×¨×•×•×§" in text:
            manual_result["relationship_type"] = "×¨×•×•×§"

        logging.info(f"ğŸ”§ ×¤×¨×¡×•×¨ ×™×“× ×™: {manual_result}")
        return manual_result, usage_data

    except Exception as e:
        logging.error(f"ğŸ’¥ ×©×’×™××” ×›×œ×œ×™×ª ×‘-GPT ××—×œ×¥ ××™×“×¢: {e}")
        return {}, usage_data



def calculate_total_cost(main_usage, summary_usage, extract_usage):
    """
    ××—×©×‘ ××ª ×¡×š ×›×œ ×”×˜×•×§× ×™× ×•×”×¢×œ×•×ª ××›×œ ×©×œ×•×© ×”×§×¨×™××•×ª ×œÖ¾GPT:
    - main_usage: ×§×¨×™××” ×¨××©×™×ª (tuple)
    - summary_usage: ×¡×™×›×•× (tuple)
    - extract_usage: ×—×™×œ×•×¥ ×ª×¢×•×“×ª ×–×”×•×ª (dict)
    ××—×–×™×¨:
    - total_tokens: ×¡×›×•× ×˜×•×§× ×™×
    - cost_usd: ×¢×œ×•×ª ×›×•×œ×œ×ª ×‘×“×•×œ×¨×™×
    - cost_ils: ×¢×œ×•×ª ×›×•×œ×œ×ª ×‘×©"×— (×¢×’×•×œ ×œ-4 ×¡×¤×¨×•×ª)
    """
    try:
        # ×©×œ×™×¤×” ××”×§×¨×™××” ×”×¨××©×™×ª
        main_prompt = main_usage[1] if len(main_usage) > 1 else 0
        main_completion = main_usage[4] if len(main_usage) > 4 else 0
        main_total = main_usage[5] if len(main_usage) > 5 else 0
        cost_main_usd = main_usage[9] if len(main_usage) > 9 else 0
        cost_main_ils = main_usage[10] if len(main_usage) > 10 else 0

        # ×©×œ×™×¤×” ××”×¡×™×›×•×
        summary_prompt = summary_usage[1] if len(summary_usage) > 1 else 0
        summary_completion = summary_usage[4] if len(summary_usage) > 4 else 0
        summary_total = summary_usage[5] if len(summary_usage) > 5 else 0
        cost_summary_usd = summary_usage[9] if len(summary_usage) > 9 else 0
        cost_summary_ils = summary_usage[10] if len(summary_usage) > 10 else 0

        # ×©×œ×™×¤×” ××”×—×™×œ×•×¥
        extract_total = extract_usage.get("total_tokens", 0)
        cost_extract_usd = extract_usage.get("cost_total", 0)
        cost_extract_ils = extract_usage.get("cost_total_ils", 0)

        # ×—×™×‘×•×¨ ×›×œ ×”×˜×•×§× ×™×
        total_tokens = main_total + summary_total + extract_total
        cost_usd = round(cost_main_usd + cost_summary_usd + cost_extract_usd, 6)
        cost_ils = round(cost_main_ils + cost_summary_ils + cost_extract_ils, 4)

        return total_tokens, cost_usd, cost_ils

    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘×—×™×©×•×‘ ×¢×œ×•×ª ×›×•×œ×œ×ª: {e}")
        return 0, 0.0, 0.0


# -------------------------------------------------------------
# ×”×¡×‘×¨ ×‘×¡×•×£ ×”×§×•×‘×¥ (×œ×©×™××•×©×š):

"""
ğŸ”µ ××” ×—×“×© ×›××Ÿ?

- ××™×Ÿ ×©×•× ×¤×•× ×§×¦×™×” ×©××•×¡×¨×ª â€” ×”×›×œ ××§×•×¨×™.
- × ×•×¡×¤×• ×—×™×©×•×‘×™ ×¢×œ×•×ª ×•×˜×•×§× ×™× ×œ×›×œ ×§×¨×™××” (×¨×’×™×œ, ×§×©×“, ×¤×œ×˜).
- ×›×œ ×§×¨×™××” ×©×•××¨×ª ×œ×•×’ ×¢× ×›×œ ×”×©×“×•×ª.
- ×¤×•× ×§×¦×™×•×ª ××—×–×™×¨×•×ª ×¢×›×©×™×• ××ª ×›×œ ×”×¢×¨×›×™× â€” ××¤×©×¨ ×œ×©××•×¨ ××•×ª× ×œÖ¾Google Sheets ×•×œ×¢×©×•×ª ×“×•×—×•×ª.
- × ×•×¡×£ ×”×—×–×¨ ×¢×œ×•×ª ×‘××’×•×¨×•×ª (cost_gptX) ×•×§×©×“ (cached_tokens_gptX) ×œ×›×œ ×§×¨×™××”.
- ×‘×›×œ ××§×•× × ×•×¡×£ # ×”×¡×‘×¨ ×§×¦×¨ ×‘×¢×‘×¨×™×ª ×›×“×™ ×©×ª×“×¢ ××” ×§×•×¨×”.
- ××™×Ÿ ××—×™×§×•×ª â€” ×¨×§ ×ª×•×¡×¤×•×ª.

×ª×¢×“×›×Ÿ ××•×ª×™ ×›×©×¢×‘×¨×ª, × ××©×™×š ×œ×—×™×‘×•×¨ ×œÖ¾Google Sheets!
"""
