"""
××—×œ×§×ª AI - ×›×œ ×¤×•× ×§×¦×™×•×ª ×”-GPT ×‘××§×•× ××—×“
"""
import json
import logging

from config import client, SYSTEM_PROMPT

def safe_float(val):
    """
    × ×™×¡×™×•×Ÿ ×œ×”××™×¨ ×¢×¨×š ×œ-float, ×‘××§×¨×” ×©×œ ×›×©×œ ×™×—×–×™×¨ 0.0 ×¢× ×œ×•×’ ××–×”×¨×”.
    """
    try:
        return float(val)
    except (ValueError, TypeError):
        logging.warning(f"safe_float: value '{val}' could not be converted to float. Using 0.0 instead.")
        return 0.0

def get_main_response(full_messages):
    """
    GPT ×¨××©×™ - × ×•×ª×Ÿ ×ª×©×•×‘×” ×œ××©×ª××©
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=full_messages
        )
        return (
            response.choices[0].message.content,
            response.usage.prompt_tokens,
            response.usage.completion_tokens,
            response.usage.total_tokens,
            response.model
        )
    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-GPT ×¨××©×™: {e}")
        raise

def summarize_bot_reply(reply_text):
    """
    GPT ××§×¦×¨ - ××§×¦×¨ ××ª ×ª×©×•×‘×ª ×”×‘×•×˜
    """
    system_prompt = "×ª××¦×ª ××ª ××©××¢×•×ª ×”×”×•×“×¢×” ×‘××©×¤×˜ ×§×¦×¨ (×¢×“ 10 ××™×œ×™×). ×‘×œ×™ ×¦×™×˜×•×˜×™×, ×‘×œ×™ × ×™×ª×•×—×™× â€“ ×¨×§ ×ª×™××•×¨ ×™×‘×© ×©×œ ××”×•×ª ×”×”×•×“×¢×”."
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": reply_text}
            ],
            temperature=0.2,
            max_tokens=30
        )
        return (
            response.choices[0].message.content.strip(),
            response.usage.prompt_tokens,
            response.usage.completion_tokens,
            response.usage.total_tokens,
            response.model
        )
    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-GPT ××§×¦×¨: {e}")
        raise

def extract_user_profile_fields(text):
    """
    GPT ××—×œ×¥ ××™×“×¢ - ××—×œ×¥ ×¤×¨×˜×™× ××™×©×™×™× ××”×”×•×“×¢×”
    """
    system_prompt = """××ª×” ××—×œ×¥ ××™×“×¢ ××™×©×™ ××˜×§×¡×˜.
×”×—×–×¨ JSON ×¢× ×”×©×“×•×ª ×”×‘××™× ×× ×”× ××•×–×›×¨×™×:

age - ×’×™×œ (×¨×§ ××¡×¤×¨)
religious_context - ×“×ª×™ ××• ×—×™×œ×•× ×™ ××• ××¡×•×¨×ª×™  
relationship_type - ×¨×•×•×§ ××• × ×©×•×™ ××• ×’×¨×•×©
closet_status - ×‘××¨×•×Ÿ ××• ×™×¦× ××• ×—×œ×§×™

×“×•×’×××•×ª:
"×× ×™ ×‘×Ÿ 25" â†’ {"age": 25}
"×× ×™ ×“×ª×™ ×•×¨×•×•×§" â†’ {"religious_context": "×“×ª×™", "relationship_type": "×¨×•×•×§"}
"×‘×—×•×¨ ×“×ª×™ ×‘×Ÿ 23" â†’ {"age": 23, "religious_context": "×“×ª×™"}

×¨×§ JSON, ×‘×œ×™ ×”×¡×‘×¨×™×!"""

    usage_data = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "model": ""}
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text}
            ],
            temperature=0,
            max_tokens=50
        )
        content = response.choices[0].message.content.strip()
        usage_data = {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens,
            "model": response.model
        }

        logging.info(f"ğŸ¤– GPT ××—×œ×¥ ××™×“×¢ ×”×—×–×™×¨: '{content}'")

        # ×× ×–×” ×œ× JSON, × × ×¡×” ×œ×—×œ×¥
        if not content.startswith("{"):
            logging.warning("âš ï¸ ×œ× JSON ×ª×§×™×Ÿ, ×× ×¡×” ×œ×—×œ×¥...")
            if "{" in content:
                start = content.find("{")
                end = content.rfind("}") + 1
                content = content[start:end]
                logging.info(f"ğŸ”§ ×—×™×œ×¦×ª×™: '{content}'")

        result = json.loads(content)
        logging.info(f"âœ… GPT ××¦× ×©×“×•×ª: {result}")
        return result, usage_data

    except json.JSONDecodeError as e:
        logging.error(f"âŒ ×©×’×™××” ×‘×¤×¨×¡×•×¨ JSON: {e}")
        logging.error(f"ğŸ“„ ×”×ª×•×›×Ÿ: '{content}'")

        # ×¤×¨×¡×•×¨ ×™×“× ×™ ×›-fallback
        manual_result = {}
        if "×‘×Ÿ " in text:
            import re
            age_match = re.search(r'×‘×Ÿ (\d+)', text)
            if age_match:
                manual_result["age"] = int(age_match.group(1))
        if "×“×ª×™" in text:
            manual_result["religious_context"] = "×“×ª×™"
        if "×¨×•×•×§" in text:
            manual_result["relationship_type"] = "×¨×•×•×§"

        logging.info(f"ğŸ”§ ×¤×¨×¡×•×¨ ×™×“× ×™: {manual_result}")
        return manual_result, usage_data

    except Exception as e:
        logging.error(f"ğŸ’¥ ×©×’×™××” ×›×œ×œ×™×ª ×‘-GPT ××—×œ×¥ ××™×“×¢: {e}")
        return {}, usage_data

def calculate_total_cost(main_usage, summary_usage, extract_usage):
    """
    ××—×©×‘ ××ª ×”×¢×œ×•×ª ×”×›×•×œ×œ×ª ×©×œ ×›×œ ×”-GPT calls
    """
    # ×©×™××•×© ×‘×¤×•× ×§×¦×™×™×ª safe_float ×›×“×™ ×œ×”×™×× ×¢ ××§×¨×™×¡×” ×¢×œ ×¢×¨×›×™× ×¨×™×§×™× ××• ×œ× ×—×•×§×™×™×
    total_tokens = (
        safe_float(main_usage[2]) +
        safe_float(summary_usage[2]) +
        safe_float(extract_usage.get("total_tokens", 0))
    )
    try:
        cost_usd = round(
            safe_float(main_usage[0]) * 0.000005 + safe_float(main_usage[1]) * 0.000015 +
            safe_float(summary_usage[0]) * 0.000005 + safe_float(summary_usage[1]) * 0.000015 +
            safe_float(extract_usage.get("prompt_tokens", 0)) * 0.000005 +
            safe_float(extract_usage.get("completion_tokens", 0)) * 0.000015,
            6
        )
        cost_ils = round(cost_usd * 3.8, 4)
    except Exception as e:
        cost_usd = cost_ils = 0
        logging.error(f"[COST ERROR] {e}")

    return total_tokens, cost_usd, cost_ils

# ×ª×•×›×œ ×œ×”×•×¡×™×£ ×›××Ÿ ×¤×•× ×§×¦×™×•×ª × ×•×¡×¤×•×ª ×œ×¤×™ ×”×¦×•×¨×š, ×›×•×œ×œ ×œ×•×’×™×§×•×ª ×¢×–×¨, ×‘×“×™×§×•×ª ×•×›×•'.
