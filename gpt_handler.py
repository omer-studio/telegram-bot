"""
××—×œ×§×ª AI - ×›×œ ×¤×•× ×§×¦×™×•×ª ×”-GPT ×‘××§×•× ××—×“
"""
import json
import logging

from config import client, SYSTEM_PROMPT


def get_main_response(full_messages):
    """
    GPT ×¨××©×™ - × ×•×ª×Ÿ ×ª×©×•×‘×” ×œ××©×ª××©
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=full_messages
        )
        
        return (
            response.choices[0].message.content,
            response.usage.prompt_tokens,
            response.usage.completion_tokens,
            response.usage.total_tokens,
            response.model
        )
    except Exception as e:
        print(f"âŒ ×©×’×™××” ×‘-GPT ×¨××©×™: {e}")
        raise


def summarize_bot_reply(reply_text):
    """
    GPT ××§×¦×¨ - ××§×¦×¨ ××ª ×ª×©×•×‘×ª ×”×‘×•×˜
    """
    system_prompt = "×ª××¦×ª ××ª ××©××¢×•×ª ×”×”×•×“×¢×” ×‘××©×¤×˜ ×§×¦×¨ (×¢×“ 10 ××™×œ×™×). ×‘×œ×™ ×¦×™×˜×•×˜×™×, ×‘×œ×™ × ×™×ª×•×—×™× â€“ ×¨×§ ×ª×™××•×¨ ×™×‘×© ×©×œ ××”×•×ª ×”×”×•×“×¢×”."
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt}, 
                {"role": "user", "content": reply_text}
            ],
            temperature=0.2,
            max_tokens=30
        )
        
        return (
            response.choices[0].message.content.strip(),
            response.usage.prompt_tokens,
            response.usage.completion_tokens, 
            response.usage.total_tokens,
            response.model
        )
    except Exception as e:
        print(f"âŒ ×©×’×™××” ×‘-GPT ××§×¦×¨: {e}")
        raise


def extract_user_profile_fields(text):
    """
    GPT ××—×œ×¥ ××™×“×¢ - ××—×œ×¥ ×¤×¨×˜×™× ××™×©×™×™× ××”×”×•×“×¢×”
    """
    system_prompt = """××ª×” ××—×œ×¥ ××™×“×¢ ××™×©×™ ××˜×§×¡×˜.
×”×—×–×¨ JSON ×¢× ×”×©×“×•×ª ×”×‘××™× ×× ×”× ××•×–×›×¨×™×:

age - ×’×™×œ (×¨×§ ××¡×¤×¨)
religious_context - ×“×ª×™ ××• ×—×™×œ×•× ×™ ××• ××¡×•×¨×ª×™  
relationship_type - ×¨×•×•×§ ××• × ×©×•×™ ××• ×’×¨×•×©
closet_status - ×‘××¨×•×Ÿ ××• ×™×¦× ××• ×—×œ×§×™

×“×•×’×××•×ª:
"×× ×™ ×‘×Ÿ 25" â†’ {"age": 25}
"×× ×™ ×“×ª×™ ×•×¨×•×•×§" â†’ {"religious_context": "×“×ª×™", "relationship_type": "×¨×•×•×§"}
"×‘×—×•×¨ ×“×ª×™ ×‘×Ÿ 23" â†’ {"age": 23, "religious_context": "×“×ª×™"}

×¨×§ JSON, ×‘×œ×™ ×”×¡×‘×¨×™×!"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt}, 
                {"role": "user", "content": text}
            ],
            temperature=0,
            max_tokens=50
        )

        content = response.choices[0].message.content.strip()
        usage_data = {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens,
            "model": response.model
        }
        
        print(f"ğŸ¤– GPT ××—×œ×¥ ××™×“×¢ ×”×—×–×™×¨: '{content}'")

        # ×× ×–×” ×œ× JSON, × × ×¡×” ×œ×—×œ×¥
        if not content.startswith("{"):
            print("âš ï¸ ×œ× JSON ×ª×§×™×Ÿ, ×× ×¡×” ×œ×—×œ×¥...")
            if "{" in content:
                start = content.find("{")
                end = content.rfind("}") + 1
                content = content[start:end]
                print(f"ğŸ”§ ×—×™×œ×¦×ª×™: '{content}'")

        result = json.loads(content)
        print(f"âœ… GPT ××¦× ×©×“×•×ª: {result}")
        return result, usage_data

    except json.JSONDecodeError as e:
        print(f"âŒ ×©×’×™××” ×‘×¤×¨×¡×•×¨ JSON: {e}")
        print(f"ğŸ“„ ×”×ª×•×›×Ÿ: '{content}'")
        
        # ×¤×¨×¡×•×¨ ×™×“× ×™ ×›-fallback
        manual_result = {}
        if "×‘×Ÿ " in text:
            import re
            age_match = re.search(r'×‘×Ÿ (\d+)', text)
            if age_match:
                manual_result["age"] = int(age_match.group(1))
        if "×“×ª×™" in text:
            manual_result["religious_context"] = "×“×ª×™"
        if "×¨×•×•×§" in text:
            manual_result["relationship_type"] = "×¨×•×•×§"
        
        print(f"ğŸ”§ ×¤×¨×¡×•×¨ ×™×“× ×™: {manual_result}")
        return manual_result, usage_data

    except Exception as e:
        print(f"ğŸ’¥ ×©×’×™××” ×›×œ×œ×™×ª ×‘-GPT ××—×œ×¥ ××™×“×¢: {e}")
        return {}, {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "model": ""}


def calculate_total_cost(main_usage, summary_usage, extract_usage):
    """
    ××—×©×‘ ××ª ×”×¢×œ×•×ª ×”×›×•×œ×œ×ª ×©×œ ×›×œ ×”-GPT calls
    """
    total_tokens = (
        main_usage[2] +  # main total_tokens  
        summary_usage[2] +  # summary total_tokens
        extract_usage["total_tokens"]  # extract total_tokens
    )
    
    try:
        cost_usd = round(
            float(main_usage[0]) * 0.000005 + float(main_usage[1]) * 0.000015 +
            float(summary_usage[0]) * 0.000005 + float(summary_usage[1]) * 0.000015 +
            float(extract_usage["prompt_tokens"]) * 0.000005 + float(extract_usage["completion_tokens"]) * 0.000015,
            6
        )
        cost_ils = round(cost_usd * 3.8, 4)
    except Exception as e:
        cost_usd = cost_ils = 0
        logging.error(f"[COST ERROR] {e}")

    return total_tokens, cost_usd, cost_ils
