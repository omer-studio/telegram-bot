"""
gpt_handler.py
--------------
×§×•×‘×¥ ×–×” ××¨×›×– ××ª ×›×œ ×”×¤×•× ×§×¦×™×•×ª ×©××‘×¦×¢×•×ª ××™× ×˜×¨××§×¦×™×” ×¢× GPT (×©×œ×™×—×ª ×”×•×“×¢×•×ª, ×—×™×©×•×‘ ×¢×œ×•×ª, ×“×™×‘××’×™× ×’).
×”×¨×¦×™×•× ×œ: ×¨×™×›×•×– ×›×œ ×”×œ×•×’×™×§×” ×©×œ GPT ×‘××§×•× ××—×“, ×›×•×œ×œ ×ª×™×¢×•×“ ××œ× ×©×œ ×˜×•×§× ×™×, ×¢×œ×•×™×•×ª, ×•×œ×•×’×™×.

××¢×¨×›×ª ×—×™×©×•×‘ ×¢×œ×•×ª GPT ×“×™× ×××™×ª (×™×•× ×™ 2025)
--------------------------------------------------
- ×›×œ ×—×™×©×•×‘ ×¢×œ×•×ª ×˜×•×§× ×™× ××ª×‘×¦×¢ ×“×™× ×××™×ª ×œ×¤×™ ×§×•×‘×¥ gpt_pricing.json.
- ×›×œ ××•×“×œ (gpt-4o, nano, mini ×•×›×•') ××•×’×“×¨ ×¢× ××—×™×¨×™ prompt/cached/completion ×‘×§×•×‘×¥ JSON ×–×”.
- ×›×œ ×©×™× ×•×™ ××—×™×¨×•×Ÿ ××• ×”×•×¡×¤×ª ××•×“×œ ×—×“×© â€“ ×™×© ×œ×¢×“×›×Ÿ ××š ×•×¨×§ ××ª gpt_pricing.json (××™×Ÿ ×¦×•×¨×š ×œ×©× ×•×ª ×§×•×“).
- ×× ×©× ×”××•×“×œ ×œ× ×§×™×™× ×‘××—×™×¨×•×Ÿ â€“ ×ª×•×—×–×¨ ×¢×œ×•×ª 0 ×•×ª×™×¨×©× ×©×’×™××” ×‘×œ×•×’.
×œ×¤× ×™ ×”×©×™× ×•×™ ×”×’×“×•×œ ×©×œ ×”GPTS ----------------------------
# ×“×•×§×•×× ×˜×¦×™×”:
# - ×œ×¢×“×›×•×Ÿ ××—×™×¨×™×: ×¢×¨×•×š ××ª gpt_pricing.json ×‘×œ×‘×“.
# - ×œ×”×•×¡×£ ××•×“×œ: ×”×•×¡×£ ×¢×¨×š ×—×“×© ×œ-gpt_pricing.json ×¢× ×©× ×”××•×“×œ ×•×”××—×™×¨×™×.
# - ×—×•×‘×” ×œ×©××•×¨ ×¢×œ ×©××•×ª ×ª×•×××™× ×‘×™×Ÿ usage (response.model) ×œ×‘×™×Ÿ ×”××¤×ª×—×•×ª ×‘-JSON.
"""

import json
import logging
from datetime import datetime
from config import client, GPT_LOG_PATH
import os
from fields_dict import FIELDS_DICT
import threading
from prompts import PROFILE_EXTRACTION_PROMPT, BOT_REPLY_SUMMARY_PROMPT, SENSITIVE_PROFILE_MERGE_PROMPT
import asyncio
import re
from gpt_usage_manager import GPTUsageManager

# ===================== ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×œ×œ×•×’×™× ×•×“×™×‘××’ =====================

def _debug_gpt_usage(model_name, prompt_tokens, completion_tokens, cached_tokens, total_tokens, call_type):
    """
    ××“×¤×™×¡ ××™×“×¢ ×“×™×‘××’ ×¢×œ ×©×™××•×© ×‘-GPT (×œ×•×’×™× ×¤× ×™××™×™× ×‘×œ×‘×“).
    """
    print(f"[DEBUG][{call_type}] ××•×“×œ: {model_name}, prompt: {prompt_tokens}, completion: {completion_tokens}, cached: {cached_tokens}, total: {total_tokens}")

import os
import json
from datetime import datetime

def write_gpt_log(call_type, usage_log, model_name):
    """
    ×©×•××¨ usage log ×œ×§×•×‘×¥ DATA/gpt_usage_log.jsonl (×©×•×¨×” ××—×ª ×œ×›×œ ×§×¨×™××”).
    """
    log_entry = {
        "timestamp": datetime.now().isoformat(timespec="microseconds"),
        "type": call_type,
        "model": model_name,
        **usage_log
    }
    log_path = GPT_LOG_PATH  # ×‘××§×•× os.path.join("DATA", "gpt_usage_log.jsonl")
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

# ===================== ×”×’×“×¨×ª ×©×¢×¨ ×”×—×œ×™×¤×™×Ÿ =====================
USD_TO_ILS = 3.7  # ×©×¢×¨ ×”×“×•×œ×¨-×©×§×œ (×™×© ×œ×¢×“×›×Ÿ ×œ×¤×™ ×”×¦×•×¨×š)

# ×”×’×“×¨×ª × ×ª×™×‘ ×œ×•×’ ××—×™×“ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
os.makedirs(PROJECT_ROOT, exist_ok=True)

# ===================== ×˜×¢×™× ×ª ××—×™×¨×•×Ÿ ×“×™× ×××™ ×œ×›×œ ×”××•×“×œ×™× (×™×•× ×™ 2025) =====================

# ×˜×•×¢×Ÿ ××ª ×”××—×™×¨×•×Ÿ ××”×§×•×‘×¥ ×¤×¢× ××—×ª ×‘×œ×‘×“
PRICING_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'gpt_pricing.json')
try:
    with open(PRICING_PATH, encoding='utf-8') as f:
        GPT_PRICING = json.load(f)
except Exception as e:
    print(f"[ERROR] ×œ× ×”×¦×œ×—×ª×™ ×œ×˜×¢×•×Ÿ ××ª gpt_pricing.json: {e}")
    GPT_PRICING = {}

# ×¤×•× ×§×¦×™×” ×©××‘×™××” ××ª ××—×™×¨×™ ×”×˜×•×§× ×™× ×œ×¤×™ ×©× ×”××•×“×œ
# ××—×–×™×¨×” ××™×œ×•×Ÿ ×¢× prompt/cached/completion, ××• None ×× ×œ× ×§×™×™×
# ××‘×¦×¢×ª normalization ×œ×©× ×”××•×“×œ (×œ××©×œ gpt-4o-2024-08-06 -> gpt-4o)
def get_model_prices(model_name):
    if not model_name:
        return None
    # × ×™×§×•×™ ×’×¨×¡××•×ª ××”×©× (×œ××©×œ gpt-4o-2024-08-06 -> gpt-4o)
    base_name = model_name.split("-")[0]
    # ×—×™×¤×•×© ××“×•×™×§
    if model_name in GPT_PRICING:
        return GPT_PRICING[model_name]
    # ×—×™×¤×•×© ×œ×¤×™ base_name
    for key in GPT_PRICING:
        if model_name.startswith(key):
            return GPT_PRICING[key]
    if base_name in GPT_PRICING:
        return GPT_PRICING[base_name]
    # ×œ× × ××¦× ××—×™×¨×•×Ÿ
    print(f"[ERROR] ×œ× × ××¦× ××—×™×¨×•×Ÿ ×œ××•×“×œ: {model_name}")
    return None

# ×™×¦×™×¨×ª ××•×¤×¢ ×’×œ×•×‘×œ×™ ×©×œ ×× ×”×œ usage (×˜×¢×™× ×” ×—×“ ×¤×¢××™×ª ×©×œ ×”××—×™×¨×•×Ÿ)
gpt_usage_manager = GPTUsageManager()

# ×¢×“×›×•×Ÿ calculate_gpt_cost ×œ×”×©×ª××© ×‘×× ×”×œ ×”×—×“×©

def calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens=0, model_name='gpt-4o', usd_to_ils=USD_TO_ILS):
    """
    ××—×©×‘ ×¢×œ×•×ª GPT (USD, ILS, ××’×•×¨×•×ª) ×œ×¤×™ ××¡×¤×¨ ×˜×•×§× ×™×, ×›×•×œ×œ ×˜×•×§× ×™× ×¨×’×™×œ×™×, ×§×©×“ ×•×¤×œ×˜, ×•×œ×¤×™ ×©× ×”××•×“×œ.
    ×§×œ×˜: prompt_tokens, completion_tokens, cached_tokens, model_name, usd_to_ils
    ×¤×œ×˜: dict usage ××—×™×“ ×¢× ×›×œ ×”×¢×¨×›×™×.
    """
    return gpt_usage_manager.calculate(model_name, prompt_tokens, completion_tokens, cached_tokens, usd_to_ils)

# ============================×”×’'×™×¤×™×˜×™ ×”-1 - ×¤×•×¢×œ ×ª××™×“ ×•×¢×•× ×” ×ª×©×•×‘×” ×œ××©×ª××© ======================= 


def get_main_response(full_messages):
    """
    ×©×•×œ×— ×”×•×“×¢×” ×œ-GPT ×”×¨××©×™ ×•××—×–×™×¨ ××ª ×”×ª×©×•×‘×”, ×›×•×œ×œ ×¤×™×¨×•×˜ ×¢×œ×•×ª ×•×˜×•×§× ×™×.
    ×§×œ×˜: full_messages â€” ×¨×©×™××ª ×”×•×“×¢×•×ª (×›×•×œ×œ system prompt).
    ×¤×œ×˜: dict ×¢× ×ª×©×•×‘×”, usage, ×¢×œ×•×ª.
    # ××”×œ×š ××¢× ×™×™×Ÿ: ×©×™××•×© ×‘×¤×¨×•××˜ ×”×¨××©×™ ×©××’×“×™×¨ ××ª ×”××™×©×™×•×ª ×©×œ ×“× ×™××œ.
    """
    try:
        # full_messages ×›×•×œ×œ ××ª ×”-SYSTEM_PROMPT ×›×‘×¨ ×‘×ª×—×™×œ×ª×• (× ×‘× ×” ×‘-message_handler)
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=full_messages,
            temperature=1,
        )

        # --- DEBUG: Print all usage fields from API ---
        try:
            def _to_serializable(val):
                if hasattr(val, '__dict__'):
                    return {k: _to_serializable(v) for k, v in vars(val).items()}
                elif isinstance(val, (list, tuple)):
                    return [_to_serializable(x) for x in val]
                elif isinstance(val, dict):
                    return {k: _to_serializable(v) for k, v in val.items()}
                else:
                    try:
                        json.dumps(val)
                        return val
                    except Exception:
                        return str(val)
            usage_dict = {}
            for k in dir(response.usage):
                if not k.startswith("_") and not callable(getattr(response.usage, k)):
                    v = getattr(response.usage, k)
                    usage_dict[k] = _to_serializable(v)
            print(f"[DEBUG] API usage raw: {json.dumps(usage_dict, ensure_ascii=False)}")
        except Exception as e:
            print(f"[DEBUG] Failed to print API usage fields: {e}")

        # ×©×œ×™×¤×ª × ×ª×•× ×™ usage
        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = response.usage.prompt_tokens_details
        cached_tokens = prompt_tokens_details.cached_tokens
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens
        model_name = response.model

        # --- Smart debug ---
        _debug_gpt_usage(model_name, prompt_tokens, completion_tokens, cached_tokens, total_tokens, "main_reply")

        # ×—×™×©×•×‘ ×¢×œ×•×ª ×“×™× ×××™ ×œ×¤×™ ×”××•×“×œ
        cost_data = calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens, model_name)
        # ×›×œ ×”×©×“×•×ª × ×©××¨×™× ×‘-usage_log
        usage_log = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,
            "prompt_regular": prompt_regular,
            **cost_data,
            "model": response.model
        }

        from utils import log_event_to_file
        log_event_to_file({
            "event": "gpt_main_call",
            "gpt_input": full_messages,
            "gpt_reply": response.choices[0].message.content,
            "model": response.model,
            "usage": usage_log
        })

        write_gpt_log("main_reply", usage_log, response.model)

        return {
            "bot_reply": response.choices[0].message.content,
            **usage_log
        }
    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-GPT ×¨××©×™: {e}")
        raise

def get_main_response_async(*args, **kwargs):
    loop = asyncio.get_event_loop()
    return loop.run_in_executor(None, get_main_response, *args, **kwargs)

# ============================×”×’'×™×¤×™×˜×™ ×”-2 - ××§×¦×¨ ××ª ×ª×©×•×‘×ª ×”×‘×•×˜ ×× ×”×™× ××¨×•×›×” ××“×™ ×›×“×™ ×œ×—×¡×•×š ×‘×”×™×¡×˜×•×¨×™×” ======================= 


def summarize_bot_reply(reply_text):
    """
    ××¡×›× ××ª ×ª×©×•×‘×ª ×”×‘×•×˜ ×œ××©×¤×˜ ×§×¦×¨ ×•×—×, ×‘×¡×’× ×•×Ÿ ×•×•××˜×¡××¤.
    ×§×œ×˜: reply_text (×˜×§×¡×˜ ×ª×©×•×‘×ª ×”×‘×•×˜)
    ×¤×œ×˜: ×¡×™×›×•× ×§×¦×¨ (str), usage (dict)
    # ××”×œ×š ××¢× ×™×™×Ÿ: ×©×™××•×© ×‘×¤×¨×•××˜ ×ª××¦×•×ª ×©××•×’×“×¨ ×‘-prompts.py.
    """
    system_prompt = BOT_REPLY_SUMMARY_PROMPT  # ×¤×¨×•××˜ ×ª××¦×•×ª ×ª×©×•×‘×”
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": reply_text}],
            temperature=1,
        )
        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = response.usage.prompt_tokens_details
        cached_tokens = prompt_tokens_details.cached_tokens
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens
        model_name = response.model

        # --- Smart debug ---
        _debug_gpt_usage(model_name, prompt_tokens, completion_tokens, cached_tokens, total_tokens, "reply_summary")

        # ×—×™×©×•×‘ ×¢×œ×•×ª ×“×™× ×××™ ×œ×¤×™ ×”××•×“×œ
        cost_data = calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens, model_name)
        usage_log = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,
            "prompt_regular": prompt_regular,
            **cost_data,
            "model": response.model
        }
        write_gpt_log("reply_summary", usage_log, response.model)
        return {
            "bot_summary": response.choices[0].message.content.strip(),
            **usage_log
        }
    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-GPT ××§×¦×¨: {e}")
        raise

def summarize_bot_reply_async(*args, **kwargs):
    loop = asyncio.get_event_loop()
    return loop.run_in_executor(None, summarize_bot_reply, *args, **kwargs)

# ============================×”×’'×™×¤×™×˜×™ ×”-3 - ×¤×•×¢×œ ×ª××™×“ ×•××—×œ×¥ ××™×“×¢ ×œ×ª.×– ×”×¨×’×©×™×ª ======================= 

def validate_extracted_data(data):
    """
    ×‘×•×“×§ ×× ×”× ×ª×•× ×™× ×©×—×•×œ×¦×• ××”-GPT ×ª×§×™× ×™× (dict, ××¤×ª×—×•×ª ××¡×•×’ str ×‘×œ×‘×“).
    ×§×œ×˜: data (dict)
    ×¤×œ×˜: True/False
    """
    validated = data.copy()
    
    # ×‘×“×™×§×ª ×’×™×œ ×”×’×™×•× ×™ - ×¨×§ ××¢×œ 80
    if FIELDS_DICT["age"] in validated:
        try:
            age = int(validated[FIELDS_DICT["age"]])
            if age > 80:
                logging.warning(f"âš ï¸ ×’×™×œ {age} ××¢×œ 80, ××¡×™×¨ ××”× ×ª×•× ×™×")
                del validated[FIELDS_DICT["age"]]
            else:
                validated[FIELDS_DICT["age"]] = age
        except (ValueError, TypeError):
            logging.warning(f"âš ï¸ ×’×™×œ ×œ× ×ª×§×™×Ÿ: {validated[FIELDS_DICT['age']]}, ××¡×™×¨ ××”× ×ª×•× ×™×")
            del validated[FIELDS_DICT["age"]]
    
    # ×”×’×‘×œ×ª ××•×¨×š ×©×“×•×ª ×œ×—×¡×›×•×Ÿ ×‘×˜×•×§× ×™×
    for field, value in list(validated.items()):
        if isinstance(value, str):
            if len(value) > 100:
                logging.warning(f"âš ï¸ ×©×“×” {field} ××¨×•×š ××“×™ ({len(value)} ×ª×•×•×™×), ××§×¦×¨")
                validated[field] = value[:97] + "..."
            elif len(value.strip()) == 0:
                logging.warning(f"âš ï¸ ×©×“×” {field} ×¨×™×§, ××¡×™×¨")
                del validated[field]
    
    return validated


# ============================×”×’'×™×¤×™×˜×™ ×”-4 - ××™×–×•×’ ×—×›× ×©×œ ××™×“×¢ ×¨×’×™×© ======================= 

def merge_sensitive_profile_data(existing_profile, new_data, user_message):
    """
    ×××–×’ ×ª×¢×•×“×ª ×–×”×•×ª ×¨×’×©×™×ª ×§×™×™××ª ×¢× ××™×“×¢ ×—×“×©, ×œ×¤×™ ×›×œ×œ×™× ×¨×’×™×©×™× (××™ ×™×•×“×¢, ×˜×¨××•××•×ª ×•×›×•').
    ×§×œ×˜: existing_profile (dict), new_data (dict), user_message (str)
    ×¤×œ×˜: dict ×××•×–×’, usage (dict)
    # ××”×œ×š ××¢× ×™×™×Ÿ: ××™×–×•×’ ×—×›× ×©×œ ×˜×¨××•××•×ª, ××™ ×™×•×“×¢/×œ× ×™×•×“×¢, ×¢×“×›×•×Ÿ summary.
    """
    # ×©×“×•×ª ×©×¦×¨×™×›×™× ××™×–×•×’ ××•×¨×›×‘
    complex_fields = [
        FIELDS_DICT["attracted_to"], FIELDS_DICT["who_knows"], FIELDS_DICT["who_doesnt_know"], FIELDS_DICT["attends_therapy"], 
        FIELDS_DICT["primary_conflict"], FIELDS_DICT["trauma_history"], FIELDS_DICT["goal_in_course"], 
        FIELDS_DICT["language_of_strength"], FIELDS_DICT["coping_strategies"], FIELDS_DICT["fears_concerns"], FIELDS_DICT["future_vision"]
    ]
    
    # ×‘×“×™×§×” ×× ×‘×××ª ×¦×¨×™×š GPT4
    needs_merge = False
    for field in complex_fields:
        if field in new_data:
            existing_value = existing_profile.get(field, "")
            if existing_value and existing_value.strip():
                needs_merge = True
                break
    
    if not needs_merge:
        logging.info("ğŸ”„ ×œ× × ×“×¨×© ××™×–×•×’ ××•×¨×›×‘, ××—×–×™×¨ ×¢×“×›×•×Ÿ ×¨×’×™×œ")
        return {**existing_profile, **new_data}

    system_prompt = SENSITIVE_PROFILE_MERGE_PROMPT  # ×¤×¨×•××˜ ××™×–×•×’ ×¨×’×™×©

    usage_data = {
        "prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0,
        "cached_tokens": 0, "cost_prompt_regular": 0, "cost_prompt_cached": 0,
        "cost_completion": 0, "cost_total": 0, "cost_total_ils": 0, "model": ""
    }

    try:
        # ×”×›× ×ª ×”××™×“×¢ ×œ××™×–×•×’
        merge_request = {
            "existing_profile": existing_profile,
            "new_data": new_data,
            "user_message": user_message
        }
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"××™×“×¢ ×§×™×™×:\n{json.dumps(existing_profile, ensure_ascii=False, indent=2)}\n\n××™×“×¢ ×—×“×©:\n{json.dumps(new_data, ensure_ascii=False, indent=2)}\n\n×”×•×“×¢×” ××§×•×¨×™×ª:\n{user_message}"}
        ]

        response = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=messages,
            temperature=0,  # ×“×™×•×§ ××§×¡×™××œ×™ ×œ××™×“×¢ ×¨×’×™×©
            max_tokens=400   # ××¡×¤×™×§ ×œ×›×œ ×”×©×“×•×ª + summary
        )

        content = response.choices[0].message.content.strip()

        # --- DEBUG: Print all usage fields from API ---
        try:
            def _to_serializable(val):
                if hasattr(val, '__dict__'):
                    return {k: _to_serializable(v) for k, v in vars(val).items()}
                elif isinstance(val, (list, tuple)):
                    return [_to_serializable(x) for x in val]
                elif isinstance(val, dict):
                    return {k: _to_serializable(v) for k, v in val.items()}
                else:
                    try:
                        json.dumps(val)
                        return val
                    except Exception:
                        return str(val)
            usage_dict = {}
            for k in dir(response.usage):
                if not k.startswith("_") and not callable(getattr(response.usage, k)):
                    v = getattr(response.usage, k)
                    usage_dict[k] = _to_serializable(v)
            print(f"[DEBUG] API usage raw: {json.dumps(usage_dict, ensure_ascii=False)}")
        except Exception as e:
            print(f"[DEBUG] Failed to print API usage fields: {e}")

        # ×—×™×©×•×‘×™ ×¢×œ×•×ª ×“×™× ×××™ ×œ×¤×™ ×”××•×“×œ
        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = response.usage.prompt_tokens_details
        cached_tokens = prompt_tokens_details.cached_tokens
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens
        model_name = response.model
        cost_data = calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens, model_name)
        usage_data = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,
            **cost_data,
            "model": response.model
        }

        logging.info(f"ğŸ¤– GPT4 ××™×–×•×’ ×”×—×–×™×¨: '{content[:200]}...'")
        write_gpt_log("sensitive_merge", usage_data, response.model)

        # ×¤×¨×¡×•×¨ ×”×ª×©×•×‘×”
        if not content.startswith("{"):
            if "{" in content:
                start = content.find("{")
                end = content.rfind("}") + 1
                content = content[start:end]

        merged_profile = json.loads(content)
        
        # validation ×¢×œ ×”×ª×•×¦××” ×”×¡×•×¤×™×ª
        validated_profile = validate_extracted_data(merged_profile)
        
        logging.info(f"âœ… GPT4 ×¢×“×›×Ÿ ×ª.×– ×¢× {len(validated_profile)} ×©×“×•×ª")
        if validated_profile != merged_profile:
            logging.info(f"ğŸ”§ ×œ××—×¨ validation: ×”×•×¡×¨×•/×ª×•×§× ×• ×©×“×•×ª")

        return {
            "prompt_tokens": prompt_tokens,
            "cached_tokens": cached_tokens,
            "prompt_regular": prompt_regular,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            **cost_data,
            "model": usage_data.get("model", "")
        }, validated_profile

    except json.JSONDecodeError as e:
        logging.error(f"âŒ ×©×’×™××” ×‘×¤×¨×¡×•×¨ JSON ×‘××™×–×•×’ GPT4: {e}")
        logging.error(f"ğŸ“„ ×”×ª×•×›×Ÿ: '{content}'")
        
        # fallback - ××™×–×•×’ ×¤×©×•×˜ ×‘××§×¨×” ×©×œ ×›×©×œ
        fallback_merge = {**existing_profile, **new_data}
        logging.warning("ğŸ”§ ××©×ª××© ×‘××™×–×•×’ fallback ×¤×©×•×˜")
        
        return {
            "prompt_tokens": 0,
            "cached_tokens": 0,
            "prompt_regular": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "cost_prompt_regular": 0.0,
            "cost_prompt_cached": 0.0,
            "cost_completion": 0.0,
            "cost_total": 0.0,
            "cost_total_ils": 0.0,
            "cost_agorot": 0,
            "model": "fallback"
        }, fallback_merge

    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×›×œ×œ×™×ª ×‘-GPT4 ××™×–×•×’: {e}")
        
        # fallback - ××™×–×•×’ ×¤×©×•×˜ ×‘××§×¨×” ×©×œ ×›×©×œ
        fallback_merge = {**existing_profile, **new_data}
        
        return {
            "prompt_tokens": 0,
            "cached_tokens": 0,
            "prompt_regular": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "cost_prompt_regular": 0.0,
            "cost_prompt_cached": 0.0,
            "cost_completion": 0.0,
            "cost_total": 0.0,
            "cost_total_ils": 0.0,
            "cost_agorot": 0,
            "model": "error"
        }, fallback_merge


# ×¤×•× ×§×¦×™×™×ª ×¢×–×¨ - ×§×•×‘×¢×ª ×× ×œ×”×¤×¢×™×œ GPT4
def should_use_gpt4_merge(existing_profile, new_data):
    """
    ××—×œ×™×˜ ×”×× ×œ×”×¤×¢×™×œ ××™×–×•×’ ×—×›× (GPT4) ×œ×¤×™ ×¡×•×’ ×”×©×™× ×•×™ ×‘×¤×¨×•×¤×™×œ.
    ×§×œ×˜: existing_profile, new_data
    ×¤×œ×˜: True/False
    """
    complex_fields = [
        FIELDS_DICT["attracted_to"], FIELDS_DICT["who_knows"], FIELDS_DICT["who_doesnt_know"], FIELDS_DICT["attends_therapy"], 
        FIELDS_DICT["primary_conflict"], FIELDS_DICT["trauma_history"], FIELDS_DICT["goal_in_course"], 
        FIELDS_DICT["language_of_strength"], FIELDS_DICT["coping_strategies"], FIELDS_DICT["fears_concerns"], FIELDS_DICT["future_vision"]
    ]
    print("[DEBUG][should_use_gpt4_merge] complex_fields:")
    for idx, field in enumerate(complex_fields):
        print(f"[DEBUG][should_use_gpt4_merge] complex_fields[{idx}] = {field} (type: {type(field)})")
        if not isinstance(field, str):
            print(f"[DEBUG][should_use_gpt4_merge][ALERT] complex_fields[{idx}] ×”×•× {type(field)}! ×¢×¨×š: {field}")
            continue  # ×“×œ×’ ×¢×œ ×©×“×•×ª ×œ× ×ª×§×™× ×™×
        if field in new_data:  # GPT3 ××¦× ×©×“×” ××•×¨×›×‘ ×—×“×©
            existing_value = existing_profile.get(field, "")
            print(f"[DEBUG][should_use_gpt4_merge] ×‘×“×™×§×”: field='{field}', existing_value='{existing_value}'")
            if existing_value and isinstance(existing_value, str) and existing_value.strip():  # ×•×”×©×“×” ×§×™×™× ×‘×ª.×–
                logging.info(f"ğŸ¯ GPT4 × ×“×¨×© - ×©×“×” '{field}' ××¦×¨×™×š ××™×–×•×’")
                print(f"[DEBUG][should_use_gpt4_merge] × ××¦× ×©×“×” ××•×¨×›×‘ ×—×“×©: {field}")
                return True
    print("[DEBUG][should_use_gpt4_merge] ××™×Ÿ ×¦×•×¨×š ×‘-GPT4 - ×¢×“×›×•×Ÿ ×¤×©×•×˜ ××¡×¤×™×§")
    logging.info("âœ… ××™×Ÿ ×¦×•×¨×š ×‘-GPT4 - ×¢×“×›×•×Ÿ ×¤×©×•×˜ ××¡×¤×™×§")
    return False


# ============================×¤×•× ×§×¦×™×” ×©××¤×¢×™×œ×” ××ª ×”×’'×™×¤×™×˜×™ ×”×¨×‘×™×¢×™ ×œ×¤×™ ×”×™×’×™×•×Ÿ -×œ× ×¤×•×¢×œ ×ª××™×“ - ×¢×“×›×•×Ÿ ×—×›× ×©×œ ×ª.×– ×”×¨×’×©×™×ª ======================= 

def smart_update_profile(existing_profile, user_message):
    """
    ××¢×“×›×Ÿ ×ª×¢×•×“×ª ×–×”×•×ª ×¨×’×©×™×ª ×©×œ ××©×ª××©, ×›×•×œ×œ ××™×–×•×’ ×—×›× ×‘××™×“×ª ×”×¦×•×¨×š.
    ×§×œ×˜: existing_profile (dict), user_message (str)
    ×¤×œ×˜: dict ×××•×–×’, usage (dict)
    # ××”×œ×š ××¢× ×™×™×Ÿ: ×‘×•×—×¨ ××•×˜×•××˜×™×ª ×”×× ×œ×”×¤×¢×™×œ ××™×–×•×’ ×—×›× ××• ×¨×’×™×œ.
    """
    print("[DEBUG][smart_update_profile] CALLED")
    try:
        logging.info("ğŸ”„ ××ª×—×™×œ ×¢×“×›×•×Ÿ ×—×›× ×©×œ ×ª.×– ×”×¨×’×©×™×ª")
        print(f"[DEBUG][smart_update_profile] --- START ---")
        print(f"[DEBUG][smart_update_profile] existing_profile: {existing_profile} (type: {type(existing_profile)})")
        # ×©×œ×‘ 1: GPT3 - ×—×™×œ×•×¥ ××™×“×¢ ×—×“×©
        new_data, extract_usage = extract_user_profile_fields(user_message)
        print(f"[DEBUG][smart_update_profile] new_data: {new_data} (type: {type(new_data)})")
        print(f"[DEBUG][smart_update_profile] extract_usage: {extract_usage} (type: {type(extract_usage)})")
        # ×”×’× ×”: ×•×“× ×©-new_data ×”×•× dict ×¢× ××¤×ª×—×•×ª str ×‘×œ×‘×“
        if not isinstance(new_data, dict) or not all(isinstance(k, str) for k in new_data.keys()):
            logging.error(f"âš ï¸ new_data ×œ× ×ª×§×™×Ÿ (×œ×¤× ×™ ××™×–×•×’): {new_data}")
            print(f"[ALERT][smart_update_profile] new_data ×œ× ×ª×§×™×Ÿ (×œ×¤× ×™ ××™×–×•×’): {new_data}")
            new_data = {}
        logging.info(f"ğŸ¤– GPT3 ×—×™×œ×¥: {list(new_data.keys())}")
        print(f"[DEBUG][smart_update_profile] new_data keys: {list(new_data.keys())}")
        # ×× ××™×Ÿ ××™×“×¢ ×—×“×© - ××™×Ÿ ××” ×œ×¢×“×›×Ÿ
        if not new_data:
            logging.info("â„¹ï¸ ××™×Ÿ ××™×“×¢ ×—×“×©, ××—×–×™×¨ ×ª.×– ×œ×œ× ×©×™× ×•×™")
            print("[DEBUG][smart_update_profile] ××™×Ÿ ××™×“×¢ ×—×“×©, ××—×–×™×¨ ×ª.×– ×œ×œ× ×©×™× ×•×™")
            return existing_profile, extract_usage, None
        # ×©×œ×‘ 2: ×‘×“×™×§×” ×× ×¦×¨×™×š GPT4
        print(f"[DEBUG][smart_update_profile] ×§×•×¨× ×œ-should_use_gpt4_merge ×¢× existing_profile: {existing_profile}, new_data: {new_data}")
        if should_use_gpt4_merge(existing_profile, new_data):
            logging.info("ğŸ¯ ××¤×¢×™×œ GPT4 ×œ××™×–×•×’ ××•×¨×›×‘")
            print("[DEBUG][smart_update_profile] ××¤×¢×™×œ GPT4 ×œ××™×–×•×’ ××•×¨×›×‘!")
            # ×©×œ×‘ 3: GPT4 - ××™×–×•×’ ×—×›×
            print(f"[DEBUG][smart_update_profile] ×œ×¤× ×™ merge_sensitive_profile_data: existing_profile={existing_profile}, new_data={new_data}, user_message={user_message}")
            merge_usage, updated_profile = merge_sensitive_profile_data(existing_profile, new_data, user_message)
            print(f"[DEBUG][smart_update_profile] ××—×¨×™ merge_sensitive_profile_data: updated_profile={updated_profile}, merge_usage={merge_usage}")
            logging.info(f"âœ… GPT4 ×¢×“×›×Ÿ ×ª.×– ×¢× {len(updated_profile)} ×©×“×•×ª")
            print(f"[DEBUG][smart_update_profile] merge_usage: {merge_usage}")
            print(f"[DEBUG][smart_update_profile] updated_profile: {updated_profile}")
            # print diff
            if existing_profile != updated_profile:
                diff_keys = set(updated_profile.keys()) - set(existing_profile.keys())
                print(f"[DEBUG][smart_update_profile] profile diff (new keys): {diff_keys}")
            else:
                print(f"[DEBUG][smart_update_profile] profile unchanged after merge")
            print(f"[DEBUG][smart_update_profile] returning: profile_updated={updated_profile}, extract_usage={extract_usage}")
            return updated_profile, extract_usage, merge_usage
        else:
            logging.info("âœ… ×¢×“×›×•×Ÿ ×¤×©×•×˜ ×œ×œ× GPT4")
            print("[DEBUG][smart_update_profile] ×¢×“×›×•×Ÿ ×¤×©×•×˜ ×œ×œ× GPT4")
            # ×¢×“×›×•×Ÿ ×¤×©×•×˜ - ××™×–×•×’ ×¨×’×™×œ
            updated_profile = {**existing_profile, **new_data}
            print(f"[DEBUG][smart_update_profile] updated_profile: {updated_profile}")
            if existing_profile != updated_profile:
                diff_keys = set(updated_profile.keys()) - set(existing_profile.keys())
                print(f"[DEBUG][smart_update_profile] profile diff (new keys): {diff_keys}")
            else:
                print(f"[DEBUG][smart_update_profile] profile unchanged after simple merge")
            print(f"[DEBUG][smart_update_profile] returning: profile_updated={updated_profile}, extract_usage={extract_usage}")
            return updated_profile, extract_usage, None
    except Exception as e:
        import traceback
        print(f"[ERROR][smart_update_profile] Exception: {e}")
        print(traceback.format_exc())
        return existing_profile, {}, None

def smart_update_profile_async(*args, **kwargs):
    loop = asyncio.get_event_loop()
    return loop.run_in_executor(None, smart_update_profile, *args, **kwargs)

def get_combined_usage_data(extract_usage, merge_usage=None):
    """
    ×××—×“ usage ×©×œ ×—×™×œ×•×¥ ×•××™×–×•×’ (×× ×™×©), ××—×–×™×¨ usage ×›×•×œ×œ ×œ×›×œ ×”×ª×”×œ×™×š.
    ×§×œ×˜: extract_usage (dict), merge_usage (dict ××• None)
    ×¤×œ×˜: dict usage ×›×•×œ×œ
    """
    # × ×ª×•× ×™ GPT3
    if not isinstance(extract_usage, dict):
        raise ValueError("extract_usage ×—×™×™×‘ ×œ×”×™×•×ª dict!")
    extract_data = extract_usage.copy()
    
    # ×× GPT4 ×¨×¥ - ×”×•×¡×£ ××ª ×”× ×ª×•× ×™× ×©×œ×•
    if merge_usage:
        if not isinstance(merge_usage, dict):
            raise ValueError("merge_usage ×—×™×™×‘ ×œ×”×™×•×ª dict!")
        merge_data = merge_usage.copy()
        return {**extract_data, **merge_data}
    else:
        return extract_data

def extract_user_profile_fields(text, system_prompt=None, client=None):
    """
    ×©×•×œ×—×ª ××ª ×”×˜×§×¡×˜ ×œ-GPT (identity_extraction) ×•××—×–×™×¨×” dict ×¢× ×©×“×•×ª ××™×“×¢ ××™×©×™ (×’×™×œ, ×“×ª, ×¢×™×¡×•×§ ×•×›×•').
    ×§×œ×˜: text (×˜×§×¡×˜ ×—×•×¤×©×™ ××”××©×ª××©), system_prompt (×¤×¨×•××˜ ×™×™×¢×•×“×™, ×‘×¨×™×¨×ª ××—×“×œ: PROFILE_EXTRACTION_PROMPT), client (××•×¤×¦×™×•× ×œ×™).
    ×¤×œ×˜: (new_data: dict, usage_data: dict)
    # ××”×œ×š ××¢× ×™×™×Ÿ: × ×™×§×•×™ ××•×˜×•××˜×™ ×©×œ ×‘×œ×•×§ ```json ... ``` ××”×ª×©×•×‘×” ×©×œ GPT, ×‘×“×™×§×ª ×ª×§×™× ×•×ª, ×•×œ×•×’×™× ××¤×•×¨×˜×™×.
    """
    print("[DEBUG][extract_user_profile_fields] CALLED")
    if system_prompt is None:
        system_prompt = PROFILE_EXTRACTION_PROMPT  # ×¤×¨×•××˜ ×—×™×œ×•×¥ ×ª×¢×•×“×ª ×–×”×•×ª
    if client is None:
        from gpt_handler import client
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text}
            ],
            temperature=0,
            max_tokens=200
        )
        content = response.choices[0].message.content.strip()
        print(f"[DEBUG][extract_user_profile_fields] raw GPT content: {content}")
        # --- × ×™×§×•×™ ×‘×œ×•×§ ```json ... ``` ×× ×§×™×™× ---
        if content.startswith("```"):
            match = re.search(r"```(?:json)?\\s*({.*?})\\s*```", content, re.DOTALL)
            if match:
                logging.debug(f"[DEBUG][extract_user_profile_fields] found ```json block, extracting only JSON...")
                content = match.group(1)
                print(f"[DEBUG][extract_user_profile_fields] cleaned content: {content}")
        logging.info(f"[DEBUG] GPT3 identity_extraction raw: '{content}'")
        try:
            new_data = json.loads(content)
            print(f"[DEBUG][extract_user_profile_fields] after json.loads: {new_data} (type: {type(new_data)})")
            if isinstance(new_data, dict):
                print(f"[DEBUG][extract_user_profile_fields] new_data keys: {list(new_data.keys())}")
                if not new_data:
                    print("[ALERT][extract_user_profile_fields] new_data is an EMPTY dict!")
            else:
                print("[ALERT][extract_user_profile_fields] new_data is NOT a dict!")
        except Exception as e:
            import traceback
            print(f"[ERROR][extract_user_profile_fields] Exception: {e}")
            print(traceback.format_exc())
            new_data = {}
        # --- usage/cost ---
        prompt_tokens = response.usage.prompt_tokens
        prompt_tokens_details = getattr(response.usage, 'prompt_tokens_details', None)
        cached_tokens = getattr(prompt_tokens_details, 'cached_tokens', 0) if prompt_tokens_details else 0
        prompt_regular = prompt_tokens - cached_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens
        model_name = response.model
        # ×—×™×©×•×‘ ×¢×œ×•×ª ×“×™× ×××™ ×œ×¤×™ ×”××•×“×œ
        cost_data = calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens, model_name)
        usage_data = {
            'prompt_tokens': prompt_tokens,
            'completion_tokens': completion_tokens,
            'total_tokens': total_tokens,
            'cached_tokens': cached_tokens,
            **cost_data,
            'model': response.model
        }
        print(f"[DEBUG][extract_user_profile_fields] returning new_data: {new_data}")
        return new_data, usage_data
    except Exception as critical_error:
        logging.error(f"âŒ ×©×’×™××” ×§×¨×™×˜×™×ª ×‘-extract_user_profile_fields: {critical_error}")
        return {}, {}

# ===================== ×‘×“×™×§×ª ×‘×¨×™××•×ª ×œ××—×™×¨×•×Ÿ =====================
def check_gpt_pricing_health(required_models=None):
    """
    ×‘×•×“×§×ª ×©×”××—×™×¨×•×Ÿ × ×˜×¢×Ÿ ×›×¨××•×™ ×•×›×•×œ×œ ××ª ×›×œ ×”××•×“×œ×™× ×”×§×¨×™×˜×™×™×.
    ×§×œ×˜: ×¨×©×™××ª ×©××•×ª ××•×“×œ×™× (××• None â€“ ×‘×“×™×§×” ×‘×¡×™×¡×™×ª ×‘×œ×‘×“)
    ×¤×œ×˜: dict ×¢× ×¡×˜×˜×•×¡, ×©×’×™××•×ª, ×•××•×“×œ×™× ×—×¡×¨×™×
    """
    health = {"loaded": False, "missing_models": [], "error": None}
    try:
        if not GPT_PRICING or not isinstance(GPT_PRICING, dict):
            health["error"] = "××—×™×¨×•×Ÿ ×œ× × ×˜×¢×Ÿ ××• ×œ× ×ª×§×™×Ÿ!"
            return health
        health["loaded"] = True
        if required_models:
            for model in required_models:
                if model not in GPT_PRICING:
                    health["missing_models"].append(model)
    except Exception as e:
        health["error"] = str(e)
    return health

