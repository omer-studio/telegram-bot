"""
gpt_handler.py
--------------
×§×•×‘×¥ ×–×” ××¨×›×– ××ª ×›×œ ×”×¤×•× ×§×¦×™×•×ª ×©××‘×¦×¢×•×ª ××™× ×˜×¨××§×¦×™×” ×¢× gpt (×©×œ×™×—×ª ×”×•×“×¢×•×ª, ×—×™×©×•×‘ ×¢×œ×•×ª, ×“×™×‘××’×™× ×’).
×”×¨×¦×™×•× ×œ: ×¨×™×›×•×– ×›×œ ×”×œ×•×’×™×§×” ×©×œ gpt ×‘××§×•× ××—×“, ×›×•×œ×œ ×ª×™×¢×•×“ ××œ× ×©×œ ×˜×•×§× ×™×, ×¢×œ×•×™×•×ª, ×•×œ×•×’×™×.

ğŸ”„ ×¢×“×›×•×Ÿ: ××¢×‘×¨ ×œ-LiteLLM ×¢× ××¢×§×‘ ×¢×œ×•×™×•×ª ××•×‘× ×”
--------------------------------------------------
- LiteLLM ××¡×¤×§ ××¢×§×‘ ×¢×œ×•×™×•×ª ××•×˜×•××˜×™ ×•××“×•×™×§
- ××™×Ÿ ×¦×•×¨×š ×‘×§×•×‘×¥ ××—×™×¨×•×Ÿ ×—×™×¦×•× ×™
- ×¢×œ×•×™×•×ª ××—×•×©×‘×•×ª ××•×˜×•××˜×™×ª ×œ×¤×™ ×”××•×“×œ ×•×”×˜×•×§× ×™×
- ×ª××™×›×” ×‘××•×“×œ×™× ××¨×•×‘×™× ×¢× ×¢×œ×•×™×•×ª ×©×•× ×•×ª
"""

import json
import logging
import os
import asyncio
import re
import threading
from datetime import datetime
from config import gpt_log_path
from fields_dict import FIELDS_DICT
from prompts import BOT_REPLY_SUMMARY_PROMPT, PROFILE_EXTRACTION_ENHANCED_PROMPT
from gpt_c_logger import append_gpt_c_html_update

# ×§×‘×•×¢×™×
USD_TO_ILS = 3.7  # ×©×¢×¨ ×”×“×•×œ×¨-×©×§×œ (×™×© ×œ×¢×“×›×Ÿ ×œ×¤×™ ×”×¦×•×¨×š)

# ×”×’×“×¨×ª × ×ª×™×‘ ×œ×•×’ ××—×™×“ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
os.makedirs(PROJECT_ROOT, exist_ok=True)

# ===================== ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×œ×œ×•×’×™× ×•×“×™×‘××’ =====================

def _debug_gpt_usage(model_name, prompt_tokens, completion_tokens, cached_tokens, total_tokens, call_type):
    """
    ×”×“×¤×¡×ª debug info ×¢×œ usage ×©×œ gpt.
    """
    print(f"[DEBUG] {call_type} - Model: {model_name}, Tokens: {prompt_tokens}p + {completion_tokens}c + {cached_tokens}cache = {total_tokens}total")

def write_gpt_log(call_type, usage_log, model_name, interaction_id=None):
    """
    ×›×•×ª×‘ ×œ×•×’ ×©×œ ×§×¨×™××ª gpt ×œ×§×•×‘×¥ JSON.
    ×§×œ×˜: call_type (main_reply/summary/identity_extraction), usage_log (dict), model_name (str), interaction_id (str, optional)
    """
    try:
        timestamp = datetime.now().isoformat()
        log_entry = {
            "timestamp": timestamp,
            "type": call_type,
            "model": model_name,
            **usage_log
        }
        if interaction_id:
            log_entry["interaction_id"] = str(interaction_id)
        
        # ×•×™×“×•× ×©×”×ª×™×§×™×™×” ×§×™×™××ª
        os.makedirs(os.path.dirname(gpt_log_path), exist_ok=True)
        
        with open(gpt_log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
            
    except Exception as e:
        logging.error(f"×©×’×™××” ×‘×›×ª×™×‘×ª ×œ×•×’ gpt: {e}")

# ğŸ”„ ×¢×“×›×•×Ÿ: ×¤×•× ×§×¦×™×” ×—×“×©×” ×œ×—×™×©×•×‘ ×¢×œ×•×™×•×ª ×¢× LiteLLM
def calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens=0, model_name='gpt-4o', usd_to_ils=USD_TO_ILS, completion_response=None):
    """
    ××—×©×‘ ××ª ×”×¢×œ×•×ª ×©×œ ×©×™××•×© ×‘-gpt ×œ×¤×™ ××¡×¤×¨ ×”×˜×•×§× ×™× ×•×”××•×“×œ.
    ××©×ª××© ××š ×•×¨×§ ×‘-LiteLLM ×¢× completion_response.
    ××—×–×™×¨ ×¨×§ ××ª ×”×¢×œ×•×ª ×”×›×•×œ×œ×ª (cost_total) ×›×¤×™ ×©××—×•×©×‘ ×¢"×™ LiteLLM, ×‘×œ×™ ×¤×™×œ×•×— ×™×“× ×™.
    """
    print(f"[DEBUG] ğŸ”¥ calculate_gpt_cost CALLED! ğŸ”¥")
    print(f"[DEBUG] Input: prompt_tokens={prompt_tokens}, completion_tokens={completion_tokens}, cached_tokens={cached_tokens}, model_name={model_name}")
    print(f"[DEBUG] calculate_gpt_cost - Model: {model_name}, Tokens: {prompt_tokens}p + {completion_tokens}c + {cached_tokens}cache")
    try:
        import litellm
        if completion_response:
            print(f"[DEBUG] Using completion_response for cost calculation")
            cost_usd = litellm.completion_cost(completion_response=completion_response)
            print(f"[DEBUG] LiteLLM completion_cost returned: {cost_usd}")
        else:
            print(f"[DEBUG] No completion_response provided, cannot calculate cost with LiteLLM")
            cost_usd = 0.0
        cost_ils = cost_usd * usd_to_ils
        cost_agorot = cost_ils * 100
        result = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
            "cached_tokens": cached_tokens,
            "cost_total": cost_usd,
            "cost_total_ils": cost_ils,
            "cost_agorot": cost_agorot,
            "model": model_name
        }
        print(f"[DEBUG] calculate_gpt_cost returning: {result}")
        return result
    except Exception as e:
        print(f"[ERROR] calculate_gpt_cost failed: {e}")
        import traceback
        print(f"[ERROR] Full traceback: {traceback.format_exc()}")
        return {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
            "cached_tokens": cached_tokens,
            "cost_total": 0.0,
            "cost_total_ils": 0.0,
            "cost_agorot": 0.0,
            "model": model_name
        }

# ============================ ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×œ×‘×“×™×§×ª ×”×¦×•×¨×š ×‘-gpt_c ============================

def should_run_gpt_c(user_message):
    """
    ×‘×•×“×§ ×× ×™×© ×˜×¢× ×œ×”×¤×¢×™×œ gpt_c ×¢×œ ×”×•×“×¢×” × ×ª×•× ×”.
    ××—×–×™×¨ False ×¨×§ ×¢×œ ×”×•×“×¢×•×ª ×©×× ×—× ×• ×‘×˜×•×—×™× ×©×œ× ××›×™×œ×•×ª ××™×“×¢ ×—×“×©.
    ×”×›×œ×œ: gpt_c ××•×¤×¢×œ ×ª××™×“, ××œ× ×× ×›×Ÿ ×”×”×•×“×¢×” ×”×™× ××©×”×• ×©×œ× ×™×›×•×œ ×œ×”×›×™×œ ××™×“×¢ ×—×“×©.
    """
    if not user_message or not user_message.strip():
        return False
    
    message = user_message.strip()
    
    # ×‘×™×˜×•×™×™× ×‘×¡×™×¡×™×™× ×©×œ× ×™×›×•×œ×™× ×œ×”×›×™×œ ××™×“×¢ ×—×“×©
    base_phrases = [
        '×”×™×™', '×©×œ×•×', '××” ×©×œ×•××š', '××” × ×©××¢', '××” ×§×•×¨×”', '××” ×”××¦×‘',
        '×ª×•×“×”', '×ª×•×“×” ×¨×‘×”', '×ª×•×“×” ×œ×š', '×ª×•×“×” ×××•×“', '×ª×•×“×” ×¢× ×§×™×ª', '×ª×•×“×”×”',
        '×‘×¡×“×¨', '××•×§×™×™', '××•×§×™', '×‘×¡×“×¨ ×’××•×¨', '×‘×¡×“×¨ ××•×©×œ×', '××•×§×™×™×™',
        '×× ×™ ××‘×™×Ÿ', '××”', '×•×•××•', '××¢× ×™×™×Ÿ', '× ×›×•×Ÿ', '××›×Ÿ', '××” ××”',
        '×›×Ÿ', '×œ×', '××•×œ×™', '×™×›×•×œ ×œ×”×™×•×ª', '××¤×©×¨×™',
        '×× ×™ ×œ× ×™×•×“×¢', '×œ× ×™×•×“×¢', '×œ× ×‘×˜×•×—', '×œ× ×™×•×“×¢ ××” ×œ×”×’×™×“', '××™×Ÿ ×œ×™ ××•×©×’',
        '×‘×”×—×œ×˜', '×‘×˜×—', '×›××•×‘×Ÿ', '×‘×¨×•×¨', '×•×“××™', '×‘×•×•×“××™',
        '××¢×•×œ×”', '× ×”×“×¨', '××“×”×™×', '×¤× ×˜×¡×˜×™', '××•×©×œ×',
        '××” ××•×§×™×™', '××” ×‘×¡×“×¨', '××” ×”×‘× ×ª×™', '××” × ×›×•×Ÿ',
        '×›×Ÿ ×œ×', '×›×Ÿ ××•×œ×™', '××•×œ×™ ×›×Ÿ', '××•×œ×™ ×œ×',
        '××” ××•×§×™×™ ×ª×•×“×”', '××” ×‘×¡×“×¨ ×ª×•×“×”', '××” ×”×‘× ×ª×™ ×ª×•×“×”',
        '×˜×•×‘', '×˜×•×‘ ×××•×“', '×˜×•×‘ ×××“', '×œ× ×¨×¢', '×œ× ×¨×¢ ×‘×›×œ×œ',
        '×‘×¡×“×¨ ×’××•×¨', '×‘×¡×“×¨ ××•×©×œ×', '×‘×¡×“×¨ ×œ×’××¨×™', '×‘×¡×“×¨ ×œ×—×œ×•×˜×™×Ÿ',
        '××¦×•×™×Ÿ', '××¦×•×™×™×Ÿ', '××¢×•×œ×”', '× ×”×“×¨', '××“×”×™×', '×¤× ×˜×¡×˜×™',
        '×× ×™ ×‘×¡×“×¨', '×× ×™ ×˜×•×‘', '×× ×™ ××¦×•×™×Ÿ', '×× ×™ ××¢×•×œ×”',
        '×”×›×œ ×˜×•×‘', '×”×›×œ ×‘×¡×“×¨', '×”×›×œ ××¦×•×™×Ÿ', '×”×›×œ ××¢×•×œ×”',
        '×¡×‘×‘×”', '×¡×‘×‘×” ×’××•×¨×”', '×¡×‘×‘×” ××•×©×œ××ª',
        '×§×•×œ', '×§×•×œ ×œ×’××¨×™', '×§×•×œ ×œ×—×œ×•×˜×™×Ÿ',
        '××—×œ×”', '××—×œ×” ×’××•×¨×”', '××—×œ×” ××•×©×œ××ª',
        '×™×•×¤×™', '×™×•×¤×™ ×’××•×¨', '×™×•×¤×™ ××•×©×œ×',
        '××¢×•×œ×”', '××¢×•×œ×” ×’××•×¨×”', '××¢×•×œ×” ××•×©×œ××ª',
        '× ×”×“×¨', '× ×”×“×¨ ×œ×’××¨×™', '× ×”×“×¨ ×œ×—×œ×•×˜×™×Ÿ',
        '××“×”×™×', '××“×”×™× ×œ×’××¨×™', '××“×”×™× ×œ×—×œ×•×˜×™×Ÿ',
        '×¤× ×˜×¡×˜×™', '×¤× ×˜×¡×˜×™ ×œ×’××¨×™', '×¤× ×˜×¡×˜×™ ×œ×—×œ×•×˜×™×Ÿ',
        '××•×©×œ×', '××•×©×œ× ×œ×’××¨×™', '××•×©×œ× ×œ×—×œ×•×˜×™×Ÿ',
        '×× ×™ ××•×§×™×™', '×× ×™ ×‘×¡×“×¨ ×’××•×¨', '×× ×™ ×‘×¡×“×¨ ××•×©×œ×',
        '×× ×™ ×˜×•×‘ ×××•×“', '×× ×™ ×˜×•×‘ ×××“', '×× ×™ ×˜×•×‘ ×œ×’××¨×™',
        '×× ×™ ××¦×•×™×Ÿ ×œ×’××¨×™', '×× ×™ ××¢×•×œ×” ×œ×’××¨×™', '×× ×™ × ×”×“×¨ ×œ×’××¨×™',
        '×× ×™ ××“×”×™× ×œ×’××¨×™', '×× ×™ ×¤× ×˜×¡×˜×™ ×œ×’××¨×™', '×× ×™ ××•×©×œ× ×œ×’××¨×™',
        '×˜×•×‘ ××—×™', '×‘×¡×“×¨ ××—×™', '××¢×•×œ×” ××—×™', '× ×”×“×¨ ××—×™', '××“×”×™× ××—×™',
        '×¡×‘×‘×” ××—×™', '×§×•×œ ××—×™', '××—×œ×” ××—×™', '×™×•×¤×™ ××—×™', '××•×©×œ× ××—×™',
        '×× ×™ ×‘×¡×“×¨ ××—×™', '×× ×™ ×˜×•×‘ ××—×™', '×× ×™ ××¢×•×œ×” ××—×™', '×× ×™ × ×”×“×¨ ××—×™',
        '×”×›×œ ×˜×•×‘ ××—×™', '×”×›×œ ×‘×¡×“×¨ ××—×™', '×”×›×œ ××¢×•×œ×” ××—×™',
        '××—×™', '××—', '××—×™×™', '××—×™×™×™', '××—×™×™×™×™',
        '××—×™ ×˜×•×‘', '××—×™ ×‘×¡×“×¨', '××—×™ ××¢×•×œ×”', '××—×™ × ×”×“×¨', '××—×™ ××“×”×™×',
        '××—×™ ×¡×‘×‘×”', '××—×™ ×§×•×œ', '××—×™ ××—×œ×”', '××—×™ ×™×•×¤×™', '××—×™ ××•×©×œ×'
    ]
    
    # ××™××•×’'×™ ×‘×œ×‘×“
    emoji_only = ['ğŸ‘', 'ğŸ‘', 'â¤ï¸', 'ğŸ˜Š', 'ğŸ˜¢', 'ğŸ˜¡', 'ğŸ¤”', 'ğŸ˜…', 'ğŸ˜‚', 'ğŸ˜­']
    
    # × ×§×•×“×•×ª ×‘×œ×‘×“
    dots_only = ['...', '....', '.....', '......']
    
    # ×¡×™×× ×™ ×§×¨×™××” ×‘×œ×‘×“
    exclamation_only = ['!!!', '!!!!', '!!!!!']
    
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×‘×“×™×•×§ ×‘×™×˜×•×™ ×‘×¡×™×¡×™
    message_lower = message.lower()
    for phrase in base_phrases:
        if message_lower == phrase.lower():
            return False
    
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×‘×™×˜×•×™ ×‘×¡×™×¡×™ + ×ª×•×•×™× × ×•×¡×¤×™×
    for phrase in base_phrases:
        phrase_lower = phrase.lower()
        
        # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ××ª×—×™×œ×” ×‘×‘×™×˜×•×™ ×”×‘×¡×™×¡×™
        if message_lower.startswith(phrase_lower):
            # ××” ×©× ×©××¨ ××—×¨×™ ×”×‘×™×˜×•×™ ×”×‘×¡×™×¡×™
            remaining = message_lower[len(phrase_lower):].strip()
            
            # ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ×ª×•×•×™× ××•×ª×¨×™×, ××– ×œ× ×œ×”×¤×¢×™×œ gpt_c
            if remaining in ['', '!', '?', ':)', ':(', '!:)', '?:(', '!:(', '?:)', '...', '....', '.....', '......', '!!!', '!!!!', '!!!!!']:
                return False
            
            # ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ××™××•×’'×™ ××• ×©×™×œ×•×‘ ×©×œ ×ª×•×•×™× ××•×ª×¨×™×
            import re
            # ×”×¡×¨×ª ×¨×•×•×—×™× ××”×ª×—×œ×” ×•××”×¡×•×£
            remaining_clean = remaining.strip()
            # ×‘×“×™×§×” ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ×ª×•×•×™× ××•×ª×¨×™×
            allowed_chars = r'^[!?:\.\s\(\)]+$'
            if re.match(allowed_chars, remaining_clean):
                return False
    
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×¨×§ ××™××•×’'×™
    if message in emoji_only:
        return False
    
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×¨×§ × ×§×•×“×•×ª
    if message in dots_only:
        return False
    
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×¨×§ ×¡×™×× ×™ ×§×¨×™××”
    if message in exclamation_only:
        return False
    
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×‘×™×˜×•×™ + ××™××•×’'×™
    for phrase in base_phrases:
        phrase_lower = phrase.lower()
        if message_lower.startswith(phrase_lower):
            remaining = message_lower[len(phrase_lower):].strip()
            # ×‘×“×™×§×” ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ××™××•×’'×™
            if remaining in ['ğŸ‘', 'ğŸ‘', 'â¤ï¸', 'ğŸ˜Š', 'ğŸ˜¢', 'ğŸ˜¡', 'ğŸ¤”', 'ğŸ˜…', 'ğŸ˜‚', 'ğŸ˜­']:
                return False
    
    # ×× ×”×’×¢× ×• ×œ×›××Ÿ, ×”×”×•×“×¢×” ×™×›×•×œ×” ×œ×”×›×™×œ ××™×“×¢ ×—×“×©
    return True

# ============================×”×’'×™×¤×™×˜×™ ×”-A - ×¤×•×¢×œ ×ª××™×“ ×•×¢×•× ×” ×ª×©×•×‘×” ×œ××©×ª××© =======================

def get_main_response(full_messages, chat_id=None, message_id=None):
    """
    ×©×•×œ×— ×”×•×“×¢×” ×œ-gpt_a ×”×¨××©×™ ×•××—×–×™×¨ ××ª ×”×ª×©×•×‘×”, ×›×•×œ×œ ×¤×™×¨×•×˜ ×¢×œ×•×ª ×•×˜×•×§× ×™×.
    """
    try:
        import litellm
        
        metadata = {"gpt_identifier": "gpt_a", "chat_id": chat_id, "message_id": message_id}
        response = litellm.completion(
            model="gpt-4o",
            messages=full_messages,
            temperature=1,
            metadata=metadata,
            store=True
        )

        # ğŸ”¥ ×“×™×‘××’ ××¤×•×¨×˜ - ×ª×•×¡×™×£ ××ª ×–×” ×‘×›×œ 3 ×”×¤×•× ×§×¦×™×•×ª
        print(f"[DEBUG] === RAW RESPONSE DEBUG (gpt_a) ===")
        print(f"[DEBUG] response type: {type(response)}")
        print(f"[DEBUG] response attributes: {dir(response)}")
        print(f"[DEBUG] usage type: {type(response.usage)}")
        print(f"[DEBUG] usage attributes: {dir(response.usage)}")
        print(f"[DEBUG] usage as dict: {response.usage.__dict__ if hasattr(response.usage, '__dict__') else 'no __dict__'}")

        # ×‘×“×™×§×” ×× ×™×© prompt_tokens_details
        if hasattr(response.usage, 'prompt_tokens_details'):
            print(f"[DEBUG] prompt_tokens_details found!")
            print(f"[DEBUG] prompt_tokens_details type: {type(response.usage.prompt_tokens_details)}")
            print(f"[DEBUG] prompt_tokens_details attributes: {dir(response.usage.prompt_tokens_details)}")
            print(f"[DEBUG] prompt_tokens_details as dict: {response.usage.prompt_tokens_details.__dict__ if hasattr(response.usage.prompt_tokens_details, '__dict__') else 'no __dict__'}")
        else:
            print(f"[DEBUG] NO prompt_tokens_details found!")

        # ×‘×“×™×§×” ×× ×™×© raw response
        if hasattr(response, '_raw_response'):
            print(f"[DEBUG] _raw_response found: {response._raw_response}")
        elif hasattr(response, 'raw'):
            print(f"[DEBUG] raw found: {response.raw}")
        else:
            print(f"[DEBUG] No raw response found")
        print(f"[DEBUG] === END RAW RESPONSE DEBUG (gpt_a) ===")

        prompt_tokens = response.usage.prompt_tokens
        cached_tokens = getattr(getattr(response.usage, 'prompt_tokens_details', None), 'cached_tokens', 0)
        completion_tokens = response.usage.completion_tokens
        model_name = response.model

        _debug_gpt_usage(model_name, prompt_tokens, completion_tokens, cached_tokens, prompt_tokens + completion_tokens, "main_reply")

        print(f"[DEBUG] === CALLING calculate_gpt_cost ===")
        cost_data = calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens, model_name, completion_response=response)
        print(f"[DEBUG] calculate_gpt_cost returned: {cost_data}")
        print(f"[DEBUG] === END calculate_gpt_cost ===")
        
        write_gpt_log("main_reply", cost_data, model_name, interaction_id=message_id)
        
        return {"bot_reply": response.choices[0].message.content, "usage": cost_data}
        
    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-gpt_a ×¨××©×™: {e}")
        raise

def get_main_response_async(*args, **kwargs):
    loop = asyncio.get_event_loop()
    return loop.run_in_executor(None, get_main_response, *args, **kwargs)

# ============================×”×’'×™×¤×™×˜×™ ×”-B - ×ª××¦×™×ª ×ª×©×•×‘×” ×œ×”×™×¡×˜×•×¨×™×” ======================= 

def summarize_bot_reply(reply_text, chat_id=None, original_message_id=None):
    """
    ×©×•×œ×— ×ª×©×•×‘×” ×©×œ ×”×‘×•×˜ ×œ-gpt_b ×•××§×‘×œ ×ª××¦×™×ª ×§×¦×¨×” ×œ×”×™×¡×˜×•×¨×™×”.
    """
    try:
        import litellm
        
        metadata = {"gpt_identifier": "gpt_b", "chat_id": chat_id, "original_message_id": original_message_id}
        response = litellm.completion(
            model="gpt-4.1-nano",
            messages=[{"role": "system", "content": BOT_REPLY_SUMMARY_PROMPT}, {"role": "user", "content": reply_text}],
            temperature=1,
            metadata=metadata,
            store=True
        )

        # ğŸ”¥ ×“×™×‘××’ ××¤×•×¨×˜ - ×ª×•×¡×™×£ ××ª ×–×” ×‘×›×œ 3 ×”×¤×•× ×§×¦×™×•×ª
        print(f"[DEBUG] === RAW RESPONSE DEBUG (gpt_b) ===")
        print(f"[DEBUG] response type: {type(response)}")
        print(f"[DEBUG] response attributes: {dir(response)}")
        print(f"[DEBUG] usage type: {type(response.usage)}")
        print(f"[DEBUG] usage attributes: {dir(response.usage)}")
        print(f"[DEBUG] usage as dict: {response.usage.__dict__ if hasattr(response.usage, '__dict__') else 'no __dict__'}")

        # ×‘×“×™×§×” ×× ×™×© prompt_tokens_details
        if hasattr(response.usage, 'prompt_tokens_details'):
            print(f"[DEBUG] prompt_tokens_details found!")
            print(f"[DEBUG] prompt_tokens_details type: {type(response.usage.prompt_tokens_details)}")
            print(f"[DEBUG] prompt_tokens_details attributes: {dir(response.usage.prompt_tokens_details)}")
            print(f"[DEBUG] prompt_tokens_details as dict: {response.usage.prompt_tokens_details.__dict__ if hasattr(response.usage.prompt_tokens_details, '__dict__') else 'no __dict__'}")
        else:
            print(f"[DEBUG] NO prompt_tokens_details found!")

        # ×‘×“×™×§×” ×× ×™×© raw response
        if hasattr(response, '_raw_response'):
            print(f"[DEBUG] _raw_response found: {response._raw_response}")
        elif hasattr(response, 'raw'):
            print(f"[DEBUG] raw found: {response.raw}")
        else:
            print(f"[DEBUG] No raw response found")
        print(f"[DEBUG] === END RAW RESPONSE DEBUG (gpt_b) ===")

        prompt_tokens = response.usage.prompt_tokens
        cached_tokens = getattr(getattr(response.usage, 'prompt_tokens_details', None), 'cached_tokens', 0)
        completion_tokens = response.usage.completion_tokens
        model_name = response.model

        _debug_gpt_usage(model_name, prompt_tokens, completion_tokens, cached_tokens, prompt_tokens + completion_tokens, "summary")

        print(f"[DEBUG] === CALLING calculate_gpt_cost (gpt_b) ===")
        cost_data = calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens, model_name, completion_response=response)
        print(f"[DEBUG] calculate_gpt_cost (gpt_b) returned: {cost_data}")
        print(f"[DEBUG] === END calculate_gpt_cost (gpt_b) ===")
        
        write_gpt_log("reply_summary", cost_data, model_name, interaction_id=original_message_id)

        return {"summary": response.choices[0].message.content, "usage": cost_data}

    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-gpt_b ×ª××¦×™×ª: {e}")
        raise

def summarize_bot_reply_async(*args, **kwargs):
    loop = asyncio.get_event_loop()
    return loop.run_in_executor(None, summarize_bot_reply, *args, **kwargs)

# ============================×”×’'×™×¤×™×˜×™ ×”-3 - ×¤×•×¢×œ ×ª××™×“ ×•××—×œ×¥ ××™×“×¢ ×œ×ª.×– ×”×¨×’×©×™×ª ======================= 

def validate_extracted_data(data):
    """
    ×‘×•×“×§ ×× ×”× ×ª×•× ×™× ×©×—×•×œ×¦×• ××”-gpt ×ª×§×™× ×™× (dict, ××¤×ª×—×•×ª ××¡×•×’ str ×‘×œ×‘×“).
    ×§×œ×˜: data (dict)
    ×¤×œ×˜: True/False
    """
    validated = data.copy()
    
    # ×‘×“×™×§×ª ×’×™×œ ×”×’×™×•× ×™ - ×¨×§ ××¢×œ 80
    if FIELDS_DICT["age"] in validated:
        try:
            age = int(validated[FIELDS_DICT["age"]])
            if age > 80:
                logging.warning(f"âš ï¸ ×’×™×œ {age} ××¢×œ 80, ××¡×™×¨ ××”× ×ª×•× ×™×")
                del validated[FIELDS_DICT["age"]]
            else:
                validated[FIELDS_DICT["age"]] = age
        except (ValueError, TypeError):
            logging.warning(f"âš ï¸ ×’×™×œ ×œ× ×ª×§×™×Ÿ: {validated[FIELDS_DICT['age']]}, ××¡×™×¨ ××”× ×ª×•× ×™×")
            del validated[FIELDS_DICT["age"]]
    
    # ×”×’×‘×œ×ª ××•×¨×š ×©×“×•×ª ×œ×—×¡×›×•×Ÿ ×‘×˜×•×§× ×™×
    for field, value in list(validated.items()):
        if isinstance(value, str):
            if len(value) > 100:
                logging.warning(f"âš ï¸ ×©×“×” {field} ××¨×•×š ××“×™ ({len(value)} ×ª×•×•×™×), ××§×¦×¨")
                validated[field] = value[:97] + "..."
            elif len(value.strip()) == 0:
                logging.warning(f"âš ï¸ ×©×“×” {field} ×¨×™×§, ××¡×™×¨")
                del validated[field]
    
    return validated


# ============================×¤×•× ×§×¦×™×” ×©××¤×¢×™×œ×” ××ª ×”×’'×™×¤×™×˜×™ ×”×¨×‘×™×¢×™ ×œ×¤×™ ×”×™×’×™×•×Ÿ -×œ× ×¤×•×¢×œ ×ª××™×“ - ×¢×“×›×•×Ÿ ×—×›× ×©×œ ×ª.×– ×”×¨×’×©×™×ª ======================= 

def smart_update_profile(existing_profile, user_message, interaction_id=None):
    """
    ××¢×“×›×Ÿ ×ª×¢×•×“×ª ×–×”×•×ª ×¨×’×©×™×ª ×©×œ ××©×ª××© ×¢×œ ×™×“×™ ×—×™×œ×•×¥ ×¤×¨×˜×™× ××”×•×“×¢×ª×•.
    ×–×•×”×™ ×¤×•× ×§×¦×™×™×ª ××¢×˜×¤×ª ×©×§×•×¨××ª ×œ-gpt_c.
    """
    print(f"[DEBUG][smart_update_profile] - interaction_id: {interaction_id}")
    try:
        gpt_c_response = gpt_c(
            user_message=user_message,
            chat_id=interaction_id
        )

        if not gpt_c_response or not gpt_c_response.get("full_data"):
            return existing_profile, {}

        new_data = gpt_c_response.get("full_data", {})
        extract_usage = {k: v for k, v in gpt_c_response.items() if k not in ["updated_summary", "full_data"]}

        if not new_data:
            return existing_profile, extract_usage

        updated_profile = {**existing_profile, **new_data}
        return updated_profile, extract_usage

    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-smart_update_profile: {e}")
        return existing_profile, {}

def smart_update_profile_async(existing_profile, user_message, interaction_id=None):
    loop = asyncio.get_event_loop()
    return loop.run_in_executor(None, smart_update_profile, existing_profile, user_message, interaction_id)

# ============================ gpt_c - ×”×¤×•× ×§×¦×™×” ×”×¨××©×™×ª ============================

def gpt_c(user_message, last_bot_message="", chat_id=None, message_id=None):
    """
    ××¤×¢×™×œ ××ª ×›×œ ×–×¨×™××ª ×”-gpt: gpt_a, gpt_b, ×•-smart_update_profile (×©×§×•×¨× ×œ-gpt_c).
    """
    print("[DEBUG][gpt_c] CALLED - ×”×¤×•× ×§×¦×™×” ×”×¨××©×™×ª")
    try:
        import litellm
        
        logging.info("ğŸ”„ ××ª×—×™×œ gpt_c - ×¢×“×›×•×Ÿ ×¡×™×›×•× ×¢× ××™×“×¢ ×—×“×©")
        print(f"[DEBUG][gpt_c] --- START ---")
        print(f"[DEBUG][gpt_c] user_message: {user_message} (type: {type(user_message)})")
        print(f"[DEBUG][gpt_c] last_bot_message: {last_bot_message} (type: {type(last_bot_message)})")
        print(f"[DEBUG][gpt_c] PROFILE_EXTRACTION_ENHANCED_PROMPT: {PROFILE_EXTRACTION_ENHANCED_PROMPT}")
        metadata = {"gpt_identifier": "gpt_c", "chat_id": chat_id, "message_id": message_id}
        
        # ×™×¦×™×¨×ª ×ª×•×›×Ÿ ×©××©×œ×‘ ××ª ×”×•×“×¢×ª ×”××©×ª××© ×¢× ×”×”×•×“×¢×” ×”××—×¨×•× ×” ×©×œ ×”×‘×•×˜
        if last_bot_message:
            user_content = f"×©××œ×ª ×”×‘×•×˜ ×œ×¦×•×¨×š ×”×§×©×¨ ×‘×œ×‘×“:\n{last_bot_message}\n\n×ª×©×•×‘×ª ×”××©×ª××© ×œ×¦×•×¨×š ×—×™×œ×•×¥ ××™×“×¢:\n{user_message}"
        else:
            user_content = f"×ª×©×•×‘×ª ×”××©×ª××© ×œ×¦×•×¨×š ×—×™×œ×•×¥ ××™×“×¢:\n{user_message}"
        
        response = litellm.completion(
            model="gpt-4o-mini",  # ××•×“×œ ×–×•×œ ×•××”×™×¨
            messages=[
                {"role": "system", "content": PROFILE_EXTRACTION_ENHANCED_PROMPT},
                {"role": "user", "content": user_content}
            ],
            temperature=0.3,
            max_tokens=500,
            metadata=metadata,
            store=True
        )
        
        # ğŸ”¥ ×“×™×‘××’ ××¤×•×¨×˜ - ×ª×•×¡×™×£ ××ª ×–×” ×‘×›×œ 3 ×”×¤×•× ×§×¦×™×•×ª
        print(f"[DEBUG] === RAW RESPONSE DEBUG (gpt_c) ===")
        print(f"[DEBUG] response type: {type(response)}")
        print(f"[DEBUG] response attributes: {dir(response)}")
        print(f"[DEBUG] usage type: {type(response.usage)}")
        print(f"[DEBUG] usage attributes: {dir(response.usage)}")
        print(f"[DEBUG] usage as dict: {response.usage.__dict__ if hasattr(response.usage, '__dict__') else 'no __dict__'}")

        # ×‘×“×™×§×” ×× ×™×© prompt_tokens_details
        if hasattr(response.usage, 'prompt_tokens_details'):
            print(f"[DEBUG] prompt_tokens_details found!")
            print(f"[DEBUG] prompt_tokens_details type: {type(response.usage.prompt_tokens_details)}")
            print(f"[DEBUG] prompt_tokens_details attributes: {dir(response.usage.prompt_tokens_details)}")
            print(f"[DEBUG] prompt_tokens_details as dict: {response.usage.prompt_tokens_details.__dict__ if hasattr(response.usage.prompt_tokens_details, '__dict__') else 'no __dict__'}")
        else:
            print(f"[DEBUG] NO prompt_tokens_details found!")

        # ×‘×“×™×§×” ×× ×™×© raw response
        if hasattr(response, '_raw_response'):
            print(f"[DEBUG] _raw_response found: {response._raw_response}")
        elif hasattr(response, 'raw'):
            print(f"[DEBUG] raw found: {response.raw}")
        else:
            print(f"[DEBUG] No raw response found")
        print(f"[DEBUG] === END RAW RESPONSE DEBUG (gpt_c) ===")

        content = response.choices[0].message.content.strip()
        print(f"[DEBUG][gpt_c] raw gpt_c response: {content}")
        
        # ×“×™×‘××’: ×”×“×¤×¡×” ×©×œ ×”×ª×’×•×‘×” ×”×’×•×œ××™×ª ××”-API
        print(f"[DEBUG][gpt_c] FULL API RESPONSE: {response}")
        
        if content.startswith("```"):
            match = re.search(r"```(?:json)?\s*({.*?})\s*```", content, re.DOTALL)
            if match:
                content = match.group(1)
                print(f"[DEBUG][gpt_c] cleaned content: {content}")
        
        try:
            result = json.loads(content)
            print(f"[DEBUG][gpt_c] parsed result: {result}")
        except Exception as e:
            print(f"[ERROR][gpt_c] JSON parsing error: {e}")
            print(f"[ERROR][gpt_c] content that failed to parse: {content}")
            result = {}
        
        # ×”××¨ ××ª ×”×ª×•×¦××” ×œ××‘× ×” ×”× ×›×•×Ÿ
        if isinstance(result, dict):
            # ×× ×™×© ×©×“×•×ª ×¤×¨×•×¤×™×œ, ×¦×•×¨ ×¡×™×›×•× ××”×
            profile_fields = []
            for key, value in result.items():
                if value and value != "null" and key in ["age", "pronoun_preference", "attracted_to", "relationship_type", "occupation_or_role"]:
                    profile_fields.append(f"{key}: {value}")
            
            summary = "; ".join(profile_fields) if profile_fields else ""
            full_data = result
        else:
            summary = ""
            full_data = {}
        
        prompt_tokens = response.usage.prompt_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens
        cached_tokens = getattr(getattr(response.usage, 'prompt_tokens_details', None), 'cached_tokens', 0)
        model_name = response.model
        
        print(f"[DEBUG] === CALLING calculate_gpt_cost (gpt_c) ===")
        cost_data = calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens, model_name, completion_response=response)
        print(f"[DEBUG] calculate_gpt_cost (gpt_c) returned: {cost_data}")
        print(f"[DEBUG] === END calculate_gpt_cost (gpt_c) ===")
        
        final_result = {
            "updated_summary": summary,
            "full_data": full_data,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cached_tokens": cached_tokens,
            "model": model_name,
            **cost_data
        }
        
        print(f"[DEBUG][gpt_c] final_result: {final_result}")
        logging.info(f"âœ… gpt_c ×”×•×©×œ× ×‘×”×¦×œ×—×”")
        
        # ×›×ª×™×‘×” ×œ×œ×•×’
        write_gpt_log("identity_extraction", cost_data, model_name, interaction_id=message_id)
        
        return final_result
        
    except Exception as e:
        import traceback
        print(f"[ERROR][gpt_c] Exception: {e}")
        print(traceback.format_exc())
        logging.error(f"âŒ ×©×’×™××” ×‘-gpt_c: {e}")
        return None

def gpt_c_async(*args, **kwargs):
    loop = asyncio.get_event_loop()
    return loop.run_in_executor(None, gpt_c, *args, **kwargs)

def normalize_usage_dict(usage, model_name=""):
    """
    ×××¤×” usage ××›×œ ×¤×•×¨××˜ (litellm/openai) ×œ×¤×•×¨××˜ ××—×™×“.
    """
    if not usage:
        return {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "cached_tokens": 0,
            "cost_total": 0.0,
            "cost_total_ils": 0.0,
            "cost_agorot": 0.0,
            "model": model_name
        }
    # mapping for litellm
    prompt = usage.get("prompt_tokens", usage.get("input_tokens", 0))
    completion = usage.get("completion_tokens", usage.get("output_tokens", 0))
    total = usage.get("total_tokens", prompt + completion)
    cached = usage.get("cached_tokens", 0)
    cost_total = usage.get("cost_total", 0.0)
    cost_total_ils = usage.get("cost_total_ils", 0.0)
    cost_agorot = usage.get("cost_agorot", cost_total_ils * 100 if cost_total_ils else 0.0)
    model = usage.get("model", model_name)
    return {
        "prompt_tokens": prompt,
        "completion_tokens": completion,
        "total_tokens": total,
        "cached_tokens": cached,
        "cost_total": cost_total,
        "cost_total_ils": cost_total_ils,
        "cost_agorot": cost_agorot,
        "model": model
    }

# ============================ gpt_d - ××•×“×œ ×—×›× ×œ×˜×™×¤×•×œ ×‘×©×“×•×ª ×©×”×©×ª× ×• ============================

def gpt_d(changed_fields, chat_id=None, message_id=None):
    """
    ××•×“×œ ×—×›× ×©××—×œ×™×˜ ××™×š ×œ×˜×¤×œ ×‘×©×“×•×ª ×©×”×©×ª× ×•.
    ××§×‘×œ ×¨×§ ××ª ×”×©×“×•×ª ×”×¨×œ×•×•× ×˜×™×™× (×”×™×©×Ÿ + ×”×—×“×©) ×›×“×™ ×œ×”×™×•×ª ×—×¡×›×•× ×™.
    
    ×§×œ×˜:
    - changed_fields: dict ×¢× ×”××‘× ×” {"field_name": {"old": "value", "new": "value"}}
    - chat_id: ××–×”×” ×”×¦'××˜
    - message_id: ××–×”×” ×”×”×•×“×¢×”
    
    ×¤×œ×˜:
    - dict ×¢× ×”×”×—×œ×˜×” ×œ×›×œ ×©×“×”: {"field_name": "final_value"}
    - usage data
    """
    print(f"[DEBUG][gpt_d] CALLED - ××•×“×œ ×—×›× ×œ×˜×™×¤×•×œ ×‘×©×“×•×ª ×©×”×©×ª× ×•")
    print(f"[DEBUG][gpt_d] changed_fields: {changed_fields}")
    
    if not changed_fields:
        print(f"[DEBUG][gpt_d] No changed fields, returning empty result")
        return {"final_values": {}, "usage": {}}
    
    try:
        import litellm
        
        # ×™×¦×™×¨×ª prompt ×—×›× ×œ×˜×™×¤×•×œ ×‘×©×“×•×ª ×©×”×©×ª× ×•
        system_prompt = """××ª×” ××•×“×œ ×—×›× ×©××—×œ×™×˜ ××™×š ×œ×˜×¤×œ ×‘×©×“×•×ª ×©×”×©×ª× ×• ×‘×ª×¢×•×“×ª ×–×”×•×ª ×¨×’×©×™×ª.

×›×œ×œ×™× ×œ×”×—×œ×˜×”:
1. **×’×™×œ**: ×× ×”×’×™×œ ×”×—×“×© ×”×’×™×•× ×™ ×™×•×ª×¨ (13-80), ×”×©×ª××© ×‘×•. ×× ×”×’×™×œ ×”×™×©×Ÿ ×”×’×™×•× ×™ ×™×•×ª×¨, ×”×©××¨ ××•×ª×•.
2. **××§×¦×•×¢/×ª×¤×§×™×“**: ×× ×”××™×“×¢ ×”×—×“×© ××¤×•×¨×˜ ×™×•×ª×¨ ××• ×¢×“×›× ×™ ×™×•×ª×¨, ×”×©×ª××© ×‘×•. ×× ×”×™×©×Ÿ ××¤×•×¨×˜ ×™×•×ª×¨, ×”×©××¨ ××•×ª×•.
3. **×”×¢×“×¤×•×ª**: ×× ×™×© ××™×“×¢ ×—×“×© ×©××©×œ×™× ××ª ×”×™×©×Ÿ, ×¦×¨×£ ××•×ª×. ×× ×™×© ×¡×ª×™×¨×”, ×‘×—×¨ ×‘××™×“×¢ ×”××¤×•×¨×˜ ×™×•×ª×¨.
4. **××™×“×¢ ××™×©×™**: ×× ×”××™×“×¢ ×”×—×“×© × ×¨××” ×××™×Ÿ ×™×•×ª×¨ ××• ××¤×•×¨×˜ ×™×•×ª×¨, ×”×©×ª××© ×‘×•.
5. **××™×›×•×ª × ×ª×•× ×™×**: ×©×§×•×œ ××ª ×¦×™×•× ×™ ×”××™×›×•×ª - ××™×“×¢ ×¢× ×¦×™×•×Ÿ ××™×›×•×ª ×’×‘×•×” ×™×•×ª×¨ ×¢×“×™×£.

×”×—×–×¨ ×ª××™×“ JSON ×‘×¤×•×¨××˜:
{
  "field_name": "final_value",
  "field_name2": "final_value2"
}

××œ ×ª×—×–×™×¨ ×”×¡×‘×¨×™×, ×¨×§ ××ª ×”-JSON."""

        # ×™×¦×™×¨×ª ×ª×•×›×Ÿ ×¢× ×”×©×“×•×ª ×©×”×©×ª× ×•
        fields_content = []
        for field_name, values in changed_fields.items():
            # ×”×¢×¨×›×ª ××™×›×•×ª ×”× ×ª×•× ×™×
            old_quality = assess_data_quality(values['old'], field_name)
            new_quality = assess_data_quality(values['new'], field_name)
            fields_content.append(
                f"{field_name}: ×™×©×Ÿ='{values['old']}' (××™×›×•×ª: {old_quality}) ×—×“×©='{values['new']}' (××™×›×•×ª: {new_quality})"
            )
        
        user_content = "×”×©×“×•×ª ×©×”×©×ª× ×• (×¢× ×¦×™×•× ×™ ××™×›×•×ª):\n" + "\n".join(fields_content)
        
        metadata = {"gpt_identifier": "gpt_d", "chat_id": chat_id, "message_id": message_id}
        
        response = litellm.completion(
            model="gpt-4o-mini",  # ××•×“×œ ×–×•×œ ×•××”×™×¨
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.1,  # ×˜××¤×¨×˜×•×¨×” × ××•×›×” ×œ×”×—×œ×˜×•×ª ×¢×§×‘×™×•×ª
            max_tokens=300,   # ××•×’×‘×œ ×›×™ ×× ×—× ×• ××§×‘×œ×™× ×¨×§ ×©×“×•×ª ××¢×˜×™×
            metadata=metadata,
            store=True
        )
        
        # ğŸ”¥ ×“×™×‘××’ ××¤×•×¨×˜
        print(f"[DEBUG] === RAW RESPONSE DEBUG (gpt_d) ===")
        print(f"[DEBUG] response type: {type(response)}")
        print(f"[DEBUG] response attributes: {dir(response)}")
        print(f"[DEBUG] usage type: {type(response.usage)}")
        print(f"[DEBUG] usage attributes: {dir(response.usage)}")
        print(f"[DEBUG] usage as dict: {response.usage.__dict__ if hasattr(response.usage, '__dict__') else 'no __dict__'}")

        if hasattr(response.usage, 'prompt_tokens_details'):
            print(f"[DEBUG] prompt_tokens_details found!")
            print(f"[DEBUG] prompt_tokens_details type: {type(response.usage.prompt_tokens_details)}")
            print(f"[DEBUG] prompt_tokens_details attributes: {dir(response.usage.prompt_tokens_details)}")
            print(f"[DEBUG] prompt_tokens_details as dict: {response.usage.prompt_tokens_details.__dict__ if hasattr(response.usage.prompt_tokens_details, '__dict__') else 'no __dict__'}")
        else:
            print(f"[DEBUG] NO prompt_tokens_details found!")

        if hasattr(response, '_raw_response'):
            print(f"[DEBUG] _raw_response found: {response._raw_response}")
        elif hasattr(response, 'raw'):
            print(f"[DEBUG] raw found: {response.raw}")
        else:
            print(f"[DEBUG] No raw response found")
        print(f"[DEBUG] === END RAW RESPONSE DEBUG (gpt_d) ===")

        content = response.choices[0].message.content.strip()
        print(f"[DEBUG][gpt_d] raw gpt_d response: {content}")
        
        # × ×™×§×•×™ ×”×ª×’×•×‘×” ×-JSON
        if content.startswith("```"):
            match = re.search(r"```(?:json)?\s*({.*?})\s*```", content, re.DOTALL)
            if match:
                content = match.group(1)
                print(f"[DEBUG][gpt_d] cleaned content: {content}")
        
        try:
            final_values = json.loads(content)
            print(f"[DEBUG][gpt_d] parsed final_values: {final_values}")
        except Exception as e:
            print(f"[ERROR][gpt_d] JSON parsing error: {e}")
            print(f"[ERROR][gpt_d] content that failed to parse: {content}")
            # ×× × ×›×©×œ, ×”×©×ª××© ×‘×¢×¨×›×™× ×”×—×“×©×™×
            final_values = {field: values['new'] for field, values in changed_fields.items()}
        
        # ×—×™×©×•×‘ ×¢×œ×•×™×•×ª
        prompt_tokens = response.usage.prompt_tokens
        completion_tokens = response.usage.completion_tokens
        cached_tokens = getattr(getattr(response.usage, 'prompt_tokens_details', None), 'cached_tokens', 0)
        model_name = response.model
        
        print(f"[DEBUG] === CALLING calculate_gpt_cost (gpt_d) ===")
        cost_data = calculate_gpt_cost(prompt_tokens, completion_tokens, cached_tokens, model_name, completion_response=response)
        print(f"[DEBUG] calculate_gpt_cost (gpt_d) returned: {cost_data}")
        print(f"[DEBUG] === END calculate_gpt_cost (gpt_d) ===")
        
        result = {
            "final_values": final_values,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": response.usage.total_tokens,
            "cached_tokens": cached_tokens,
            "model": model_name,
            **cost_data
        }
        
        print(f"[DEBUG][gpt_d] final result: {result}")
        logging.info(f"âœ… gpt_d ×”×•×©×œ× ×‘×”×¦×œ×—×”")
        
        # ×›×ª×™×‘×” ×œ×œ×•×’
        write_gpt_log("field_conflict_resolution", cost_data, model_name, interaction_id=message_id)
        
        # ×ª×™×¢×•×“ ×‘×™×¦×•×¢×™×
        log_gpt_d_performance(changed_fields, final_values, cost_data)
        
        return result
        
    except Exception as e:
        import traceback
        print(f"[ERROR][gpt_d] Exception: {e}")
        print(traceback.format_exc())
        logging.error(f"âŒ ×©×’×™××” ×‘-gpt_d: {e}")
        # ×× × ×›×©×œ, ×”×©×ª××© ×‘×¢×¨×›×™× ×”×—×“×©×™×
        return {
            "final_values": {field: values['new'] for field, values in changed_fields.items()},
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "cached_tokens": 0,
            "cost_total": 0.0,
            "cost_total_ils": 0.0,
            "cost_agorot": 0.0,
            "model": "gpt-4o-mini"
        }

def log_gpt_d_performance(changed_fields, final_values, cost_data):
    """
    ××ª×¢×“ ××ª ×”×‘×™×¦×•×¢×™× ×©×œ gpt_d ×œ×§×•×‘×¥ × ×¤×¨×“.
    """
    try:
        timestamp = datetime.now().isoformat()
        performance_log = {
            "timestamp": timestamp,
            "changed_fields_count": len(changed_fields),
            "changed_fields": list(changed_fields.keys()),
            "final_values": final_values,
            "cost_usd": cost_data.get("cost_total", 0.0),
            "tokens_used": cost_data.get("total_tokens", 0),
            "model": cost_data.get("model", "gpt-4o")
        }
        
        # ×›×ª×™×‘×” ×œ×§×•×‘×¥ × ×¤×¨×“ ×œ×‘×™×¦×•×¢×™×
        performance_log_path = "data/gpt_d_performance.jsonl"
        os.makedirs(os.path.dirname(performance_log_path), exist_ok=True)
        
        with open(performance_log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(performance_log, ensure_ascii=False) + "\n")
            
    except Exception as e:
        logging.error(f"×©×’×™××” ×‘×ª×™×¢×•×“ ×‘×™×¦×•×¢×™ gpt_d: {e}")

def gpt_d_async(*args, **kwargs):
    loop = asyncio.get_event_loop()
    return loop.run_in_executor(None, gpt_d, *args, **kwargs)

# ============================ ×¤×•× ×§×¦×™×” ×œ×‘×“×™×§×ª ×“×•×’×××•×ª ×©×œ gpt_d ============================

def test_gpt_d_examples():
    """
    ×‘×•×“×§ ××ª gpt_d ×¢× ×“×•×’×××•×ª ×©×•× ×•×ª ×›×“×™ ×œ×•×•×“× ×©×”×•× ×¢×•×‘×“ ×›××• ×©×¦×¨×™×š.
    """
    print("ğŸ§ª ×‘×“×™×§×ª gpt_d ×¢× ×“×•×’×××•×ª...")
    
    # ×“×•×’××” 1: ×’×™×œ ×©×”×©×ª× ×”
    example1 = {
        "age": {"old": "25", "new": "26"}
    }
    print(f"\nğŸ“ ×“×•×’××” 1 - ×’×™×œ: {example1}")
    result1 = gpt_d(example1, chat_id="test_1")
    print(f"âœ… ×ª×•×¦××”: {result1.get('final_values', {})}")
    
    # ×“×•×’××” 2: ××§×¦×•×¢ ×©×”×©×ª× ×”
    example2 = {
        "occupation_or_role": {"old": "×¡×˜×•×“× ×˜", "new": "××”× ×“×¡ ×ª×•×›× ×”"}
    }
    print(f"\nğŸ“ ×“×•×’××” 2 - ××§×¦×•×¢: {example2}")
    result2 = gpt_d(example2, chat_id="test_2")
    print(f"âœ… ×ª×•×¦××”: {result2.get('final_values', {})}")
    
    # ×“×•×’××” 3: ×’×™×œ ×œ× ×”×’×™×•× ×™
    example3 = {
        "age": {"old": "25", "new": "150"}
    }
    print(f"\nğŸ“ ×“×•×’××” 3 - ×’×™×œ ×œ× ×”×’×™×•× ×™: {example3}")
    result3 = gpt_d(example3, chat_id="test_3")
    print(f"âœ… ×ª×•×¦××”: {result3.get('final_values', {})}")
    
    # ×“×•×’××” 4: ××¡×¤×¨ ×©×“×•×ª
    example4 = {
        "age": {"old": "30", "new": "31"},
        "occupation_or_role": {"old": "××•×¨×”", "new": "××¨×¦×” ×‘××•× ×™×‘×¨×¡×™×˜×”"},
        "interests": {"old": "×§×¨×™××”", "new": "×§×¨×™××”, ×›×ª×™×‘×”, ×˜×™×•×œ×™×"}
    }
    print(f"\nğŸ“ ×“×•×’××” 4 - ××¡×¤×¨ ×©×“×•×ª: {example4}")
    result4 = gpt_d(example4, chat_id="test_4")
    print(f"âœ… ×ª×•×¦××”: {result4.get('final_values', {})}")
    
    print("\nğŸ‰ ×‘×“×™×§×ª ×”×“×•×’×××•×ª ×”×•×©×œ××”!")

# ============================ ×¤×•× ×§×¦×™×” ×œ×”×¢×¨×›×ª ×¢×œ×•×™×•×ª ×©×œ gpt_d ============================

def estimate_gpt_d_cost(existing_profile, new_data):
    """
    ××¢×¨×™×š ××ª ×”×¢×œ×•×ª ×©×œ ×”×¤×¢×œ×ª gpt_d.
    
    ×§×œ×˜:
    - existing_profile: dict ×¢× ×”×¤×¨×•×¤×™×œ ×”×§×™×™×
    - new_data: dict ×¢× ×”× ×ª×•× ×™× ×”×—×“×©×™×
    
    ×¤×œ×˜:
    - dict ×¢× ×”×¢×¨×›×ª ×¢×œ×•×ª ××¤×•×¨×˜×ª
    """
    # ×–×™×”×•×™ ×©×“×•×ª ×©×”×©×ª× ×• (×¨×§ ×›××œ×” ×©×”×™×©×Ÿ ×”×™×” ××œ×)
    changed_fields = identify_changed_fields(existing_profile, new_data)
    
    if not changed_fields:
        return {
            "estimated_cost_usd": 0.0,
            "reason": "××™×Ÿ ×©×“×•×ª ×©×”×©×ª× ×• ××• ×”×©×“×” ×”×™×©×Ÿ ×”×™×” ×¨×™×§",
            "changed_fields_count": 0
        }
    
    # ×”×¢×¨×›×ª ×˜×•×§× ×™×
    # Prompt: ~200 ×˜×•×§× ×™×
    # ×›×œ ×©×“×”: ~50 ×˜×•×§× ×™× (×©× + ×¢×¨×š ×™×©×Ÿ + ×¢×¨×š ×—×“×©)
    # ×ª×©×•×‘×”: ~100 ×˜×•×§× ×™× ×œ×›×œ ×©×“×”
    base_prompt_tokens = 200
    field_tokens = len(changed_fields) * 50
    response_tokens = len(changed_fields) * 100
    
    total_input_tokens = base_prompt_tokens + field_tokens
    total_output_tokens = response_tokens
    
    # ×¢×œ×•×ª gpt-4o (× ×›×•×Ÿ ×œ-2024)
    input_cost_per_1k = 0.005  # $5 per 1M tokens
    output_cost_per_1k = 0.015  # $15 per 1M tokens
    
    input_cost = (total_input_tokens / 1000) * input_cost_per_1k
    output_cost = (total_output_tokens / 1000) * output_cost_per_1k
    total_cost = input_cost + output_cost
    
    return {
        "estimated_cost_usd": total_cost,
        "input_tokens": total_input_tokens,
        "output_tokens": total_output_tokens,
        "changed_fields_count": len(changed_fields),
        "changed_fields": list(changed_fields.keys()),
        "cost_breakdown": {
            "input_cost": input_cost,
            "output_cost": output_cost
        }
    }

# ============================ ×¤×•× ×§×¦×™×” ×œ×‘×“×™×§×ª ×™×¢×™×œ×•×ª ×©×œ gpt_d ============================

def compare_gpt_d_efficiency(existing_profile, new_data):
    """
    ××©×•×•×” ×‘×™×Ÿ ×™×¢×™×œ×•×ª gpt_d ×œ××™×–×•×’ ×¤×©×•×˜.
    
    ×§×œ×˜:
    - existing_profile: dict ×¢× ×”×¤×¨×•×¤×™×œ ×”×§×™×™×
    - new_data: dict ×¢× ×”× ×ª×•× ×™× ×”×—×“×©×™×
    
    ×¤×œ×˜:
    - dict ×¢× ×”×©×•×•××” ××¤×•×¨×˜×ª
    """
    # ××™×–×•×’ ×¤×©×•×˜
    simple_merge_result = simple_merge_profile(existing_profile, new_data)
    
    # ×‘×“×™×§×” ×× ×¦×¨×™×š gpt_d
    should_use_gpt_d = should_activate_gpt_d(existing_profile, new_data)
    
    # ×”×¢×¨×›×ª ×¢×œ×•×ª gpt_d
    estimated_gpt_d_cost = estimate_gpt_d_cost(existing_profile, new_data)
    
    # ×–×™×”×•×™ ×©×“×•×ª ×©×”×©×ª× ×• (×¨×§ ×›××œ×” ×©×”×™×©×Ÿ ×”×™×” ××œ×)
    changed_fields = identify_changed_fields(existing_profile, new_data)
    
    return {
        "should_use_gpt_d": should_use_gpt_d,
        "simple_merge_result": simple_merge_result,
        "changed_fields": changed_fields,
        "estimated_gpt_d_cost": estimated_gpt_d_cost,
        "reasoning": "gpt_d ××•×¤×¢×œ ×¨×§ ×›×©×”×©×“×” ×”×™×©×Ÿ ×”×™×” ××œ× ×•×™×© ×¦×•×¨×š ×‘×”×—×œ×˜×” ×—×›××”"
    }

# ============================ ×¤×•× ×§×¦×™×” ×œ×”×“×’××ª ×©×™××•×© ××¢×©×™ ×‘-gpt_d ============================

def demonstrate_gpt_d_usage():
    """
    ××“×’×™× ×©×™××•×© ××¢×©×™ ×‘-gpt_d ×¢× ×ª×¨×—×™×©×™× ×××™×ª×™×™×.
    """
    print("ğŸ¯ ×”×“×’××ª ×©×™××•×© ××¢×©×™ ×‘-gpt_d...")
    
    # ×ª×¨×—×™×© 1: ××©×ª××© ××¢×“×›×Ÿ ××ª ×”×’×™×œ ×©×œ×•
    print("\nğŸ“‹ ×ª×¨×—×™×© 1: ×¢×“×›×•×Ÿ ×’×™×œ")
    existing_profile1 = {"age": "25", "occupation_or_role": "×¡×˜×•×“× ×˜"}
    new_data1 = {"age": "26"}
    
    comparison1 = compare_gpt_d_efficiency(existing_profile1, new_data1)
    print(f"×”×©×•×•××”: {comparison1}")
    
    if comparison1["should_use_gpt_d"]:
        result1 = gpt_d(comparison1["changed_fields"], chat_id="demo_1")
        print(f"×ª×•×¦××ª gpt_d: {result1.get('final_values', {})}")
    
    # ×ª×¨×—×™×© 2: ××©×ª××© ××©× ×” ××§×¦×•×¢
    print("\nğŸ“‹ ×ª×¨×—×™×© 2: ×©×™× ×•×™ ××§×¦×•×¢")
    existing_profile2 = {"age": "30", "occupation_or_role": "××•×¨×”"}
    new_data2 = {"occupation_or_role": "××¨×¦×” ×‘××•× ×™×‘×¨×¡×™×˜×”"}
    
    comparison2 = compare_gpt_d_efficiency(existing_profile2, new_data2)
    print(f"×”×©×•×•××”: {comparison2}")
    
    if comparison2["should_use_gpt_d"]:
        result2 = gpt_d(comparison2["changed_fields"], chat_id="demo_2")
        print(f"×ª×•×¦××ª gpt_d: {result2.get('final_values', {})}")
    
    # ×ª×¨×—×™×© 3: ×¢×“×›×•×Ÿ ××¨×•×‘×”
    print("\nğŸ“‹ ×ª×¨×—×™×© 3: ×¢×“×›×•×Ÿ ××¨×•×‘×”")
    existing_profile3 = {
        "age": "28", 
        "occupation_or_role": "××¤×ª×— ×ª×•×›× ×”",
        "interests": "×ª×›× ×•×ª, ×§×¨×™××”"
    }
    new_data3 = {
        "age": "29",
        "occupation_or_role": "××”× ×“×¡ ×ª×•×›× ×” ×‘×›×™×¨",
        "interests": "×ª×›× ×•×ª, ×§×¨×™××”, ×˜×™×•×œ×™×"
    }
    
    comparison3 = compare_gpt_d_efficiency(existing_profile3, new_data3)
    print(f"×”×©×•×•××”: {comparison3}")
    
    if comparison3["should_use_gpt_d"]:
        result3 = gpt_d(comparison3["changed_fields"], chat_id="demo_3")
        print(f"×ª×•×¦××ª gpt_d: {result3.get('final_values', {})}")
    
    # ×ª×¨×—×™×© 4: × ×ª×•× ×™× ×œ× ×”×’×™×•× ×™×™×
    print("\nğŸ“‹ ×ª×¨×—×™×© 4: × ×ª×•× ×™× ×œ× ×”×’×™×•× ×™×™×")
    existing_profile4 = {"age": "25"}
    new_data4 = {"age": "150"}  # ×’×™×œ ×œ× ×”×’×™×•× ×™
    
    comparison4 = compare_gpt_d_efficiency(existing_profile4, new_data4)
    print(f"×”×©×•×•××”: {comparison4}")
    
    if comparison4["should_use_gpt_d"]:
        result4 = gpt_d(comparison4["changed_fields"], chat_id="demo_4")
        print(f"×ª×•×¦××ª gpt_d: {result4.get('final_values', {})}")
    
    print("\nğŸ‰ ×”×“×’××ª ×”×©×™××•×© ×”×•×©×œ××”!")

# ============================ ×¤×•× ×§×¦×™×” ×¢×–×¨ ×œ×–×™×”×•×™ ×©×“×•×ª ×©×”×©×ª× ×• ============================

def identify_changed_fields(existing_profile, new_data):
    """
    ××–×”×” ×©×“×•×ª ×©×”×©×ª× ×• ×‘×™×Ÿ ×”×¤×¨×•×¤×™×œ ×”×§×™×™× ×œ× ×ª×•× ×™× ×”×—×“×©×™×.
    ×›×•×œ×œ ×¨×§ ×©×“×•×ª ×©×”×™×©×Ÿ ×”×™×” ××œ× ×•×”×—×“×© ×©×•× ×”.
    
    ×§×œ×˜:
    - existing_profile: dict ×¢× ×”×¤×¨×•×¤×™×œ ×”×§×™×™×
    - new_data: dict ×¢× ×”× ×ª×•× ×™× ×”×—×“×©×™×
    
    ×¤×œ×˜:
    - dict ×¢× ×”×©×“×•×ª ×©×”×©×ª× ×• ×‘×¤×•×¨××˜ {"field_name": {"old": "value", "new": "value"}}
    """
    changed_fields = {}
    
    for field, new_value in new_data.items():
        if field in existing_profile:
            old_value = existing_profile[field]
            # ×‘×“×™×§×” ×× ×”×¢×¨×š ×”×©×ª× ×” ××©××¢×•×ª×™×ª ×•×¨×§ ×× ×”×™×©×Ÿ ×”×™×” ××œ×
            if (old_value and old_value != "" and old_value != "null" and 
                str(old_value).strip() and 
                old_value != new_value and new_value and new_value != "null"):
                changed_fields[field] = {
                    "old": str(old_value),
                    "new": str(new_value)
                }
        # ×œ× ×›×•×œ×œ ×©×“×•×ª ×—×“×©×™× (×©×”×©×“×” ×”×™×©×Ÿ ×”×™×” ×¨×™×§) - ××œ×” ×™×˜×•×¤×œ×• ×‘××™×–×•×’ ×¤×©×•×˜
    
    return changed_fields

# ============================ ×¤×•× ×§×¦×™×” ×œ×‘×“×™×§×” ×× ×¦×¨×™×š ×œ×”×¤×¢×™×œ gpt_d ============================

def should_activate_gpt_d(existing_profile, new_data):
    """
    ×‘×•×“×§ ×× ×™×© ×¦×•×¨×š ×œ×”×¤×¢×™×œ ××ª gpt_d ×œ×˜×™×¤×•×œ ×‘×©×“×•×ª ×©×”×©×ª× ×•.
    ××¤×¢×™×œ ×¨×§ ×›×©×”×©×“×” ×”×™×©×Ÿ ×”×™×” ××œ× ×•×™×© ×¦×•×¨×š ×‘×”×—×œ×˜×” ×—×›××”.
    
    ×§×œ×˜:
    - existing_profile: dict ×¢× ×”×¤×¨×•×¤×™×œ ×”×§×™×™×
    - new_data: dict ×¢× ×”× ×ª×•× ×™× ×”×—×“×©×™×
    
    ×¤×œ×˜:
    - True ×× ×™×© ×©×“×•×ª ×©×”×©×ª× ×• ×•×™×© ×¦×•×¨×š ×‘×”×—×œ×˜×” ×—×›××”
    - False ×× ××™×Ÿ ×©×“×•×ª ×©×”×©×ª× ×• ××• ×©××™×Ÿ ×¦×•×¨×š ×‘×”×—×œ×˜×”
    """
    if not existing_profile or not new_data:
        return False
    
    # ×‘×•×“×§ ×× ×™×© ×©×“×•×ª ×©×“×•×¨×©×™× ×”×—×œ×˜×” ×—×›××”
    fields_requiring_smart_decision = [
        "age", "pronoun_preference", "attracted_to", "relationship_type", 
        "occupation_or_role", "interests", "personality_traits"
    ]
    
    for field, new_value in new_data.items():
        if field in existing_profile:
            old_value = existing_profile[field]
            # ×¨×§ ×× ×”×©×“×” ×”×™×©×Ÿ ×”×™×” ××œ× ×•×œ× ×¨×™×§
            if old_value and old_value != "" and old_value != "null" and str(old_value).strip():
                # ×¨×§ ×× ×”×¢×¨×š ×”×—×“×© ×©×•× ×” ××”×™×©×Ÿ
                if old_value != new_value and new_value and new_value != "null":
                    if field in fields_requiring_smart_decision:
                        return True
    
    return False

# ============================ ×¤×•× ×§×¦×™×” ×¤×©×•×˜×” ×œ××™×–×•×’ × ×ª×•× ×™× ============================

def simple_merge_profile(existing_profile, new_data):
    """
    ×¤×•× ×§×¦×™×” ×¤×©×•×˜×” ×œ××™×–×•×’ × ×ª×•× ×™× ×œ×œ× ×©×™××•×© ×‘-gpt_d.
    ××˜×¤×œ×ª ×‘×©×“×•×ª ×—×“×©×™× ×•×‘×©×“×•×ª ×©×œ× ×“×•×¨×©×™× ×”×—×œ×˜×” ×—×›××”.
    """
    print(f"[DEBUG][simple_merge_profile] Simple merge without gpt_d")
    
    if not existing_profile:
        return new_data
    
    if not new_data:
        return existing_profile
    
    # ××™×–×•×’ ×¤×©×•×˜ - ×”×¢×¨×›×™× ×”×—×“×©×™× ×“×•×¨×¡×™× ××ª ×”×™×©× ×™×
    merged_profile = {**existing_profile, **new_data}
    
    # × ×™×§×•×™ ×¢×¨×›×™× ×¨×™×§×™× ××• ×œ× ×ª×§×™× ×™×
    cleaned_profile = {}
    for key, value in merged_profile.items():
        if value and value != "null" and str(value).strip():
            cleaned_profile[key] = value
    
    print(f"[DEBUG][simple_merge_profile] Final merged profile: {cleaned_profile}")
    return cleaned_profile

# ============================ ×¤×•× ×§×¦×™×” ×œ×”×¢×¨×›×ª ××™×›×•×ª × ×ª×•× ×™× ============================

def assess_data_quality(value, field_type="general"):
    """
    ××¢×¨×™×š ××ª ××™×›×•×ª ×”× ×ª×•× ×™× ×‘×©×“×” ××¡×•×™×.
    
    ×§×œ×˜:
    - value: ×”×¢×¨×š ×œ×‘×“×™×§×”
    - field_type: ×¡×•×’ ×”×©×“×” (age, occupation, etc.)
    
    ×¤×œ×˜:
    - dict ×¢× ×”×¢×¨×›×ª ××™×›×•×ª
    """
    if not value or value == "null" or str(value).strip() == "":
        return {
            "quality_score": 0,
            "is_valid": False,
            "issues": ["×¢×¨×š ×¨×™×§ ××• ×œ× ×ª×§×™×Ÿ"],
            "recommendation": "×™×© ×œ××œ× ×¢×¨×š ×ª×§×™×Ÿ"
        }
    
    value_str = str(value).strip()
    
    # ×‘×“×™×§×•×ª ×¡×¤×¦×™×¤×™×•×ª ×œ×¤×™ ×¡×•×’ ×”×©×“×”
    if field_type == "age":
        try:
            age = int(value_str)
            if age < 0 or age > 120:
                return {
                    "quality_score": 0,
                    "is_valid": False,
                    "issues": [f"×’×™×œ ×œ× ×”×’×™×•× ×™: {age}"],
                    "recommendation": "×’×™×œ ×¦×¨×™×š ×œ×”×™×•×ª ×‘×™×Ÿ 0 ×œ-120"
                }
            elif age < 13 or age > 100:
                return {
                    "quality_score": 0.5,
                    "is_valid": True,
                    "issues": [f"×’×™×œ ×—×¨×™×’: {age}"],
                    "recommendation": "×‘×“×•×§ ×× ×”×’×™×œ × ×›×•×Ÿ"
                }
            else:
                return {
                    "quality_score": 1.0,
                    "is_valid": True,
                    "issues": [],
                    "recommendation": "×’×™×œ ×ª×§×™×Ÿ"
                }
        except ValueError:
            return {
                "quality_score": 0,
                "is_valid": False,
                "issues": ["×’×™×œ ××™× ×• ××¡×¤×¨"],
                "recommendation": "×’×™×œ ×¦×¨×™×š ×œ×”×™×•×ª ××¡×¤×¨"
            }
    
    elif field_type == "occupation_or_role":
        if len(value_str) < 2:
            return {
                "quality_score": 0.3,
                "is_valid": True,
                "issues": ["×ª×™××•×¨ ××§×¦×•×¢ ×§×¦×¨ ××“×™"],
                "recommendation": "×”×•×¡×£ ×ª×™××•×¨ ××§×¦×•×¢ ××¤×•×¨×˜ ×™×•×ª×¨"
            }
        elif len(value_str) > 100:
            return {
                "quality_score": 0.7,
                "is_valid": True,
                "issues": ["×ª×™××•×¨ ××§×¦×•×¢ ××¨×•×š ××“×™"],
                "recommendation": "×§×¦×¨ ××ª ×”×ª×™××•×¨"
            }
        else:
            return {
                "quality_score": 1.0,
                "is_valid": True,
                "issues": [],
                "recommendation": "×ª×™××•×¨ ××§×¦×•×¢ ×ª×§×™×Ÿ"
            }
    
    else:
        # ×‘×“×™×§×” ×›×œ×œ×™×ª
        if len(value_str) < 1:
            return {
                "quality_score": 0,
                "is_valid": False,
                "issues": ["×¢×¨×š ×¨×™×§"],
                "recommendation": "×™×© ×œ××œ× ×¢×¨×š"
            }
        elif len(value_str) > 200:
            return {
                "quality_score": 0.5,
                "is_valid": True,
                "issues": ["×¢×¨×š ××¨×•×š ××“×™"],
                "recommendation": "×§×¦×¨ ××ª ×”×¢×¨×š"
            }
        else:
            return {
                "quality_score": 1.0,
                "is_valid": True,
                "issues": [],
                "recommendation": "×¢×¨×š ×ª×§×™×Ÿ"
            }

def smart_update_profile_with_gpt_d(existing_profile, user_message, interaction_id=None):
    """
    ××¢×“×›×Ÿ ×ª×¢×•×“×ª ×–×”×•×ª ×¨×’×©×™×ª ×©×œ ××©×ª××© ×¢× ×©×™××•×© ×‘-gpt_d ×œ×˜×™×¤×•×œ ×‘×©×“×•×ª ×©×”×©×ª× ×•.
    """
    print(f"[DEBUG][smart_update_profile_with_gpt_d] - interaction_id: {interaction_id}")
    try:
        # ×§×•×“× ×›×œ ××—×œ×¥ ××™×“×¢ ×—×“×©
        gpt_c_response = gpt_c(
            user_message=user_message,
            chat_id=interaction_id
        )

        if not gpt_c_response or not gpt_c_response.get("full_data"):
            return existing_profile, {}

        new_data = gpt_c_response.get("full_data", {})
        extract_usage = {k: v for k, v in gpt_c_response.items() if k not in ["updated_summary", "full_data"]}

        if not new_data:
            return existing_profile, extract_usage

        # ×‘×•×“×§ ×× ×¦×¨×™×š ×œ×”×¤×¢×™×œ ××ª gpt_d
        if should_activate_gpt_d(existing_profile, new_data):
            print(f"[DEBUG][smart_update_profile_with_gpt_d] Activating gpt_d for smart field resolution")
            
            # ××–×”×” ×©×“×•×ª ×©×”×©×ª× ×• (×¨×§ ×›××œ×” ×©×”×™×©×Ÿ ×”×™×” ××œ×)
            changed_fields = identify_changed_fields(existing_profile, new_data)
            print(f"[DEBUG][smart_update_profile_with_gpt_d] changed_fields: {changed_fields}")
            
            # ××©×ª××© ×‘-gpt_d ×œ×”×—×œ×˜×” ×¢×œ ×”×©×“×•×ª ×©×”×©×ª× ×•
            gpt_d_response = gpt_d(
                changed_fields=changed_fields,
                chat_id=interaction_id
            )
            
            if not gpt_d_response:
                print(f"[DEBUG][smart_update_profile_with_gpt_d] gpt_d failed, using simple merge")
                updated_profile = simple_merge_profile(existing_profile, new_data)
                return updated_profile, extract_usage
            
            final_values = gpt_d_response.get("final_values", {})
            gpt_d_usage = {k: v for k, v in gpt_d_response.items() if k != "final_values"}
            
            # ××¢×“×›×Ÿ ××ª ×”×¤×¨×•×¤×™×œ ×¢× ×”×¢×¨×›×™× ×”×¡×•×¤×™×™× ×-gpt_d + ××™×–×•×’ ×¤×©×•×˜ ×œ×©××¨
            updated_profile = simple_merge_profile(existing_profile, new_data)
            updated_profile.update(final_values)  # ×“×•×¨×¡ ×¢× ×”×—×œ×˜×•×ª gpt_d
            
            # ××©×œ×‘ ××ª ×”× ×ª×•× ×™× ×¢×œ ×”×©×™××•×©
            combined_usage = {**extract_usage, **gpt_d_usage}
            
            print(f"[DEBUG][smart_update_profile_with_gpt_d] final updated_profile: {updated_profile}")
            return updated_profile, combined_usage
        else:
            print(f"[DEBUG][smart_update_profile_with_gpt_d] No need for gpt_d, using simple merge")
            # ×× ××™×Ÿ ×¦×•×¨×š ×‘-gpt_d, ×¤×©×•×˜ ××©×œ×‘ ××ª ×”× ×ª×•× ×™× ×”×—×“×©×™×
            updated_profile = simple_merge_profile(existing_profile, new_data)
            return updated_profile, extract_usage

    except Exception as e:
        logging.error(f"âŒ ×©×’×™××” ×‘-smart_update_profile_with_gpt_d: {e}")
        return existing_profile, {}

def smart_update_profile_with_gpt_d_async(existing_profile, user_message, interaction_id=None):
    loop = asyncio.get_event_loop()
    return loop.run_in_executor(None, smart_update_profile_with_gpt_d, existing_profile, user_message, interaction_id)

def run_gpt_d_demo():
    """
    ×”×¨×¦×ª ×”×“×’××” ××œ××” ×©×œ gpt_d.
    """
    print("ğŸš€ ×”×¨×¦×ª ×”×“×’××” ××œ××” ×©×œ gpt_d...")
    print("=" * 60)
    
    # 1. ×”×“×’××ª ×™×¢×™×œ×•×ª
    print("\nğŸ“Š ×”×“×’××ª ×™×¢×™×œ×•×ª gpt_d:")
    demonstrate_gpt_d_efficiency()
    
    # 2. ×“×•×’×××•×ª ×©×™××•×©
    print("\nğŸ“š ×“×•×’×××•×ª ×©×™××•×© ××¢×©×™×•×ª:")
    gpt_d_usage_examples()
    
    # 3. ×‘×“×™×§×•×ª ××™× ×˜×’×¨×¦×™×”
    print("\nğŸ§ª ×‘×“×™×§×•×ª ××™× ×˜×’×¨×¦×™×”:")
    test_gpt_d_integration()
    
    # 4. ×“×•×’××ª ××™× ×˜×’×¨×¦×™×” ××œ××”
    print("\nğŸ”— ×“×•×’××ª ××™× ×˜×’×¨×¦×™×” ××œ××”:")
    gpt_d_integration_example()
    
    print("\n" + "=" * 60)
    print("âœ… ×”×“×’××” ××œ××” ×”×•×©×œ××”!")
    print("\nğŸ“ ×¡×™×›×•×:")
    print("â€¢ gpt_d ××•×¤×¢×œ ×¨×§ ×›×©×”×©×“×” ×”×™×©×Ÿ ×”×™×” ××œ× ×•×™×© ×¦×•×¨×š ×‘×”×—×œ×˜×” ×—×›××”")
    print("â€¢ ×©×“×•×ª ×—×“×©×™× (×”×™×©×Ÿ ×¨×™×§) ××˜×•×¤×œ×™× ×‘××™×–×•×’ ×¤×©×•×˜")
    print("â€¢ ×¢×œ×•×ª × ××•×›×”: ~$0.0001-0.001 ×œ×›×œ ×¤×¢×•×œ×”")
    print("â€¢ ××”×™×¨×•×ª ×’×‘×•×”×”: ~1-2 ×©× ×™×•×ª")
    print("â€¢ ××™×›×•×ª ×”×—×œ×˜×•×ª ×’×‘×•×”×” ×¢× gpt-4o")

def gpt_d_integration_example():
    """
    ×“×•×’××” ××œ××” ×©×œ ××™× ×˜×’×¨×¦×™×” ×¢× gpt_d.
    """
    print("ğŸ”— ×“×•×’××ª ××™× ×˜×’×¨×¦×™×” ××œ××” ×¢× gpt_d...")
    
    # ×ª×¨×—×™×©: ××©×ª××© ××¢×“×›×Ÿ ××ª ×”×¤×¨×•×¤×™×œ ×©×œ×•
    existing_profile = {
        "age": "25",
        "occupation_or_role": "×¡×˜×•×“× ×˜",
        "interests": "××•×–×™×§×”, ×§×¨×™××”"
    }
    
    new_data = {
        "age": "26",
        "occupation_or_role": "××¤×ª×— ×ª×•×›× ×”",
        "personality_traits": "×™×¦×™×¨×ª×™, ×—×‘×¨×•×ª×™"
    }
    
    print(f"×¤×¨×•×¤×™×œ ×§×™×™×: {existing_profile}")
    print(f"× ×ª×•× ×™× ×—×“×©×™×: {new_data}")
    
    # ×©×™××•×© ×‘×¤×•× ×§×¦×™×” ×”×—×›××”
    updated_profile, usage_stats = smart_update_profile_with_gpt_d(
        existing_profile=existing_profile,
        new_data=new_data,
        interaction_id="demo_user_123"
    )
    
    print(f"\n×¤×¨×•×¤×™×œ ××¢×•×“×›×Ÿ: {updated_profile}")
    print(f"×¡×˜×˜×™×¡×˜×™×§×•×ª ×©×™××•×©: {usage_stats}")
    
    # × ×™×ª×•×— ×”×ª×•×¦××”
    print(f"\nğŸ“Š × ×™×ª×•×— ×”×ª×•×¦××”:")
    
    # ×‘×“×™×§×” ××™×œ×• ×©×“×•×ª ×”×©×ª× ×•
    changed_fields = identify_changed_fields(existing_profile, new_data)
    print(f"×©×“×•×ª ×©×”×©×ª× ×• (×”×™×©×Ÿ ××œ×): {changed_fields}")
    
    # ×‘×“×™×§×” ××™×œ×• ×©×“×•×ª ×—×“×©×™×
    new_fields = {k: v for k, v in new_data.items() if k not in existing_profile or not existing_profile.get(k)}
    print(f"×©×“×•×ª ×—×“×©×™× (×”×™×©×Ÿ ×¨×™×§): {new_fields}")
    
    # ×”×¢×¨×›×ª ×¢×œ×•×ª
    cost_estimate = estimate_gpt_d_cost(existing_profile, new_data)
    print(f"×¢×œ×•×ª ××©×•×¢×¨×ª: ${cost_estimate['estimated_cost_usd']:.6f}")
    
    print("\nâœ… ×“×•×’××ª ×”××™× ×˜×’×¨×¦×™×” ×”×•×©×œ××”!")

# ============================ ×¤×•× ×§×¦×™×” ×œ×‘×“×™×§×” ××”×™×¨×” ×©×œ gpt_d ============================

def quick_gpt_d_test():
    """
    ×‘×“×™×§×” ××”×™×¨×” ×©×œ gpt_d ×¢× ×“×•×’×××•×ª ×¤×©×•×˜×•×ª.
    """
    print("âš¡ ×‘×“×™×§×” ××”×™×¨×” ×©×œ gpt_d...")
    
    # ×“×•×’××” ×¤×©×•×˜×” - ×’×™×œ
    test_fields = {
        "age": {"old": "25", "new": "26"}
    }
    
    print(f"ğŸ“ ×‘×“×™×§×”: {test_fields}")
    result = gpt_d(test_fields, chat_id="quick_test")
    
    if result:
        print(f"âœ… ×ª×•×¦××”: {result.get('final_values', {})}")
        print(f"ğŸ’° ×¢×œ×•×ª: ${result.get('cost_total', 0):.6f}")
        print(f"ğŸ”¢ ×˜×•×§× ×™×: {result.get('total_tokens', 0)}")
    else:
        print("âŒ ×©×’×™××” ×‘×‘×“×™×§×”")
    
    print("ğŸ‰ ×‘×“×™×§×” ××”×™×¨×” ×”×•×©×œ××”!")

# ============================ ×¤×•× ×§×¦×™×” ×œ×”×¤×¢×œ×ª ×›×œ ×”×‘×“×™×§×•×ª ============================

def run_all_gpt_d_tests():
    """
    ××¤×¢×™×œ ××ª ×›×œ ×”×‘×“×™×§×•×ª ×©×œ gpt_d.
    """
    print("ğŸ§ª ×”×¤×¢×œ×ª ×›×œ ×‘×“×™×§×•×ª gpt_d...")
    
    # ×¡×™×›×•× ×ª×›×•× ×•×ª
    gpt_d_summary()
    
    # × ×™×ª×•×— ×¢×œ×•×™×•×ª
    analyze_gpt_d_costs()
    
    # ×”×“×’××ª ×™×¢×™×œ×•×ª
    demonstrate_gpt_d_efficiency()
    
    # ×‘×“×™×§×” ××”×™×¨×”
    quick_gpt_d_test()
    
    # ×“×•×’×××•×ª ×©×™××•×© ××¢×©×™×•×ª
    gpt_d_usage_examples()
    
    # ×‘×“×™×§×ª ×“×•×’×××•×ª
    test_gpt_d_examples()
    
    # ×”×“×’××ª ×©×™××•×© ××¢×©×™
    demonstrate_gpt_d_usage()
    
    print("\nğŸ‰ ×›×œ ×”×‘×“×™×§×•×ª ×”×•×©×œ××•!")

# ============================ ×× ×”×§×•×‘×¥ ×¨×¥ ×™×©×™×¨×•×ª ============================

if __name__ == "__main__":
    print("ğŸš€ ×”×¤×¢×œ×ª ×‘×“×™×§×•×ª gpt_d...")
    run_all_gpt_d_tests()

# ============================ ×¤×•× ×§×¦×™×” ×œ× ×™×ª×•×— ×¢×œ×•×™×•×ª ×©×œ gpt_d ============================

def analyze_gpt_d_costs():
    """
    ×× ×ª×— ××ª ×”×¢×œ×•×™×•×ª ×©×œ gpt_d ×•××©×•×•×” ×œ××™×–×•×’ ×¤×©×•×˜.
    """
    print("ğŸ’° × ×™×ª×•×— ×¢×œ×•×™×•×ª gpt_d...")
    
    # ×“×•×’×××•×ª ×©×•× ×•×ª ×¢× ××¡×¤×¨ ×©×“×•×ª ×©×•× ×”
    scenarios = [
        {"name": "×©×“×” ××—×“", "existing": {"age": "25"}, "new": {"age": "26"}},
        {"name": "×©× ×™ ×©×“×•×ª", "existing": {"age": "25", "occupation": "×¡×˜×•×“× ×˜"}, "new": {"age": "26", "occupation": "××”× ×“×¡"}},
        {"name": "×©×œ×•×©×” ×©×“×•×ª", "existing": {"age": "25", "occupation": "×¡×˜×•×“× ×˜", "interests": "××•×–×™×§×”"}, "new": {"age": "26", "occupation": "××”× ×“×¡", "interests": "×¡×¤×•×¨×˜"}},
        {"name": "×—××™×©×” ×©×“×•×ª", "existing": {"age": "25", "occupation": "×¡×˜×•×“× ×˜", "interests": "××•×–×™×§×”", "personality": "×©×§×˜", "location": "×ª×œ ××‘×™×‘"}, "new": {"age": "26", "occupation": "××”× ×“×¡", "interests": "×¡×¤×•×¨×˜", "personality": "×—×‘×¨×•×ª×™", "location": "×™×¨×•×©×œ×™×"}}
    ]
    
    print("\nğŸ“Š ×”×©×•×•××ª ×¢×œ×•×™×•×ª:")
    print("=" * 60)
    print(f"{'×ª×¨×—×™×©':<15} {'×©×“×•×ª':<8} {'×¢×œ×•×ª ××©×•×¢×¨×ª':<15} {'×˜×•×§× ×™× ××©×•×¢×¨×™×':<20}")
    print("=" * 60)
    
    for scenario in scenarios:
        cost_estimate = estimate_gpt_d_cost(scenario["existing"], scenario["new"])
        print(f"{scenario['name']:<15} {cost_estimate['changed_fields_count']:<8} ${cost_estimate['estimated_cost_usd']:<14.6f} {cost_estimate['input_tokens'] + cost_estimate['output_tokens']:<20}")
    
    print("=" * 60)
    print("ğŸ’¡ ×”×¢×¨×•×ª:")
    print("- ×¢×œ×•×™×•×ª ××‘×•×¡×¡×•×ª ×¢×œ ××•×“×œ gpt-4o")
    print("- ××™×–×•×’ ×¤×©×•×˜: ×¢×œ×•×ª 0 (×œ×œ× ×§×¨×™××” ×œ-API)")
    print("- gpt_d: ×¢×œ×•×ª × ××•×›×” ×××•×“ ×œ×“×™×•×§ ×’×‘×•×”")
    print("- ××•××œ×¥ ×œ×”×©×ª××© ×‘-gpt_d ×¨×§ ×›×©× ×“×¨×©×ª ×”×—×œ×˜×” ×—×›××”")

# ============================ ×¡×™×›×•× ×ª×›×•× ×•×ª gpt_d ============================

def gpt_d_summary():
    """
    ××¦×™×’ ×¡×™×›×•× ×©×œ ×ª×›×•× ×•×ª gpt_d.
    """
    print("ğŸ“‹ ×¡×™×›×•× ×ª×›×•× ×•×ª gpt_d:")
    print("=" * 50)
    
    features = [
        "ğŸ¤– ××•×“×œ ×—×›× ×œ×”×—×œ×˜×•×ª ×¢×œ ×©×“×•×ª ×©×”×©×ª× ×•",
        "ğŸ’° ×—×¡×›×•× ×™ - ××§×‘×œ ×¨×§ ×©×“×•×ª ×¨×œ×•×•× ×˜×™×™×",
        "âš¡ ××”×™×¨ - ××©×ª××© ×‘××•×“×œ gpt-4o",
        "ğŸ¯ ××“×•×™×§ - ×›×•×œ×œ ×”×¢×¨×›×ª ××™×›×•×ª × ×ª×•× ×™×",
        "ğŸ“Š ×× ×•×˜×¨ - ××¢×§×‘ ×¢×œ ×‘×™×¦×•×¢×™× ×•×¢×œ×•×™×•×ª",
        "ğŸ”„ ×—×›× - ×‘×•×“×§ ×× ×¦×¨×™×š ×œ×”×¤×¢×™×œ ××• ×œ×",
        "ğŸ›¡ï¸ ×‘×˜×•×— - ×’×™×‘×•×™ ×œ××™×–×•×’ ×¤×©×•×˜",
        "ğŸ“ˆ ××ª×¤×ª×— - ×œ×•××“ ××”×©×™××•×©"
    ]
    
    for feature in features:
        print(f"  {feature}")
    
    print("\nğŸ”§ ×¤×•× ×§×¦×™×•×ª ×¢×™×§×¨×™×•×ª:")
    print("  - gpt_d() - ×”×¤×•× ×§×¦×™×” ×”×¨××©×™×ª")
    print("  - should_activate_gpt_d() - ×‘×“×™×§×” ×× ×¦×¨×™×š ×œ×”×¤×¢×™×œ")
    print("  - identify_changed_fields() - ×–×™×”×•×™ ×©×“×•×ª ×©×”×©×ª× ×•")
    print("  - assess_data_quality() - ×”×¢×¨×›×ª ××™×›×•×ª × ×ª×•× ×™×")
    print("  - smart_update_profile_with_gpt_d() - ×¢×“×›×•×Ÿ ×—×›×")
    
    print("\nğŸ’¡ ××ª×™ ×œ×”×©×ª××©:")
    print("  - ×›×©×”×©×ª× ×• ×©×“×•×ª ×—×©×•×‘×™× (×’×™×œ, ××§×¦×•×¢, ×•×›×•')")
    print("  - ×›×©×™×© ×¡×ª×™×¨×” ×‘×™×Ÿ × ×ª×•× ×™× ×™×©× ×™× ×•×—×“×©×™×")
    print("  - ×›×©× ×“×¨×©×ª ×”×—×œ×˜×” ×—×›××” ×¢×œ ××™×›×•×ª ×”× ×ª×•× ×™×")
    
    print("\nâŒ ××ª×™ ×œ× ×œ×”×©×ª××©:")
    print("  - ×›×©×”×©×ª× ×• ×¨×§ ×©×“×•×ª ×œ× ×—×©×•×‘×™×")
    print("  - ×›×©××™×Ÿ ×¡×ª×™×¨×” ×‘×™×Ÿ ×”× ×ª×•× ×™×")
    print("  - ×›×©××™×–×•×’ ×¤×©×•×˜ ××¡×¤×™×§")

# ============================ ×“×•×’×××•×ª ×©×™××•×© ××¢×©×™×•×ª ×‘-gpt_d ============================

def gpt_d_usage_examples():
    """
    ×“×•×’×××•×ª ×©×™××•×© ××¢×©×™×•×ª ×‘-gpt_d.
    """
    print("ğŸ“š ×“×•×’×××•×ª ×©×™××•×© ××¢×©×™×•×ª ×‘-gpt_d...")
    
    # ×“×•×’××” 1: ×¢×“×›×•×Ÿ ×’×™×œ ×¤×©×•×˜ (×”×™×©×Ÿ ×”×™×” ××œ×)
    print("\nğŸ”¹ ×“×•×’××” 1: ×¢×“×›×•×Ÿ ×’×™×œ (×”×™×©×Ÿ ×”×™×” ××œ×)")
    existing_profile = {"age": "25", "occupation_or_role": "×¡×˜×•×“× ×˜"}
    new_data = {"age": "26"}
    
    # ×‘×“×™×§×” ×× ×¦×¨×™×š gpt_d
    if should_activate_gpt_d(existing_profile, new_data):
        changed_fields = identify_changed_fields(existing_profile, new_data)
        result = gpt_d(changed_fields, chat_id="example_1")