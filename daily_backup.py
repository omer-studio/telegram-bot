#!/usr/bin/env python3
"""
ğŸ”’ ××¢×¨×›×ª ×’×™×‘×•×™ ××•×˜×•××˜×™ ×™×•××™ ×œ×˜×‘×œ××•×ª ×§×¨×™×˜×™×•×ª
×”×¨×¥ ×™×•××™×ª ×‘-3:00 ×‘×œ×™×œ×” ×•×©×•××¨ ×’×™×‘×•×™×™×
"""

import os
import json
import psycopg2
from datetime import datetime, timedelta
from config import config
from simple_logger import logger

DB_URL = config.get("DATABASE_EXTERNAL_URL") or config.get("DATABASE_URL")
BACKUP_DIR = "backups/daily_db_backups"

# ğŸ”’ ×˜×‘×œ××•×ª ×§×¨×™×˜×™×•×ª ×©×—×•×‘×” ×œ×’×‘×•×ª
CRITICAL_TABLES = [
    "user_profiles",     # ×”×œ×‘ ×©×œ ×”××¢×¨×›×ª - ×§×•×“×™ ××™×©×•×¨ ×•××©×ª××©×™×
    "chat_messages",     # ×›×œ ×”×™×¡×˜×•×¨×™×™×ª ×”×©×™×—×•×ª
    "gpt_calls_log"      # ×›×œ ×”×§×¨×™××•×ª ×•×”×¢×œ×•×™×•×ª
]

def ensure_backup_dir():
    """×™×•×¦×¨ ×ª×™×§×™×™×ª ×’×™×‘×•×™×™× ×× ×œ× ×§×™×™××ª"""
    os.makedirs(BACKUP_DIR, exist_ok=True)

def backup_table_to_json(table_name, backup_date):
    """××’×‘×” ×˜×‘×œ×” ×™×—×™×“×” ×œ×§×•×‘×¥ JSON"""
    try:
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()
        
        # ×©×œ×™×¤×ª ×›×œ ×”× ×ª×•× ×™× ××”×˜×‘×œ×”
        cur.execute(f"SELECT * FROM {table_name}")
        rows = cur.fetchall()
        
        # ×©×œ×™×¤×ª ×©××•×ª ×”×¢××•×“×•×ª
        cur.execute(f"""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = '{table_name}' 
            ORDER BY ordinal_position
        """)
        columns = [row[0] for row in cur.fetchall()]
        
        # ×”××¨×” ×œ-JSON
        data = []
        for row in rows:
            row_dict = {}
            for i, col in enumerate(columns):
                value = row[i]
                # ×”××¨×ª datetime ×œstring
                if isinstance(value, datetime):
                    value = value.isoformat()
                row_dict[col] = value
            data.append(row_dict)
        
        # ×©××™×¨×ª ×”×’×™×‘×•×™
        backup_file = f"{BACKUP_DIR}/{table_name}_{backup_date}.json"
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        
        cur.close()
        conn.close()
        
        logger.info(f"âœ… ×’×™×‘×•×™ {table_name}: {len(data)} ×©×•×¨×•×ª × ×©××¨×• ×œ-{backup_file}")
        return len(data)
        
    except Exception as e:
        logger.error(f"âŒ ×©×’×™××” ×‘×’×™×‘×•×™ {table_name}: {e}")
        return 0

def create_backup_summary(backup_date, results):
    """×™×•×¦×¨ ×§×•×‘×¥ ×¡×™×›×•× ×œ×’×™×‘×•×™"""
    summary = {
        "backup_date": backup_date,
        "backup_time": datetime.now().isoformat(),
        "tables_backed_up": len(results),
        "total_rows": sum(results.values()),
        "results": results,
        "backup_location": BACKUP_DIR
    }
    
    summary_file = f"{BACKUP_DIR}/backup_summary_{backup_date}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    return summary

def cleanup_old_backups(days_to_keep=30):
    """××—×™×§×ª ×’×™×‘×•×™×™× ×™×©× ×™× (×©×•××¨ ×¨×§ 30 ×™××™× ××—×¨×•× ×™×)"""
    try:
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        
        for filename in os.listdir(BACKUP_DIR):
            file_path = os.path.join(BACKUP_DIR, filename)
            if os.path.isfile(file_path):
                # ×‘×“×™×§×ª ×ª××¨×™×š ×”×§×•×‘×¥
                file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                if file_time < cutoff_date:
                    os.remove(file_path)
                    logger.info(f"ğŸ—‘ï¸ × ××—×§ ×’×™×‘×•×™ ×™×©×Ÿ: {filename}")
                    
    except Exception as e:
        logger.error(f"âŒ ×©×’×™××” ×‘× ×™×§×•×™ ×’×™×‘×•×™×™× ×™×©× ×™×: {e}")

def run_daily_backup():
    """××¨×™×¥ ×’×™×‘×•×™ ×™×•××™ ××œ×"""
    try:
        logger.info("ğŸ”’ ××ª×—×™×œ ×’×™×‘×•×™ ×™×•××™ ×©×œ ×˜×‘×œ××•×ª ×§×¨×™×˜×™×•×ª")
        
        # ×”×›× ×ª ×ª×™×§×™×™×ª ×’×™×‘×•×™×™×
        ensure_backup_dir()
        
        # ×ª××¨×™×š ×œ×©× ×”×§×•×‘×¥
        backup_date = datetime.now().strftime("%Y%m%d")
        
        # ×’×™×‘×•×™ ×›×œ ×˜×‘×œ×” ×§×¨×™×˜×™×ª
        backup_results = {}
        for table in CRITICAL_TABLES:
            rows_backed_up = backup_table_to_json(table, backup_date)
            backup_results[table] = rows_backed_up
        
        # ×™×¦×™×¨×ª ×¡×™×›×•×
        summary = create_backup_summary(backup_date, backup_results)
        
        # × ×™×§×•×™ ×’×™×‘×•×™×™× ×™×©× ×™×
        cleanup_old_backups()
        
        logger.info(f"âœ… ×’×™×‘×•×™ ×™×•××™ ×”×•×©×œ×: {summary['total_rows']} ×©×•×¨×•×ª × ×©××¨×•")
        print(f"âœ… ×’×™×‘×•×™ ×™×•××™ ×”×•×©×œ×: {summary['total_rows']} ×©×•×¨×•×ª × ×©××¨×•")
        
        return summary
        
    except Exception as e:
        logger.error(f"âŒ ×©×’×™××” ×‘×’×™×‘×•×™ ×™×•××™: {e}")
        print(f"âŒ ×©×’×™××” ×‘×’×™×‘×•×™ ×™×•××™: {e}")
        return None

def restore_table_from_backup(table_name, backup_date):
    """×©×—×–×•×¨ ×˜×‘×œ×” ××’×™×‘×•×™ (×œ××§×¨×™ ×—×™×¨×•×)"""
    try:
        backup_file = f"{BACKUP_DIR}/{table_name}_{backup_date}.json"
        
        if not os.path.exists(backup_file):
            print(f"âŒ ×§×•×‘×¥ ×’×™×‘×•×™ ×œ× × ××¦×: {backup_file}")
            return False
        
        # ×§×¨×™××ª ×”×’×™×‘×•×™
        with open(backup_file, 'r', encoding='utf-8') as f:
            backup_data = json.load(f)
        
        print(f"âš ï¸  ×”×× ××ª×” ×‘×˜×•×— ×©××ª×” ×¨×•×¦×” ×œ×©×—×–×¨ {table_name} ×-{backup_date}?")
        print(f"âš ï¸  ×–×” ×™××—×§ ××ª ×›×œ ×”× ×ª×•× ×™× ×”× ×•×›×—×™×™× ×‘×˜×‘×œ×”!")
        print(f"ğŸ“Š ×’×™×‘×•×™ ××›×™×œ: {len(backup_data)} ×©×•×¨×•×ª")
        
        confirm = input("×”×§×œ×“ 'YES' ×›×“×™ ×œ××©×¨: ")
        if confirm != "YES":
            print("âŒ ×©×—×–×•×¨ ×‘×•×˜×œ")
            return False
        
        # ×‘×™×¦×•×¢ ×”×©×—×–×•×¨
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()
        
        # ××—×™×§×ª × ×ª×•× ×™× × ×•×›×—×™×™×
        cur.execute(f"DELETE FROM {table_name}")
        
        # ×©×—×–×•×¨ ×”× ×ª×•× ×™×
        if backup_data:
            columns = list(backup_data[0].keys())
            placeholders = ', '.join(['%s'] * len(columns))
            
            insert_sql = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})"
            
            for row in backup_data:
                values = [row[col] for col in columns]
                cur.execute(insert_sql, values)
        
        conn.commit()
        cur.close()
        conn.close()
        
        print(f"âœ… ×©×—×–×•×¨ ×”×•×©×œ×: {len(backup_data)} ×©×•×¨×•×ª")
        logger.info(f"âœ… ×©×—×–×•×¨ {table_name} ×-{backup_date}: {len(backup_data)} ×©×•×¨×•×ª")
        
        return True
        
    except Exception as e:
        print(f"âŒ ×©×’×™××” ×‘×©×—×–×•×¨: {e}")
        logger.error(f"âŒ ×©×’×™××” ×‘×©×—×–×•×¨ {table_name}: {e}")
        return False

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "restore":
        if len(sys.argv) < 4:
            print("×©×™××•×©: python daily_backup.py restore <table_name> <backup_date>")
            print("×“×•×’××”: python daily_backup.py restore user_profiles 20250109")
        else:
            table_name = sys.argv[2]
            backup_date = sys.argv[3]
            restore_table_from_backup(table_name, backup_date)
    else:
        # ×’×™×‘×•×™ ×¨×’×™×œ
        run_daily_backup() 