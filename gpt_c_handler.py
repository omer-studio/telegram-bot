"""
gpt_c_handler.py
----------------
×× ×•×¢ gpt_c: ×—×™×œ×•×¥ ××™×“×¢ ××”×•×“×¢×•×ª ××©×ª××© ×œ×¢×“×›×•×Ÿ ×¤×¨×•×¤×™×œ
"""

from simple_logger import logger
from datetime import datetime
import json
import time
import lazy_litellm as litellm
import re
from prompts import build_profile_extraction_enhanced_prompt
from config import GPT_MODELS, GPT_PARAMS, GPT_FALLBACK_MODELS, should_log_data_extraction_debug, should_log_gpt_cost_debug
from gpt_utils import normalize_usage_dict, measure_llm_latency, calculate_gpt_cost, extract_json_from_text

def extract_user_info(user_msg, chat_id=None, message_id=None):
    """
    ××—×œ×¥ ××™×“×¢ ×¨×œ×•×•× ×˜×™ ××”×•×“×¢×ª ×”××©×ª××© ×œ×¢×“×›×•×Ÿ ×”×¤×¨×•×¤×™×œ ×©×œ×•
    ×›×•×œ×œ ××¢×¨×›×ª fallback ×œ××§×¨×” ×©×œ rate limit ×‘-Gemini.
    """
    start_time = time.time()  # ××“×™×“×ª ×–××Ÿ ×”×ª×—×œ×”
    
    # ×‘×“×™×§×” ×§×¨×™×˜×™×ª - ×× ××™×Ÿ chat_id ×ª×§×™×Ÿ, ×œ× ××¤×¢×™×œ×™× GPT-C
    if not chat_id:
        logger.error("[GPT_C] chat_id is None - skipping GPT-C execution")
        print("âŒ [GPT-C] chat_id is None - skipping GPT-C execution")
        return {"extracted_fields": {}, "usage": {}, "model": "none"}
    
    metadata = {"gpt_identifier": "gpt_c", "chat_id": chat_id, "message_id": message_id}
    params = GPT_PARAMS["gpt_c"]
    model = GPT_MODELS["gpt_c"]
    
    completion_params = {
        "model": model,
        "messages": [{"role": "system", "content": build_profile_extraction_enhanced_prompt()}, {"role": "user", "content": user_msg}],
        "temperature": params["temperature"],
        "metadata": metadata
    }
    
    # ×”×•×¡×¤×ª max_tokens ×¨×§ ×× ×”×•× ×œ× None
    if params["max_tokens"] is not None:
        completion_params["max_tokens"] = params["max_tokens"]
    
    # × ×™×¡×™×•×Ÿ ×¢× ×”××•×“×œ ×”×¨××©×™
    try:
        with measure_llm_latency(model):
            response = litellm.completion(**completion_params)
        content_raw = response.choices[0].message.content.strip()
        content = extract_json_from_text(content_raw)

        usage = normalize_usage_dict(response.usage, response.model)
        
        # × ×™×¡×™×•×Ÿ ×œ×¤×¨×¡ JSON ×¢× ×œ×•×’×™× ××¤×•×¨×˜×™×
        try:
            if content and content.strip():
                content_clean = content.strip()
                if content_clean.startswith("{") and content_clean.endswith("}"):
                    extracted_fields = json.loads(content_clean)
                    if not isinstance(extracted_fields, dict):
                        logger.warning(f"[GPT_C] JSON parsed but not a dict: {type(extracted_fields)}")
                        extracted_fields = {}
                else:
                    logger.warning(f"[GPT_C] Content doesn't look like JSON: {content_clean[:100]}...")
                    extracted_fields = {}
            else:
                extracted_fields = {}
        except json.JSONDecodeError as e:
            logger.error(f"[GPT_C] JSON decode error: {e} | Content: {content[:200] if content else 'None'}...")
            extracted_fields = {}
        except Exception as e:
            logger.error(f"[GPT_C] Unexpected error parsing JSON: {e} | Content: {content[:200] if content else 'None'}...")
            extracted_fields = {}
        
        # ×”×“×¤×¡×ª ××™×“×¢ ×—×©×•×‘ ×¢×œ ×—×™×œ×•×¥ × ×ª×•× ×™× (×ª××™×“ ×™×•×¤×™×¢!)
        print(f"ğŸ” [GPT-C] ×—×•×œ×¦×• {len(extracted_fields)} ×©×“×•×ª: {list(extracted_fields.keys())}")
        if should_log_gpt_cost_debug():
            print(f"ğŸ’° [GPT-C] ×¢×œ×•×ª: {usage.get('cost_total', 0):.6f}$ | ×˜×•×§× ×™×: {usage.get('total_tokens', 0)}")
        if should_log_data_extraction_debug():
            print(f"ğŸ“‹ [GPT-C] × ×ª×•× ×™× ×©×—×•×œ×¦×•: {json.dumps(extracted_fields, ensure_ascii=False, indent=2)}")
        
        try:
            cost_info = calculate_gpt_cost(
                prompt_tokens=usage.get("prompt_tokens", 0),
                completion_tokens=usage.get("completion_tokens", 0),
                cached_tokens=usage.get("cached_tokens", 0),
                model_name=response.model,
                completion_response=response
            )
            usage.update(cost_info)
        except Exception as _cost_e:
            logger.warning(f"[gpt_c] Cost calc failed: {_cost_e}")
        
        # ×—×™×©×•×‘ ×–××Ÿ ×¢×™×‘×•×“ ×˜×”×•×¨
        gpt_duration = time.time() - start_time
        
        # ğŸ’¾ ×©××™×¨×ª ××˜×¨×™×§×•×ª ×–××Ÿ GPT-C ×œ××¡×“ ×”× ×ª×•× ×™×
        try:
            from db_manager import save_system_metrics
            save_system_metrics(
                metric_type="gpt_timing",
                chat_id=str(chat_id) if chat_id else None,
                gpt_latency_seconds=gpt_duration,
                additional_data={
                    "message_id": message_id,
                    "gpt_type": "C",
                    "model": response.model,
                    "tokens_used": usage.get("total_tokens", 0),
                    "cost_usd": usage.get("cost_total", 0),
                    "operation": "extract_user_info",
                    "extracted_fields_count": len(extracted_fields)
                }
            )
        except Exception as save_err:
            logger.warning(f"Could not save GPT-C timing metrics: {save_err}")
        
        result = {"extracted_fields": extracted_fields, "usage": usage, "model": response.model}
        try:
            from gpt_jsonl_logger import GPTJSONLLogger
            # ×‘× ×™×™×ª response ××œ× ×¢× ×›×œ ×”××™×“×¢ ×”× ×“×¨×©
            response_data = {
                "id": getattr(response, "id", ""),
                "choices": [
                    {
                        "message": {
                            "content": content_raw,
                            "role": "assistant"
                        }
                    }
                ],
                "usage": usage,
                "model": response.model
            }
            GPTJSONLLogger.log_gpt_call(
                log_path="data/openai_calls.jsonl",
                gpt_type="C",
                request=completion_params,
                response=response_data,
                cost_usd=usage.get("cost_total", 0),
                extra={
                    "chat_id": chat_id, 
                    "message_id": message_id,
                    "gpt_pure_latency": time.time() - start_time,
                    "processing_time_seconds": time.time() - start_time
                }
            )
        except Exception as log_exc:
            print(f"[LOGGING_ERROR] Failed to log GPT-C call: {log_exc}")
        return result
        
    except Exception as e:
        error_str = str(e)
        is_rate_limit = "429" in error_str or "quota" in error_str.lower() or "rate limit" in error_str.lower()
        
        if is_rate_limit and "gpt_c" in GPT_FALLBACK_MODELS:
            # × ×™×¡×™×•×Ÿ ×¢× ××•×“×œ fallback
            fallback_model = GPT_FALLBACK_MODELS["gpt_c"]
            logger.warning(f"[gpt_c] Rate limit for {model}, trying fallback: {fallback_model}")
            
            try:
                completion_params["model"] = fallback_model
                completion_params["metadata"]["fallback_used"] = True
                
                with measure_llm_latency(fallback_model):
                    response = litellm.completion(**completion_params)
                content_raw = response.choices[0].message.content.strip()
                content = extract_json_from_text(content_raw)

                usage = normalize_usage_dict(response.usage, response.model)
                
                # × ×™×¡×™×•×Ÿ ×œ×¤×¨×¡ JSON ×¢× ×œ×•×’×™× ××¤×•×¨×˜×™× (fallback)
                try:
                    if content and content.strip():
                        content_clean = content.strip()
                        if content_clean.startswith("{") and content_clean.endswith("}"):
                            extracted_fields = json.loads(content_clean)
                            if not isinstance(extracted_fields, dict):
                                logger.warning(f"[GPT_C FALLBACK] JSON parsed but not a dict: {type(extracted_fields)}")
                                extracted_fields = {}
                        else:
                            logger.warning(f"[GPT_C FALLBACK] Content doesn't look like JSON: {content_clean[:100]}...")
                            extracted_fields = {}
                    else:
                        extracted_fields = {}
                except json.JSONDecodeError as e:
                    logger.error(f"[GPT_C FALLBACK] JSON decode error: {e} | Content: {content[:200] if content else 'None'}...")
                    extracted_fields = {}
                except Exception as e:
                    logger.error(f"[GPT_C FALLBACK] Unexpected error parsing JSON: {e} | Content: {content[:200] if content else 'None'}...")
                    extracted_fields = {}
                
                logger.info(f"[gpt_c] Fallback successful: {fallback_model}")
                
                # ×”×“×¤×¡×ª ××™×“×¢ ×—×©×•×‘ ×¢×œ ×—×™×œ×•×¥ × ×ª×•× ×™× (×ª××™×“ ×™×•×¤×™×¢!)
                print(f"ğŸ” [GPT-C FALLBACK] ×—×•×œ×¦×• {len(extracted_fields)} ×©×“×•×ª: {list(extracted_fields.keys())}")
                if should_log_gpt_cost_debug():
                    print(f"ğŸ’° [GPT-C FALLBACK] ×¢×œ×•×ª: {usage.get('cost_total', 0):.6f}$ | ×˜×•×§× ×™×: {usage.get('total_tokens', 0)}")
                if should_log_data_extraction_debug():
                    print(f"ğŸ“‹ [GPT-C FALLBACK] × ×ª×•× ×™× ×©×—×•×œ×¦×•: {json.dumps(extracted_fields, ensure_ascii=False, indent=2)}")
                
                try:
                    cost_info = calculate_gpt_cost(
                        prompt_tokens=usage.get("prompt_tokens", 0),
                        completion_tokens=usage.get("completion_tokens", 0),
                        cached_tokens=usage.get("cached_tokens", 0),
                        model_name=response.model,
                        completion_response=response
                    )
                    usage.update(cost_info)
                except Exception as _cost_e:
                    logger.warning(f"[gpt_c] Cost calc failed: {_cost_e}")
                
                result = {"extracted_fields": extracted_fields, "usage": usage, "model": response.model, "fallback_used": True}
                try:
                    from gpt_jsonl_logger import GPTJSONLLogger
                    # ×‘× ×™×™×ª response ××œ× ×¢× ×›×œ ×”××™×“×¢ ×”× ×“×¨×©
                    response_data = {
                        "id": getattr(response, "id", ""),
                        "choices": [
                            {
                                "message": {
                                    "content": content_raw,
                                    "role": "assistant"
                                }
                            }
                        ],
                        "usage": usage,
                        "model": response.model
                    }
                    GPTJSONLLogger.log_gpt_call(
                        log_path="data/openai_calls.jsonl",
                        gpt_type="C",
                        request=completion_params,
                        response=response_data,
                        cost_usd=usage.get("cost_total", 0),
                                                 extra={
                             "chat_id": chat_id, 
                             "message_id": message_id,
                             "gpt_pure_latency": time.time() - start_time,
                             "processing_time_seconds": time.time() - start_time
                         }
                    )
                except Exception as log_exc:
                    print(f"[LOGGING_ERROR] Failed to log GPT-C call: {log_exc}")
                return result
                
            except Exception as fallback_error:
                logger.error(f"[gpt_c] Fallback also failed: {fallback_error}")
                print(f"âŒ [GPT-C] ×©×’×™××” ×’× ×‘-fallback: {fallback_error}")
                result = {"extracted_fields": {}, "usage": {}, "model": fallback_model}
                try:
                    from gpt_jsonl_logger import GPTJSONLLogger
                    # ×‘× ×™×™×ª response ×¨×™×§ ×‘××§×¨×” ×©×’×™××”
                    response_data = {
                        "id": "",
                        "choices": [
                            {
                                "message": {
                                    "content": f"[×©×’×™××”: {fallback_error}]",
                                    "role": "assistant"
                                }
                            }
                        ],
                        "usage": {},
                        "model": fallback_model
                    }
                    GPTJSONLLogger.log_gpt_call(
                        log_path="data/openai_calls.jsonl",
                        gpt_type="C",
                        request=completion_params,
                        response=response_data,
                        cost_usd=0,
                        extra={
                            "chat_id": chat_id, 
                            "message_id": message_id,
                            "gpt_pure_latency": 0,
                            "processing_time_seconds": 0
                        }
                    )
                except Exception as log_exc:
                    print(f"[LOGGING_ERROR] Failed to log GPT-C call: {log_exc}")
                return result
        else:
            logger.error(f"[gpt_c] Error (not rate limit): {e}")
            print(f"âŒ [GPT-C] ×©×’×™××”: {e}")
            result = {"extracted_fields": {}, "usage": {}, "model": model}
            try:
                from gpt_jsonl_logger import GPTJSONLLogger
                # ×‘× ×™×™×ª response ×¨×™×§ ×‘××§×¨×” ×©×’×™××”
                response_data = {
                    "id": "",
                    "choices": [
                        {
                            "message": {
                                "content": f"[×©×’×™××”: {e}]",
                                "role": "assistant"
                            }
                        }
                    ],
                    "usage": {},
                    "model": model
                }
                GPTJSONLLogger.log_gpt_call(
                    log_path="data/openai_calls.jsonl",
                    gpt_type="C",
                    request=completion_params,
                    response=response_data,
                    cost_usd=0,
                    extra={
                        "chat_id": chat_id, 
                        "message_id": message_id,
                        "gpt_pure_latency": 0,
                        "processing_time_seconds": 0
                    }
                )
            except Exception as log_exc:
                print(f"[LOGGING_ERROR] Failed to log GPT-C call: {log_exc}")
            return result

def should_run_gpt_c(user_message):
    """
    ×‘×•×“×§ ×× ×™×© ×˜×¢× ×œ×”×¤×¢×™×œ gpt_c ×¢×œ ×”×•×“×¢×” × ×ª×•× ×”.
    ××—×–×™×¨ False ×¨×§ ×¢×œ ×”×•×“×¢×•×ª ×©×× ×—× ×• ×‘×˜×•×—×™× ×©×œ× ××›×™×œ×•×ª ××™×“×¢ ×—×“×©.
    ×”×›×œ×œ: gpt_c ××•×¤×¢×œ ×ª××™×“, ××œ× ×× ×›×Ÿ ×”×”×•×“×¢×” ×”×™× ××©×”×• ×©×œ× ×™×›×•×œ ×œ×”×›×™×œ ××™×“×¢ ×—×“×©.
    """
    if not user_message or not user_message.strip():
        return False
    message = user_message.strip()
    # ×‘×™×˜×•×™×™× ×‘×¡×™×¡×™×™× ×©×œ× ×™×›×•×œ×™× ×œ×”×›×™×œ ××™×“×¢ ×—×“×©
    base_phrases = [
        '×”×™×™', '×©×œ×•×', '××” ×©×œ×•××š', '××” × ×©××¢', '××” ×§×•×¨×”', '××” ×”××¦×‘',
        '×ª×•×“×”', '×ª×•×“×” ×¨×‘×”', '×ª×•×“×” ×œ×š', '×ª×•×“×” ×××•×“', '×ª×•×“×” ×¢× ×§×™×ª', '×ª×•×“×”×”',
        '×‘×¡×“×¨', '××•×§×™×™', '××•×§×™', '×‘×¡×“×¨ ×’××•×¨', '×‘×¡×“×¨ ××•×©×œ×', '××•×§×™×™×™',
        '×× ×™ ××‘×™×Ÿ', '××”', '×•×•××•', '××¢× ×™×™×Ÿ', '× ×›×•×Ÿ', '××›×Ÿ', '××” ××”',
        '×›×Ÿ', '×œ×', '××•×œ×™', '×™×›×•×œ ×œ×”×™×•×ª', '××¤×©×¨×™',
        '×× ×™ ×œ× ×™×•×“×¢', '×œ× ×™×•×“×¢', '×œ× ×‘×˜×•×—', '×œ× ×™×•×“×¢ ××” ×œ×”×’×™×“', '××™×Ÿ ×œ×™ ××•×©×’',
        '×‘×”×—×œ×˜', '×‘×˜×—', '×›××•×‘×Ÿ', '×‘×¨×•×¨', '×•×“××™', '×‘×•×•×“××™',
        '××¢×•×œ×”', '× ×”×“×¨', '××“×”×™×', '×¤× ×˜×¡×˜×™', '××•×©×œ×',
        '××” ××•×§×™×™', '××” ×‘×¡×“×¨', '××” ×”×‘× ×ª×™', '××” × ×›×•×Ÿ',
        '×›×Ÿ ×œ×', '×›×Ÿ ××•×œ×™', '××•×œ×™ ×›×Ÿ', '××•×œ×™ ×œ×',
        '××” ××•×§×™×™ ×ª×•×“×”', '××” ×‘×¡×“×¨ ×ª×•×“×”', '××” ×”×‘× ×ª×™ ×ª×•×“×”',
        '×˜×•×‘', '×˜×•×‘ ×××•×“', '×˜×•×‘ ×××“', '×œ× ×¨×¢', '×œ× ×¨×¢ ×‘×›×œ×œ',
        '×‘×¡×“×¨ ×’××•×¨', '×‘×¡×“×¨ ××•×©×œ×', '×‘×¡×“×¨ ×œ×’××¨×™', '×‘×¡×“×¨ ×œ×—×œ×•×˜×™×Ÿ',
        '××¦×•×™×Ÿ', '××¦×•×™×™×Ÿ', '××¢×•×œ×”', '× ×”×“×¨', '××“×”×™×', '×¤× ×˜×¡×˜×™',
        '×× ×™ ×‘×¡×“×¨', '×× ×™ ×˜×•×‘', '×× ×™ ××¦×•×™×Ÿ', '×× ×™ ××¢×•×œ×”',
        '×”×›×œ ×˜×•×‘', '×”×›×œ ×‘×¡×“×¨', '×”×›×œ ××¦×•×™×Ÿ', '×”×›×œ ××¢×•×œ×”',
        '×¡×‘×‘×”', '×¡×‘×‘×” ×’××•×¨×”', '×¡×‘×‘×” ××•×©×œ××ª',
        '×§×•×œ', '×§×•×œ ×œ×’××¨×™', '×§×•×œ ×œ×—×œ×•×˜×™×Ÿ',
        '××—×œ×”', '××—×œ×” ×’××•×¨×”', '××—×œ×” ××•×©×œ××ª',
        '×™×•×¤×™', '×™×•×¤×™ ×’××•×¨', '×™×•×¤×™ ××•×©×œ×',
        '××¢×•×œ×”', '××¢×•×œ×” ×’××•×¨×”', '××¢×•×œ×” ××•×©×œ××ª',
        '× ×”×“×¨', '× ×”×“×¨ ×œ×’××¨×™', '× ×”×“×¨ ×œ×—×œ×•×˜×™×Ÿ',
        '××“×”×™×', '××“×”×™× ×œ×’××¨×™', '××“×”×™× ×œ×—×œ×•×˜×™×Ÿ',
        '×¤× ×˜×¡×˜×™', '×¤× ×˜×¡×˜×™ ×œ×’××¨×™', '×¤× ×˜×¡×˜×™ ×œ×—×œ×•×˜×™×Ÿ',
        '××•×©×œ×', '××•×©×œ× ×œ×’××¨×™', '××•×©×œ× ×œ×—×œ×•×˜×™×Ÿ',
        '×× ×™ ××•×§×™×™', '×× ×™ ×‘×¡×“×¨ ×’××•×¨', '×× ×™ ×‘×¡×“×¨ ××•×©×œ×',
        '×× ×™ ×˜×•×‘ ×××•×“', '×× ×™ ×˜×•×‘ ×××“', '×× ×™ ×˜×•×‘ ×œ×’××¨×™',
        '×× ×™ ××¦×•×™×Ÿ ×œ×’××¨×™', '×× ×™ ××¢×•×œ×” ×œ×’××¨×™', '×× ×™ × ×”×“×¨ ×œ×’××¨×™',
        '×× ×™ ××“×”×™× ×œ×’××¨×™', '×× ×™ ×¤× ×˜×¡×˜×™ ×œ×’××¨×™', '×× ×™ ××•×©×œ× ×œ×’××¨×™',
        '×˜×•×‘ ××—×™', '×‘×¡×“×¨ ××—×™', '××¢×•×œ×” ××—×™', '× ×”×“×¨ ××—×™', '××“×”×™× ××—×™',
        '×¡×‘×‘×” ××—×™', '×§×•×œ ××—×™', '××—×œ×” ××—×™', '×™×•×¤×™ ××—×™', '××•×©×œ× ××—×™',
        '×× ×™ ×‘×¡×“×¨ ××—×™', '×× ×™ ×˜×•×‘ ××—×™', '×× ×™ ××¢×•×œ×” ××—×™', '×× ×™ × ×”×“×¨ ××—×™',
        '×”×›×œ ×˜×•×‘ ××—×™', '×”×›×œ ×‘×¡×“×¨ ××—×™', '×”×›×œ ××¢×•×œ×” ××—×™',
        '××—×™', '××—', '××—×™×™', '××—×™×™×™', '××—×™×™×™×™',
        '××—×™ ×˜×•×‘', '××—×™ ×‘×¡×“×¨', '××—×™ ××¢×•×œ×”', '××—×™ × ×”×“×¨', '××—×™ ××“×”×™×',
        '××—×™ ×¡×‘×‘×”', '××—×™ ×§×•×œ', '××—×™ ××—×œ×”', '××—×™ ×™×•×¤×™', '××—×™ ××•×©×œ×'
    ]
    # ××™××•×’'×™ ×‘×œ×‘×“
    emoji_only = ['ğŸ‘', 'ğŸ‘', 'â¤ï¸', 'ğŸ˜Š', 'ğŸ˜¢', 'ğŸ˜¡', 'ğŸ¤”', 'ğŸ˜…', 'ğŸ˜‚', 'ğŸ˜­']
    # × ×§×•×“×•×ª ×‘×œ×‘×“
    dots_only = ['...', '....', '.....', '......']
    # ×¡×™×× ×™ ×§×¨×™××” ×‘×œ×‘×“
    exclamation_only = ['!!!', '!!!!', '!!!!!']
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×‘×“×™×•×§ ×‘×™×˜×•×™ ×‘×¡×™×¡×™
    message_lower = message.lower()
    for phrase in base_phrases:
        if message_lower == phrase.lower():
            return False
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×‘×™×˜×•×™ ×‘×¡×™×¡×™ + ×ª×•×•×™× × ×•×¡×¤×™×
    for phrase in base_phrases:
        phrase_lower = phrase.lower()
        # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ××ª×—×™×œ×” ×‘×‘×™×˜×•×™ ×”×‘×¡×™×¡×™
        if message_lower.startswith(phrase_lower):
            # ××” ×©× ×©××¨ ××—×¨×™ ×”×‘×™×˜×•×™ ×”×‘×¡×™×¡×™
            remaining = message_lower[len(phrase_lower):].strip()
            # ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ×ª×•×•×™× ××•×ª×¨×™×, ××– ×œ× ×œ×”×¤×¢×™×œ gpt_c
            if remaining in ['', '!', '?', ':)', ':(', '!:)', '?:(', '!:(', '?:)', '...', '....', '.....', '......', '!!!', '!!!!', '!!!!!']:
                return False
            # ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ××™××•×’'×™ ××• ×©×™×œ×•×‘ ×©×œ ×ª×•×•×™× ××•×ª×¨×™×
            # ×”×¡×¨×ª ×¨×•×•×—×™× ××”×ª×—×œ×” ×•××”×¡×•×£
            remaining_clean = remaining.strip()
            # ×‘×“×™×§×” ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ×ª×•×•×™× ××•×ª×¨×™×
            allowed_chars = r'^[!?:\.\s\(\)]+$'
            if re.match(allowed_chars, remaining_clean):
                return False
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×¨×§ ××™××•×’'×™
    if message in emoji_only:
        return False
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×¨×§ × ×§×•×“×•×ª
    if message in dots_only:
        return False
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×¨×§ ×¡×™×× ×™ ×§×¨×™××”
    if message in exclamation_only:
        return False
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×‘×™×˜×•×™ + ××™××•×’'×™
    for phrase in base_phrases:
        phrase_lower = phrase.lower()
        if message_lower.startswith(phrase_lower):
            remaining = message_lower[len(phrase_lower):].strip()
            # ×‘×“×™×§×” ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ××™××•×’'×™
            if remaining in ['ğŸ‘', 'ğŸ‘', 'â¤ï¸', 'ğŸ˜Š', 'ğŸ˜¢', 'ğŸ˜¡', 'ğŸ¤”', 'ğŸ˜…', 'ğŸ˜‚', 'ğŸ˜­']:
                return False
    # ×× ×”×’×¢× ×• ×œ×›××Ÿ, ×”×”×•×“×¢×” ×™×›×•×œ×” ×œ×”×›×™×œ ××™×“×¢ ×—×“×©
    return True 