"""
gpt_c_handler.py
----------------
×× ×•×¢ gpt_c: ×—×™×œ×•×¥ ××™×“×¢ ××”×•×“×¢×•×ª ××©×ª××© ×œ×¢×“×›×•×Ÿ ×¤×¨×•×¤×™×œ
"""

import logging
from datetime import datetime
import json
import litellm
import re
from prompts import build_profile_extraction_enhanced_prompt
from config import GPT_MODELS, GPT_PARAMS, GPT_FALLBACK_MODELS, should_log_data_extraction_debug, should_log_gpt_cost_debug
from gpt_utils import normalize_usage_dict, measure_llm_latency, calculate_gpt_cost, extract_json_from_text

def extract_user_info(user_msg, chat_id=None, message_id=None):
    """
    ××—×œ×¥ ××™×“×¢ ×¨×œ×•×•× ×˜×™ ××”×•×“×¢×ª ×”××©×ª××© ×œ×¢×“×›×•×Ÿ ×”×¤×¨×•×¤×™×œ ×©×œ×•
    ×›×•×œ×œ ××¢×¨×›×ª fallback ×œ××§×¨×” ×©×œ rate limit ×‘-Gemini.
    """
    metadata = {"gpt_identifier": "gpt_c", "chat_id": chat_id, "message_id": message_id}
    params = GPT_PARAMS["gpt_c"]
    model = GPT_MODELS["gpt_c"]
    
    completion_params = {
        "model": model,
        "messages": [{"role": "system", "content": build_profile_extraction_enhanced_prompt()}, {"role": "user", "content": user_msg}],
        "temperature": params["temperature"],
        "metadata": metadata,
        "store": True
    }
    
    # ×”×•×¡×¤×ª max_tokens ×¨×§ ×× ×”×•× ×œ× None
    if params["max_tokens"] is not None:
        completion_params["max_tokens"] = params["max_tokens"]
    
    # × ×™×¡×™×•×Ÿ ×¢× ×”××•×“×œ ×”×¨××©×™
    try:
        with measure_llm_latency(model):
            response = litellm.completion(**completion_params)
        content_raw = response.choices[0].message.content.strip()
        content = extract_json_from_text(content_raw)

        usage = normalize_usage_dict(response.usage, response.model)
        
        # × ×™×¡×™×•×Ÿ ×œ×¤×¨×¡ JSON ×¢× ×œ×•×’×™× ××¤×•×¨×˜×™×
        try:
            if content and content.strip():
                content_clean = content.strip()
                if content_clean.startswith("{") and content_clean.endswith("}"):
                    extracted_fields = json.loads(content_clean)
                    if not isinstance(extracted_fields, dict):
                        logging.warning(f"[GPT_C] JSON parsed but not a dict: {type(extracted_fields)}")
                        extracted_fields = {}
                else:
                    logging.warning(f"[GPT_C] Content doesn't look like JSON: {content_clean[:100]}...")
                    extracted_fields = {}
            else:
                extracted_fields = {}
        except json.JSONDecodeError as e:
            logging.error(f"[GPT_C] JSON decode error: {e} | Content: {content[:200] if content else 'None'}...")
            extracted_fields = {}
        except Exception as e:
            logging.error(f"[GPT_C] Unexpected error parsing JSON: {e} | Content: {content[:200] if content else 'None'}...")
            extracted_fields = {}
        
        # ×”×“×¤×¡×ª ××™×“×¢ ×—×©×•×‘ ×¢×œ ×—×™×œ×•×¥ × ×ª×•× ×™× (×ª××™×“ ×™×•×¤×™×¢!)
        print(f"ğŸ” [GPT-C] ×—×•×œ×¦×• {len(extracted_fields)} ×©×“×•×ª: {list(extracted_fields.keys())}")
        if should_log_gpt_cost_debug():
            print(f"ğŸ’° [GPT-C] ×¢×œ×•×ª: {usage.get('cost_total', 0):.6f}$ | ×˜×•×§× ×™×: {usage.get('total_tokens', 0)}")
        if should_log_data_extraction_debug():
            print(f"ğŸ“‹ [GPT-C] × ×ª×•× ×™× ×©×—×•×œ×¦×•: {json.dumps(extracted_fields, ensure_ascii=False, indent=2)}")
        
        try:
            cost_info = calculate_gpt_cost(
                prompt_tokens=usage.get("prompt_tokens", 0),
                completion_tokens=usage.get("completion_tokens", 0),
                cached_tokens=usage.get("cached_tokens", 0),
                model_name=response.model,
                completion_response=response
            )
            usage.update(cost_info)
        except Exception as _cost_e:
            logging.warning(f"[gpt_c] Cost calc failed: {_cost_e}")
        
        return {"extracted_fields": extracted_fields, "usage": usage, "model": response.model}
        
    except Exception as e:
        error_str = str(e)
        is_rate_limit = "429" in error_str or "quota" in error_str.lower() or "rate limit" in error_str.lower()
        
        if is_rate_limit and "gpt_c" in GPT_FALLBACK_MODELS:
            # × ×™×¡×™×•×Ÿ ×¢× ××•×“×œ fallback
            fallback_model = GPT_FALLBACK_MODELS["gpt_c"]
            logging.warning(f"[gpt_c] Rate limit for {model}, trying fallback: {fallback_model}")
            
            try:
                completion_params["model"] = fallback_model
                completion_params["metadata"]["fallback_used"] = True
                
                with measure_llm_latency(fallback_model):
                    response = litellm.completion(**completion_params)
                content_raw = response.choices[0].message.content.strip()
                content = extract_json_from_text(content_raw)

                usage = normalize_usage_dict(response.usage, response.model)
                
                # × ×™×¡×™×•×Ÿ ×œ×¤×¨×¡ JSON ×¢× ×œ×•×’×™× ××¤×•×¨×˜×™× (fallback)
                try:
                    if content and content.strip():
                        content_clean = content.strip()
                        if content_clean.startswith("{") and content_clean.endswith("}"):
                            extracted_fields = json.loads(content_clean)
                            if not isinstance(extracted_fields, dict):
                                logging.warning(f"[GPT_C FALLBACK] JSON parsed but not a dict: {type(extracted_fields)}")
                                extracted_fields = {}
                        else:
                            logging.warning(f"[GPT_C FALLBACK] Content doesn't look like JSON: {content_clean[:100]}...")
                            extracted_fields = {}
                    else:
                        extracted_fields = {}
                except json.JSONDecodeError as e:
                    logging.error(f"[GPT_C FALLBACK] JSON decode error: {e} | Content: {content[:200] if content else 'None'}...")
                    extracted_fields = {}
                except Exception as e:
                    logging.error(f"[GPT_C FALLBACK] Unexpected error parsing JSON: {e} | Content: {content[:200] if content else 'None'}...")
                    extracted_fields = {}
                
                logging.info(f"[gpt_c] Fallback successful: {fallback_model}")
                
                # ×”×“×¤×¡×ª ××™×“×¢ ×—×©×•×‘ ×¢×œ ×—×™×œ×•×¥ × ×ª×•× ×™× (×ª××™×“ ×™×•×¤×™×¢!)
                print(f"ğŸ” [GPT-C FALLBACK] ×—×•×œ×¦×• {len(extracted_fields)} ×©×“×•×ª: {list(extracted_fields.keys())}")
                if should_log_gpt_cost_debug():
                    print(f"ğŸ’° [GPT-C FALLBACK] ×¢×œ×•×ª: {usage.get('cost_total', 0):.6f}$ | ×˜×•×§× ×™×: {usage.get('total_tokens', 0)}")
                if should_log_data_extraction_debug():
                    print(f"ğŸ“‹ [GPT-C FALLBACK] × ×ª×•× ×™× ×©×—×•×œ×¦×•: {json.dumps(extracted_fields, ensure_ascii=False, indent=2)}")
                
                try:
                    cost_info = calculate_gpt_cost(
                        prompt_tokens=usage.get("prompt_tokens", 0),
                        completion_tokens=usage.get("completion_tokens", 0),
                        cached_tokens=usage.get("cached_tokens", 0),
                        model_name=response.model,
                        completion_response=response
                    )
                    usage.update(cost_info)
                except Exception as _cost_e:
                    logging.warning(f"[gpt_c] Cost calc failed: {_cost_e}")
                
                return {"extracted_fields": extracted_fields, "usage": usage, "model": response.model, "fallback_used": True}
                
            except Exception as fallback_error:
                logging.error(f"[gpt_c] Fallback also failed: {fallback_error}")
                print(f"âŒ [GPT-C] ×©×’×™××” ×’× ×‘-fallback: {fallback_error}")
                return {"extracted_fields": {}, "usage": {}, "model": fallback_model}
        else:
            logging.error(f"[gpt_c] Error (not rate limit): {e}")
            print(f"âŒ [GPT-C] ×©×’×™××”: {e}")
            return {"extracted_fields": {}, "usage": {}, "model": model}

def should_run_gpt_c(user_message):
    """
    ×‘×•×“×§ ×× ×™×© ×˜×¢× ×œ×”×¤×¢×™×œ gpt_c ×¢×œ ×”×•×“×¢×” × ×ª×•× ×”.
    ××—×–×™×¨ False ×¨×§ ×¢×œ ×”×•×“×¢×•×ª ×©×× ×—× ×• ×‘×˜×•×—×™× ×©×œ× ××›×™×œ×•×ª ××™×“×¢ ×—×“×©.
    ×”×›×œ×œ: gpt_c ××•×¤×¢×œ ×ª××™×“, ××œ× ×× ×›×Ÿ ×”×”×•×“×¢×” ×”×™× ××©×”×• ×©×œ× ×™×›×•×œ ×œ×”×›×™×œ ××™×“×¢ ×—×“×©.
    """
    if not user_message or not user_message.strip():
        return False
    message = user_message.strip()
    # ×‘×™×˜×•×™×™× ×‘×¡×™×¡×™×™× ×©×œ× ×™×›×•×œ×™× ×œ×”×›×™×œ ××™×“×¢ ×—×“×©
    base_phrases = [
        '×”×™×™', '×©×œ×•×', '××” ×©×œ×•××š', '××” × ×©××¢', '××” ×§×•×¨×”', '××” ×”××¦×‘',
        '×ª×•×“×”', '×ª×•×“×” ×¨×‘×”', '×ª×•×“×” ×œ×š', '×ª×•×“×” ×××•×“', '×ª×•×“×” ×¢× ×§×™×ª', '×ª×•×“×”×”',
        '×‘×¡×“×¨', '××•×§×™×™', '××•×§×™', '×‘×¡×“×¨ ×’××•×¨', '×‘×¡×“×¨ ××•×©×œ×', '××•×§×™×™×™',
        '×× ×™ ××‘×™×Ÿ', '××”', '×•×•××•', '××¢× ×™×™×Ÿ', '× ×›×•×Ÿ', '××›×Ÿ', '××” ××”',
        '×›×Ÿ', '×œ×', '××•×œ×™', '×™×›×•×œ ×œ×”×™×•×ª', '××¤×©×¨×™',
        '×× ×™ ×œ× ×™×•×“×¢', '×œ× ×™×•×“×¢', '×œ× ×‘×˜×•×—', '×œ× ×™×•×“×¢ ××” ×œ×”×’×™×“', '××™×Ÿ ×œ×™ ××•×©×’',
        '×‘×”×—×œ×˜', '×‘×˜×—', '×›××•×‘×Ÿ', '×‘×¨×•×¨', '×•×“××™', '×‘×•×•×“××™',
        '××¢×•×œ×”', '× ×”×“×¨', '××“×”×™×', '×¤× ×˜×¡×˜×™', '××•×©×œ×',
        '××” ××•×§×™×™', '××” ×‘×¡×“×¨', '××” ×”×‘× ×ª×™', '××” × ×›×•×Ÿ',
        '×›×Ÿ ×œ×', '×›×Ÿ ××•×œ×™', '××•×œ×™ ×›×Ÿ', '××•×œ×™ ×œ×',
        '××” ××•×§×™×™ ×ª×•×“×”', '××” ×‘×¡×“×¨ ×ª×•×“×”', '××” ×”×‘× ×ª×™ ×ª×•×“×”',
        '×˜×•×‘', '×˜×•×‘ ×××•×“', '×˜×•×‘ ×××“', '×œ× ×¨×¢', '×œ× ×¨×¢ ×‘×›×œ×œ',
        '×‘×¡×“×¨ ×’××•×¨', '×‘×¡×“×¨ ××•×©×œ×', '×‘×¡×“×¨ ×œ×’××¨×™', '×‘×¡×“×¨ ×œ×—×œ×•×˜×™×Ÿ',
        '××¦×•×™×Ÿ', '××¦×•×™×™×Ÿ', '××¢×•×œ×”', '× ×”×“×¨', '××“×”×™×', '×¤× ×˜×¡×˜×™',
        '×× ×™ ×‘×¡×“×¨', '×× ×™ ×˜×•×‘', '×× ×™ ××¦×•×™×Ÿ', '×× ×™ ××¢×•×œ×”',
        '×”×›×œ ×˜×•×‘', '×”×›×œ ×‘×¡×“×¨', '×”×›×œ ××¦×•×™×Ÿ', '×”×›×œ ××¢×•×œ×”',
        '×¡×‘×‘×”', '×¡×‘×‘×” ×’××•×¨×”', '×¡×‘×‘×” ××•×©×œ××ª',
        '×§×•×œ', '×§×•×œ ×œ×’××¨×™', '×§×•×œ ×œ×—×œ×•×˜×™×Ÿ',
        '××—×œ×”', '××—×œ×” ×’××•×¨×”', '××—×œ×” ××•×©×œ××ª',
        '×™×•×¤×™', '×™×•×¤×™ ×’××•×¨', '×™×•×¤×™ ××•×©×œ×',
        '××¢×•×œ×”', '××¢×•×œ×” ×’××•×¨×”', '××¢×•×œ×” ××•×©×œ××ª',
        '× ×”×“×¨', '× ×”×“×¨ ×œ×’××¨×™', '× ×”×“×¨ ×œ×—×œ×•×˜×™×Ÿ',
        '××“×”×™×', '××“×”×™× ×œ×’××¨×™', '××“×”×™× ×œ×—×œ×•×˜×™×Ÿ',
        '×¤× ×˜×¡×˜×™', '×¤× ×˜×¡×˜×™ ×œ×’××¨×™', '×¤× ×˜×¡×˜×™ ×œ×—×œ×•×˜×™×Ÿ',
        '××•×©×œ×', '××•×©×œ× ×œ×’××¨×™', '××•×©×œ× ×œ×—×œ×•×˜×™×Ÿ',
        '×× ×™ ××•×§×™×™', '×× ×™ ×‘×¡×“×¨ ×’××•×¨', '×× ×™ ×‘×¡×“×¨ ××•×©×œ×',
        '×× ×™ ×˜×•×‘ ×××•×“', '×× ×™ ×˜×•×‘ ×××“', '×× ×™ ×˜×•×‘ ×œ×’××¨×™',
        '×× ×™ ××¦×•×™×Ÿ ×œ×’××¨×™', '×× ×™ ××¢×•×œ×” ×œ×’××¨×™', '×× ×™ × ×”×“×¨ ×œ×’××¨×™',
        '×× ×™ ××“×”×™× ×œ×’××¨×™', '×× ×™ ×¤× ×˜×¡×˜×™ ×œ×’××¨×™', '×× ×™ ××•×©×œ× ×œ×’××¨×™',
        '×˜×•×‘ ××—×™', '×‘×¡×“×¨ ××—×™', '××¢×•×œ×” ××—×™', '× ×”×“×¨ ××—×™', '××“×”×™× ××—×™',
        '×¡×‘×‘×” ××—×™', '×§×•×œ ××—×™', '××—×œ×” ××—×™', '×™×•×¤×™ ××—×™', '××•×©×œ× ××—×™',
        '×× ×™ ×‘×¡×“×¨ ××—×™', '×× ×™ ×˜×•×‘ ××—×™', '×× ×™ ××¢×•×œ×” ××—×™', '×× ×™ × ×”×“×¨ ××—×™',
        '×”×›×œ ×˜×•×‘ ××—×™', '×”×›×œ ×‘×¡×“×¨ ××—×™', '×”×›×œ ××¢×•×œ×” ××—×™',
        '××—×™', '××—', '××—×™×™', '××—×™×™×™', '××—×™×™×™×™',
        '××—×™ ×˜×•×‘', '××—×™ ×‘×¡×“×¨', '××—×™ ××¢×•×œ×”', '××—×™ × ×”×“×¨', '××—×™ ××“×”×™×',
        '××—×™ ×¡×‘×‘×”', '××—×™ ×§×•×œ', '××—×™ ××—×œ×”', '××—×™ ×™×•×¤×™', '××—×™ ××•×©×œ×'
    ]
    # ××™××•×’'×™ ×‘×œ×‘×“
    emoji_only = ['ğŸ‘', 'ğŸ‘', 'â¤ï¸', 'ğŸ˜Š', 'ğŸ˜¢', 'ğŸ˜¡', 'ğŸ¤”', 'ğŸ˜…', 'ğŸ˜‚', 'ğŸ˜­']
    # × ×§×•×“×•×ª ×‘×œ×‘×“
    dots_only = ['...', '....', '.....', '......']
    # ×¡×™×× ×™ ×§×¨×™××” ×‘×œ×‘×“
    exclamation_only = ['!!!', '!!!!', '!!!!!']
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×‘×“×™×•×§ ×‘×™×˜×•×™ ×‘×¡×™×¡×™
    message_lower = message.lower()
    for phrase in base_phrases:
        if message_lower == phrase.lower():
            return False
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×‘×™×˜×•×™ ×‘×¡×™×¡×™ + ×ª×•×•×™× × ×•×¡×¤×™×
    for phrase in base_phrases:
        phrase_lower = phrase.lower()
        # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ××ª×—×™×œ×” ×‘×‘×™×˜×•×™ ×”×‘×¡×™×¡×™
        if message_lower.startswith(phrase_lower):
            # ××” ×©× ×©××¨ ××—×¨×™ ×”×‘×™×˜×•×™ ×”×‘×¡×™×¡×™
            remaining = message_lower[len(phrase_lower):].strip()
            # ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ×ª×•×•×™× ××•×ª×¨×™×, ××– ×œ× ×œ×”×¤×¢×™×œ gpt_c
            if remaining in ['', '!', '?', ':)', ':(', '!:)', '?:(', '!:(', '?:)', '...', '....', '.....', '......', '!!!', '!!!!', '!!!!!']:
                return False
            # ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ××™××•×’'×™ ××• ×©×™×œ×•×‘ ×©×œ ×ª×•×•×™× ××•×ª×¨×™×
            # ×”×¡×¨×ª ×¨×•×•×—×™× ××”×ª×—×œ×” ×•××”×¡×•×£
            remaining_clean = remaining.strip()
            # ×‘×“×™×§×” ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ×ª×•×•×™× ××•×ª×¨×™×
            allowed_chars = r'^[!?:\.\s\(\)]+$'
            if re.match(allowed_chars, remaining_clean):
                return False
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×¨×§ ××™××•×’'×™
    if message in emoji_only:
        return False
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×¨×§ × ×§×•×“×•×ª
    if message in dots_only:
        return False
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×¨×§ ×¡×™×× ×™ ×§×¨×™××”
    if message in exclamation_only:
        return False
    # ×‘×“×™×§×” ×× ×”×”×•×“×¢×” ×”×™× ×‘×™×˜×•×™ + ××™××•×’'×™
    for phrase in base_phrases:
        phrase_lower = phrase.lower()
        if message_lower.startswith(phrase_lower):
            remaining = message_lower[len(phrase_lower):].strip()
            # ×‘×“×™×§×” ×× ××” ×©× ×©××¨ ×”×•× ×¨×§ ××™××•×’'×™
            if remaining in ['ğŸ‘', 'ğŸ‘', 'â¤ï¸', 'ğŸ˜Š', 'ğŸ˜¢', 'ğŸ˜¡', 'ğŸ¤”', 'ğŸ˜…', 'ğŸ˜‚', 'ğŸ˜­']:
                return False
    # ×× ×”×’×¢× ×• ×œ×›××Ÿ, ×”×”×•×“×¢×” ×™×›×•×œ×” ×œ×”×›×™×œ ××™×“×¢ ×—×“×©
    return True 