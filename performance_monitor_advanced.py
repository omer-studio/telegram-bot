"""
performance_monitor_advanced.py
-------------------------------
注专转  爪专 拽拽 转拽转 - 注 转 -streaming
=========================================================
专住 转拽转 砖 注专转 转 爪注 注:
1. 转 TTFT 拽转 爪注转 streaming
2. 专  转 砖 TPS  爪专转 转
3. 转 驻专 转专 砖 驻住 爪注
4.   砖 住 爪专 拽拽 砖
"""

import time
import json
import logging
import asyncio
from typing import Dict, AsyncGenerator, Optional
from dataclasses import dataclass
from performance_monitor import PerformanceMonitor, PerformanceMetrics, performance_monitor
import litellm

class StreamingPerformanceMonitor(PerformanceMonitor):
    """
    专 爪注 转拽 注 转 转 streaming 拽转
    """
    
    def __init__(self, target_samples: int = 100):
        super().__init__(target_samples)
        self.streaming_measurements: Dict[str, Dict] = {}
    
    async def measure_streaming_response(self, full_messages, model, completion_params, measurement_id):
        """
         爪注  streaming response
        专 转 转  + 转 拽转
        """
        if measurement_id not in self.active_measurements:
            logging.warning(f" Measurement ID not found: {measurement_id}")
            return None
        
        try:
            # 住驻转 streaming 驻专专
            stream_params = completion_params.copy()
            stream_params['stream'] = True
            
            # 转转 拽砖
            request_start = time.time()
            response_stream = litellm.completion(**stream_params)
            
            # 砖转 注拽
            first_token_received = False
            tokens_received = 0
            content_chunks = []
            last_chunk_time = time.time()
            token_times = []
            
            # 注 -stream
            async for chunk in response_stream:
                current_time = time.time()
                
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    content_chunks.append(content)
                    
                    # 专砖 拽 专砖
                    if not first_token_received:
                        first_token_received = True
                        self.record_first_token(measurement_id)
                        logging.info(f" First token received after {current_time - request_start:.3f}s")
                    
                    # 砖 TPS 
                    if tokens_received > 0:
                        time_since_last = current_time - last_chunk_time
                        if time_since_last > 0:
                            # 注专转 住驻专 拽 拽  (住  注)
                            estimated_tokens = len(content) / 3.5  # 注专 住
                            instantaneous_tps = estimated_tokens / time_since_last
                            token_times.append({
                                'time': current_time,
                                'estimated_tps': instantaneous_tps,
                                'content_length': len(content)
                            })
                    
                    tokens_received += 1
                    last_chunk_time = current_time
            
            # 住祝 注 住驻
            full_content = ''.join(content_chunks)
            total_response_time = time.time() - request_start
            
            # 专砖 住住拽转 streaming 住驻转
            if measurement_id in self.active_measurements:
                self.streaming_measurements[measurement_id] = {
                    'total_chunks': tokens_received,
                    'avg_chunk_interval': total_response_time / max(tokens_received, 1),
                    'token_timing_data': token_times[-10:],  # 砖专转 10 专
                    'content_length': len(full_content),
                    'estimated_tokens_by_length': len(full_content) / 3.5
                }
            
            return {
                'content': full_content,
                'total_response_time': total_response_time,
                'chunks_received': tokens_received,
                'first_token_time': time.time() if first_token_received else None
            }
            
        except Exception as e:
            logging.error(f" Streaming measurement failed: {e}")
            # 专  专 拽专 砖 砖
            return None
    
    def get_detailed_analysis(self) -> Dict:
        """
        转 驻专 转专  转 streaming
        """
        base_analysis = super().analyze_performance()
        
        if "error" in base_analysis:
            return base_analysis
        
        # 住驻转 转 streaming 转拽
        streaming_data = []
        for measurement_id, stream_info in self.streaming_measurements.items():
            streaming_data.append(stream_info)
        
        if streaming_data:
            # 转 驻住  TPS
            avg_chunk_intervals = [d['avg_chunk_interval'] for d in streaming_data]
            
            base_analysis['streaming_analysis'] = {
                'total_streaming_measurements': len(streaming_data),
                'avg_chunk_interval': sum(avg_chunk_intervals) / len(avg_chunk_intervals),
                'response_consistency': self._analyze_consistency(streaming_data),
                'streaming_efficiency': self._analyze_streaming_efficiency(streaming_data)
            }
        
        return base_analysis
    
    def _analyze_consistency(self, streaming_data) -> Dict:
        """
        转 注拽转 爪注 streaming
        """
        if not streaming_data:
            return {}
        
        intervals = [d['avg_chunk_interval'] for d in streaming_data]
        mean_interval = sum(intervals) / len(intervals)
        
        # 砖 住转 转拽
        variance = sum((x - mean_interval) ** 2 for x in intervals) / len(intervals)
        std_dev = variance ** 0.5
        
        consistency_score = 1 - (std_dev / mean_interval) if mean_interval > 0 else 0
        
        return {
            'consistency_score': consistency_score,
            'interpretation': '爪' if consistency_score > 0.8 else '' if consistency_score > 0.6 else ' 爪',
            'std_deviation': std_dev,
            'mean_interval': mean_interval
        }
    
    def _analyze_streaming_efficiency(self, streaming_data) -> Dict:
        """
        转 注转 转 -streaming
        """
        if not streaming_data:
            return {}
        
        total_chunks = sum(d['total_chunks'] for d in streaming_data)
        total_content = sum(d['content_length'] for d in streaming_data)
        
        avg_content_per_chunk = total_content / max(total_chunks, 1)
        
        efficiency_score = min(avg_content_per_chunk / 10, 1.0)  # 爪 注转 住
        
        return {
            'avg_content_per_chunk': avg_content_per_chunk,
            'efficiency_score': efficiency_score,
            'interpretation': '注' if efficiency_score > 0.7 else '' if efficiency_score > 0.4 else ' 注',
            'total_measurements': len(streaming_data)
        }

# 驻拽爪转 注专 砖砖 -GPT handlers
async def measure_gpt_response_with_streaming(full_messages, model, completion_params, chat_id, message_id):
    """
    驻拽爪转 注专 转 转转 GPT 注 streaming
    砖转砖 -gpt_a_handler 拽 专住 专
    """
    # 爪专转 measurement_id
    user_message = full_messages[-1]["content"] if full_messages else ""
    measurement_id = performance_monitor.start_measurement(
        chat_id=str(chat_id),
        message_id=str(message_id),
        user_message=user_message
    )
    
    # 砖砖 专 转拽  
    if hasattr(performance_monitor, 'measure_streaming_response'):
        try:
            result = await performance_monitor.measure_streaming_response(
                full_messages, model, completion_params, measurement_id
            )
            
            if result:
                # 砖转  注 转 转
                #  爪专 注 注 转 usage 转 -response
                performance_monitor.record_response_complete(
                    measurement_id=measurement_id,
                    prompt_tokens=0,  # 注 response 转
                    completion_tokens=int(result.get('estimated_tokens_by_length', 0)),
                    model_used=model,
                    model_tier="streaming"
                )
                
                return result['content']
        except Exception as e:
            logging.error(f" Streaming measurement failed, falling back to regular: {e}")
    
    # fallback  专
    try:
        response = litellm.completion(**completion_params)
        performance_monitor.record_first_token(measurement_id)
        
        content = response.choices[0].message.content.strip()
        
        performance_monitor.record_response_complete(
            measurement_id=measurement_id,
            prompt_tokens=getattr(response.usage, 'prompt_tokens', 0),
            completion_tokens=getattr(response.usage, 'completion_tokens', 0),
            model_used=response.model,
            model_tier="regular"
        )
        
        return content
        
    except Exception as e:
        performance_monitor.record_error(measurement_id, str(e))
        raise

# 爪专转 instance 转拽
advanced_performance_monitor = StreamingPerformanceMonitor()

# 驻拽爪 砖专 专 拽
def upgrade_to_advanced_monitor():
    """
    砖专 转 专 拽 专住 转拽转
    """
    global performance_monitor
    
    # 注专转 转 拽
    current_data = performance_monitor.load_all_measurements()
    
    # 爪专转 专 砖
    new_monitor = StreamingPerformanceMonitor(performance_monitor.target_samples)
    
    # 砖专转 转 拽
    for measurement in current_data:
        new_monitor._save_measurement(measurement)
    
    # 驻
    performance_monitor = new_monitor
    
    logging.info(" Performance monitor upgraded to advanced streaming version")
    return new_monitor